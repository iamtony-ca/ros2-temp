ref: https://kimelab.notion.site/KIRIA-25-08-20-21-203875de525f807eb728eb98e76e2382  
  

# **[KIRIA] (25.08.20~21) ë¹„ì „-ì–¸ì–´-ì•¡ì…˜ ëª¨ë¸ ê¸°ë°˜ ë¡œë´‡ ë§¤ë‹ˆí“°ë ˆì´ì…˜**

# êµìœ¡ ë‚´ìš©

**ì¥ì†Œ:** 

**ì¼ì‹œ:** 

**êµìœ¡ëª©í‘œ:**

- íŠ¸ëœìŠ¤í¬ë¨¸ ë° ë””í“¨ì „ ëª¨ë¸ ê¸°ì´ˆ ë° ì‹¤ìŠµ
- ë¹„ì „-ì–¸ì–´-ì•¡ì…˜ ëª¨ë¸ (OpenVLA) ì„¸íŒ… ë° ì‹¤ìŠµ
- LIBERO ë²¤ì¹˜ë§ˆí¬ ê¸°ë°˜ OpenVLA êµ¬ë™ ì‹¤ìŠµ

![image.png](attachment:4f80d082-a17a-42b3-9ab3-2a5d5f4a0dc1:image.png)

---

# ê°•ì˜ìë£Œ

# 1. ê°œë°œí™˜ê²½ êµ¬ì¶• (PyTorch)

## Python ë° ê°œë°œ í™˜ê²½ êµ¬ì¶•

[Miniconda ì„¤ì¹˜ ê°€ì´ë“œ (ë¼ì´ì„ ìŠ¤ ë¬¸ì œ ì—†ì´ Jupyter Notebook ì‚¬ìš©í•˜ê¸°!) | ì½”ë“œì‡](https://www.codeit.kr/tutorials/150/miniconda-guide)

## **Installation Guide â€” Models**

```bash
# Create and activate conda environment
conda create -n ws_mdl python=3.10 -y
conda activate ws_mdl 

# Install PyTorch
# https://pytorch.org/get-started/locally/
conda install pytorch torchvision torchaudio torchtext torchmetrics pytorch-cuda=12.4 tensorboard spacy datasets tokenizers matplotlib ipykernel ipywidgets -c pytorch -c nvidia -y

git clone https://github.com/knowledge-intelligence/KIMe-VLA-Tutorial.git
```

## **Installation Guide â€” OpenVLA**

- **í™˜ê²½ êµ¬ì„±:** PyTorch 2.2.0, torchvision 0.17.0, transformers 4.40.1, tokenizers 0.19.1, timm 0.9.10, and flash-attn 2.5.5

### Set-up: Conda Env

```bash
# Create and activate conda environment
conda create -n openvla python=3.10 -y
conda activate openvla

# Install PyTorch.
# https://pytorch.org/get-started/locally/

# CUDA 12.1 (Conda)
conda install pytorch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 pytorch-cuda=12.1 -c pytorch -c nvidia -y 
# OR
# CUDA 12.1 (Pip) -Optional
pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu121 -q
```

### Set-up: Openvla Repo

```bash
# Clone and install the openvla repo
git clone https://github.com/openvla/openvla.git
cd openvla
pip install -e .
# openvla pip ì„¤ì¹˜ì´í›„ Pytorch CPU ë²„ì „ìœ¼ë¡œ downgrade í˜„ìƒ ì²´í¬
# openvla pip ì„¤ì¹˜ì´í›„ CUDA 12.1 (Conda) ì„¤ì¹˜ ì¶”ì²œ
conda install nvidia/label/cuda-12.1.0::cuda-nvcc
```

### Set-up(for Ubuntu): Ninja & Flash Attention 2

```bash
# Install Flash Attention 2 for training (https://github.com/Dao-AILab/flash-attention)
#   =>> If you run into difficulty, try `pip cache remove flash_attn` first
pip install packaging ninja
# ninja --version; echo $?  # Verify Ninja --> should return exit code "0"
# Linux GCC í•„ìˆ˜ (ìœˆë„ìš°ì—ì„œëŠ” MSVC í•„ìš”) 
pip install "flash-attn==2.5.5" --no-build-isolation
```

## **Installation Guide â€” Gr00t**

1. Git Repo ë³µì‚¬

```bash
git clone https://github.com/NVIDIA/Isaac-GR00T
cd Isaac-GR00T
```

1. Conda í™˜ê²½ êµ¬ì„± (Python 3.10)
    
    > âš ï¸CUDA 12.4 ë²„ì „ í•„ìˆ˜ (flash-attn ë•Œë¬¸ì—)
    > 

```bash
conda create -n gr00t python=3.10
conda activate gr00t
pip install --upgrade setuptools
pip install -e .[base]
conda install nvidia/label/cuda-12.4.0::cuda-nvcc
pip install --no-build-isolation flash-attn==2.7.1.post4
```

# 2. Autoregressive (Transformer) ëª¨ë¸

[Autoregressive (Transformer) ëª¨ë¸ ê¸°ì´ˆ ì´ë¡ ](https://www.notion.so/Autoregressive-Transformer-24f875de525f8190a6d4ed32f8b8ffe1?pvs=21)

[Transformer â€” Python ì½”ë“œ ì„¤ëª… (tformer > model.py)](https://www.notion.so/Transformer-Python-tformer-model-py-24f875de525f8138a732ef6c31d791f9?pvs=21)

[Transformer â€” Python ì½”ë“œ ì„¤ëª… (Training)](https://www.notion.so/Transformer-Python-Training-24f875de525f814c9381eb3fb81864f4?pvs=21)

[Transformer â€” Python ì½”ë“œ ê¸°ë°˜ ì‹¤ìŠµ (í›ˆë ¨) Check Point](https://www.notion.so/Transformer-Python-Check-Point-24f875de525f81d1b253cd9d721d2d82?pvs=21)

# 3. Diffusion ëª¨ë¸

[Diffusion ëª¨ë¸ ê¸°ì´ˆ ì´ë¡ ](https://www.notion.so/Diffusion-24f875de525f81839591feaa1fe7a392?pvs=21)

[Diffusion â€” Python ì½”ë“œ ê¸°ë°˜ ì‹¤ìŠµ (Sprite & MNIST)](https://www.notion.so/Diffusion-Python-Sprite-MNIST-24f875de525f8137888fe7f6ccf5f78e?pvs=21)

# 4. OpenVLA & LIBERO ì†Œê°œ ë° ê°œë°œí™˜ê²½ êµ¬ì¶•

[OpenVLA ë° LIBERO í™˜ê²½ êµ¬ì¶•](https://www.notion.so/OpenVLA-LIBERO-24f875de525f813984b9f84b75e5d5fb?pvs=21)

[OpenVLA ì†Œê°œ](https://www.notion.so/OpenVLA-24f875de525f81abb3f7d05a605dd86f?pvs=21)

[LIBERO ì†Œê°œ](https://www.notion.so/LIBERO-24f875de525f81678c1ac06a756f4e45?pvs=21)

# 5. OpenVLA ë° LIBERO ì‹¤ìŠµ

[FastAPI êµ¬ì¶• ì‹¤ìŠµ](https://www.notion.so/FastAPI-24f875de525f81eaa068d001fa8aecae?pvs=21)

[LIBERO+OpenVLA Jupyter ì‹¤ìŠµ](https://www.notion.so/LIBERO-OpenVLA-Jupyter-24f875de525f8182bbd9d8aee7ddbb08?pvs=21)

# 6.  GR00T N1.5 êµ¬ë™ ì‹¤ìŠµ

[GR00T N1.5 Tutorial](https://www.notion.so/GR00T-N1-5-Tutorial-24f875de525f817d92f2f5d42a459434?pvs=21)
#####################################  

# Autoregressive (Transformer) ëª¨ë¸ ê¸°ì´ˆ ì´ë¡ 

# **íŠ¸ëœìŠ¤í¬ë¨¸ ê°œìš”**

Recurrent Neural Networks (RNNs)

- Long Short-Term Memory (LSTM) networks
- Gated Recurrent Units (GRUs)

![image.png](attachment:61ad044d-5033-4fb9-8473-33a6b3fc3bba:image.png)

**RNNì˜ ë¬¸ì œì **

- **ê¸´ ì‹œí€€ìŠ¤ì˜ ëŠë¦° ê³„ì‚°**: ê° í† í°ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” for-loopë¡œ ì¸í•´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼.
- **ê¸°ìš¸ê¸° ì†Œì‹¤/í­ë°œ**: ì†ì‹¤ í•¨ìˆ˜ì˜ ë¯¸ë¶„ì„ ì²´ì¸ ë£°ë¡œ ê³„ì‚° ì‹œ, ìˆ¨ê²¨ì§„ ì¸µì´ ë§ì•„ì§ˆìˆ˜ë¡ ê¸°ìš¸ê¸°ê°€ ë§¤ìš° ì‘ê±°ë‚˜ ì»¤ì ¸ CPU/GPUì˜ ì •ë°€ë„ í•œê³„ë¡œ ì¸í•´ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ê°€ ë¹„íš¨ìœ¨ì ì„.
- **ì´ˆê¸° ì •ë³´ ì ‘ê·¼ ì–´ë ¤ì›€**: ê¸´ ì˜ì¡´ì„± ì²´ì¸ìœ¼ë¡œ ì¸í•´ ì´ˆê¸° í† í°ì˜ ìˆ¨ê²¨ì§„ ìƒíƒœ ì˜í–¥ì´ ì‹œí€€ìŠ¤ í›„ë°˜ë¶€ì—ì„œ ì•½í™”ë˜ì–´ ì¥ê±°ë¦¬ ì˜ì¡´ì„± ìœ ì§€ê°€ ì–´ë ¤ì›€.

**íŠ¸ëœìŠ¤í¬ë¨¸ â€” ì¸ì½”ë”**ì™€ **ë””ì½”ë”** ë¸”ë¡ìœ¼ë¡œ **í•´ê²°**

- **Transformer ëª¨ë¸ í‚¤ í¬ì¸íŠ¸**
    - Embedding
    - Encoding
    - Attention
    - Self-Attention
    - Masked
    - Encoder-Decoder
    - Multi-Head

![image.png](attachment:1a63e6eb-294d-441a-96a5-2b3884e4cd16:image.png)

# **Encoder**

![image.png](attachment:5debec99-728f-454c-a0d7-26ae3c85fc72:dff6e8cf-f172-4eb4-956b-5ba53e15786d.png)

## **Input Embeddings**

![image.png](attachment:2cf74eb7-f0c9-472c-9ce7-9c6bcb2b5abc:image.png)

**íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ì˜ ì…ë ¥ ì²˜ë¦¬ ê³¼ì •**

- **ì…ë ¥ ë¬¸ì¥**: ì˜ˆ: â€œThe quick brown fox jumps overâ€ (6ë‹¨ì–´ ë¬¸ì¥).
- **í† í°í™”**: ë¬¸ì¥ì„ ê°œë³„ í† í°ìœ¼ë¡œ ë‚˜ëˆ”.
- **ì…ë ¥ ID ìƒì„±**: ê° í† í°ì„ ì–´íœ˜ ì‚¬ì „ ë‚´ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ«ìë¡œ ë§¤í•‘.
- **ì„ë² ë”© ë³€í™˜**: ì…ë ¥ IDë¥¼ 512ì°¨ì› ë²¡í„°(ì„ë² ë”©)ë¡œ ë³€í™˜.
- **ì„ë² ë”© í•™ìŠµ**: ì„ë² ë”©ì€ ê³ ì •ë˜ì§€ ì•Šê³ , ëª¨ë¸ì´ ë‹¨ì–´ ì˜ë¯¸ë¥¼ í•™ìŠµí•˜ë©° í›ˆë ¨ ê³¼ì •ì—ì„œ ì¡°ì •ë¨.
- **ì…ë ¥ ID ê³ ì •**: ì…ë ¥ IDëŠ” í›ˆë ¨ ì¤‘ì—ë„ ë³€í•˜ì§€ ì•ŠìŒ.

## **Positional Encoding**

![image.png](attachment:5f775783-5808-42a4-9bd1-589be17ff7fc:image.png)

**ìœ„ì¹˜ ì¸ì½”ë”©(Position Encoding)ì˜ ì—­í• **

- **ë‹¨ì–´ ì„ë² ë”©ì˜ í•œê³„**: ë‹¨ì–´ì˜ ì˜ë¯¸ëŠ” ì „ë‹¬í•˜ì§€ë§Œ, ë¬¸ì¥ì—ì„œ ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ëŠ” ì œê³µí•˜ì§€ ì•ŠìŒ.
- **ìœ„ì¹˜ ì •ë³´ì˜ í•„ìš”ì„±**: ë‹¨ì–´ì˜ ë¬¸ì¥ ë‚´ ìœ„ì¹˜ë¥¼ ì•Œë ¤ì£¼ì–´, ê°€ê¹Œìš´ ë‹¨ì–´ëŠ” ë” ê´€ë ¨ ìˆê³  ë¨¼ ë‹¨ì–´ëŠ” ëœ ê´€ë ¨ ìˆìŒì„ ëª¨ë¸ì´ ì´í•´í•˜ë„ë¡ í•¨.
- **ê¸°ëŠ¥**: ê° ë‹¨ì–´ì˜ ìœ„ì¹˜ì— ëŒ€í•œ ê³µê°„ ì •ë³´ë¥¼ ì œê³µí•˜ì—¬ ë¬¸ì¥ êµ¬ì¡°ì˜ íŒ¨í„´ ì¸ì‹ì„ ë„ì›€.

![image.png](attachment:0d93b3f0-3f6d-400a-b205-96ff03ca1ef9:image.png)

**íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ìœ„ì¹˜ ì¸ì½”ë”© ê³„ì‚°**

- **ë‘ ê°€ì§€ ê³µì‹ ì‚¬ìš©**: ì›ë˜ íŠ¸ëœìŠ¤í¬ë¨¸ ë…¼ë¬¸ì— ë”°ë¼, ì§ìˆ˜ ìœ„ì¹˜ì™€ í™€ìˆ˜ ìœ„ì¹˜ì— ê°ê° ë‹¤ë¥¸ ê³µì‹ì„ ì ìš©.
- **í•œ ë²ˆ ê³„ì‚°, ì¬ì‚¬ìš©**: ìœ„ì¹˜ ì¸ì½”ë”©ì€ í•œ ë²ˆë§Œ ê³„ì‚°ë˜ë©°, í›ˆë ¨ê³¼ ì¶”ë¡  ì‹œ ëª¨ë“  ë¬¸ì¥ì— ì¬ì‚¬ìš©ë¨.

## **Multi-Head Attention**

![image.png](attachment:1d1a9a8b-f1e2-4906-afb4-f574e8f4b404:image.png)

**ì…€í”„ ì–´í…ì…˜(Self-Attention) ê°œìš”**

- **ì •ì˜**: íŠ¸ëœìŠ¤í¬ë¨¸ ì´ì „ì— ì¡´ì¬í–ˆë˜ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ, íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ë©€í‹°í—¤ë“œ ì–´í…ì…˜ìœ¼ë¡œ ë°œì „.
- **ê¸°ëŠ¥**: ë‹¨ì–´ ê°„ ê´€ê³„ë¥¼ í•™ìŠµí•´ ëª¨ë¸ì´ ë¬¸ë§¥ì„ ì´í•´í•˜ë„ë¡ ë•ëŠ”ë‹¤.
- **ì…ë ¥ ì„ë² ë”©**: ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ í¬ì°©.
- **ìœ„ì¹˜ ì¸ì½”ë”©**: ë¬¸ì¥ì—ì„œ ê° ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ì œê³µ.
- **ì…ë ¥ í–‰ë ¬ ì‚¬ìš©**: ì…ë ¥ í–‰ë ¬(6,512)ì„ Q(ì¿¼ë¦¬), K(í‚¤), V(ê°’)ìœ¼ë¡œ 3ë²ˆ ì‚¬ìš©.
- **ê³„ì‚° ê³¼ì •**:
    1. Q(6,512)ì™€ Káµ€(512,6)ë¥¼ ê³±í•¨.
    2. ê²°ê³¼ì— âˆšdâ‚–ë¡œ ë‚˜ëˆ”.
    3. ì†Œí”„íŠ¸ë§¥ìŠ¤(softmax) ì ìš©.
- **ê°’(`score`)ì˜ ì˜ë¯¸**: ë‹¨ì–´ ê°„ ê´€ê³„ì˜ ê°•ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì ìˆ˜.
- **íŠ¸ëœìŠ¤í¬ë¨¸ í•™ìŠµ**: ë‹¨ì–´ ê°„ ìƒí˜¸ì‘ìš©ì˜ ê°•ë„ë¥¼ ì ìˆ˜ë¡œ í‘œí˜„í•´ ì´í•´.

![image.png](attachment:2b25fbb1-6653-4da8-8d81-d56c01845ca1:image.png)

**íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì…€í”„ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ìš”ì•½**

- **ì–´í…ì…˜ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±**:
    - V í–‰ë ¬ê³¼ ê³±í•˜ì—¬ ì–´í…ì…˜ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ìƒì„±.
    - ì–´í…ì…˜ ë§¤íŠ¸ë¦­ìŠ¤ì˜ ì°¨ì›ì€ ì´ˆê¸° í–‰ë ¬ê³¼ ë™ì¼.
    - ê²°ê³¼ ì„ë² ë”©ì€ ë‹¨ì–´ì˜ ì˜ë¯¸, ìœ„ì¹˜, ê·¸ë¦¬ê³  ë‹¤ë¥¸ ë‹¨ì–´ì™€ì˜ ê´€ê³„ë¥¼ í¬í•¨.
- **ì…€í”„ ì–´í…ì…˜ì˜ íŠ¹ì§•**:
    - **ìˆœì—´ ë¶ˆë³€ì„±**: ì…ë ¥ ìˆœì„œì— ê´€ê³„ì—†ì´ ë™ì¼í•œ ê²°ê³¼ë¥¼ ìƒì„±.
    - **ëŒ€ê°ì„  ê°’ ê°•ì¡°**: ëŒ€ê°ì„  ë°©í–¥ì˜ ê°’ì´ ì¼ë°˜ì ìœ¼ë¡œ ê°€ì¥ ë†’ìŒ.
- **íŠ¹ì • ìœ„ì¹˜ ìƒí˜¸ì‘ìš© ì œì–´**:
    - ì†Œí”„íŠ¸ë§¥ìŠ¤ ì ìš© ì „, ì›ì¹˜ ì•ŠëŠ” ìœ„ì¹˜ì˜ ê°’ì„ -âˆë¡œ ì„¤ì • ê°€ëŠ¥.
    - ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ íŠ¹ì • ìœ„ì¹˜ ê°„ ìƒí˜¸ì‘ìš©ì„ í•™ìŠµí•˜ì§€ ì•Šë„ë¡ ì¡°ì •.

![image.png](attachment:6eff2eaa-49ef-4766-afa1-49d37f4331cd:image.png)

**ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ì˜ ë™ì‘ ê³¼ì •**

- **ì…ë ¥ ì„ë² ë”© ì²˜ë¦¬**:
    - ì…ë ¥ ì„ë² ë”©(seq, d_model)ì„ ë³µì‚¬í•´ Q, K, V 3ê°œ í–‰ë ¬ ìƒì„±.
    - ê° í–‰ë ¬ì— ê°€ì¤‘ì¹˜ í–‰ë ¬(WêŸ´, Wá´·, Wâ±½, í¬ê¸°: d_model, d_model)ì„ ê³±í•´ Qâ€™, Kâ€™, Vâ€™(seq, d_model) ìƒì„±.
- **í—¤ë“œ ë¶„í• **:
    - Qâ€™, Kâ€™, Vâ€™ë¥¼ d_model ì°¨ì›ì—ì„œ h(í—¤ë“œ ìˆ˜, ì—¬ê¸°ì„œëŠ” 4)ë¡œ ë‚˜ëˆ”.
    - dâ‚– = d_model/hë¡œ ê° í—¤ë“œì˜ ì°¨ì› ê³„ì‚°(dâ‚– = dáµ¥).
- **ì–´í…ì…˜ ê³„ì‚°**:
    - ê° í—¤ë“œì—ì„œ ë…¼ë¬¸ì˜ ì–´í…ì…˜ ê³µì‹ìœ¼ë¡œ ì‘ì€ í–‰ë ¬(head1, head2, head3, head4, í¬ê¸°: seq, dáµ¥) ê³„ì‚°.
- **í—¤ë“œ ë³‘í•©**:
    - ê° í—¤ë“œì˜ ê²°ê³¼ë¥¼ dáµ¥ ì°¨ì›ì—ì„œ ì—°ê²°(concatenate).
    - ì—°ê²°ëœ í–‰ë ¬ì— ê°€ì¤‘ì¹˜ í–‰ë ¬(h*dáµ¥, d_model)ì„ ê³±í•´ ìµœì¢… ì¶œë ¥(seq, d_model) ìƒì„±.

**ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ì˜ ì—­í• **

- **ë¶„í•  ê³„ì‚°**: Qâ€™, Kâ€™, Vâ€™ë¥¼ ì§ì ‘ ê³„ì‚°í•˜ì§€ ì•Šê³ , ì—¬ëŸ¬ í—¤ë“œë¡œ ë‚˜ëˆ„ì–´ ì‘ì€ í–‰ë ¬ë¡œ ì–´í…ì…˜ ê³„ì‚°.
- **ë‹¤ì–‘í•œ ê´€ì  í•™ìŠµ**: ê° í—¤ë“œëŠ” ë‹¨ì–´ì˜ ë‹¤ë¥¸ ì¸¡ë©´(ì˜ˆ: ëª…ì‚¬, ë™ì‚¬, ë¶€ì‚¬ ë“±)ì„ í•™ìŠµ.
    - ì˜ˆ: head1ì€ ë‹¨ì–´ë¥¼ ëª…ì‚¬ë¡œ, head2ëŠ” ë™ì‚¬ë¡œ í•´ì„í•˜ë©° ê´€ê³„ í•™ìŠµ.

**ì¥ì **: ë‹¨ì–´ì˜ ë‹¤ì–‘í•œ ë§¥ë½ì„ ë™ì‹œì— íŒŒì•…í•´ ë” í’ë¶€í•œ í‘œí˜„ í•™ìŠµ.

## **ADD & Norm (Layer Normalization)**

![image.png](attachment:466845ac-8b70-49ac-9f5c-a9500f0184c6:image.png)

**ë°°ì¹˜ ì •ê·œí™”(Batch Normalization) ìš”ì•½**

- **ì…ë ¥**: nê°œì˜ ì•„ì´í…œ ë°°ì¹˜, ê° ì•„ì´í…œì€ ì„ë² ë”©ê³¼ ê°™ì€ íŠ¹ì§•(feature)ì„ ê°€ì§.
- **ê³„ì‚°**: ê° ì•„ì´í…œì˜ íŠ¹ì§•ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ í‰ê· ê³¼ ë¶„ì‚°ì„ ê³„ì‚°.
- **ì •ê·œí™”**: ê° ê°’ì„ ë‹¤ìŒ ì‹ìœ¼ë¡œ ë³€í™˜:
    - ê³±ì…ˆ íŒŒë¼ë¯¸í„°(ğ›¾ ë˜ëŠ” ğœ¶)ë¡œ ìŠ¤ì¼€ì¼ë§.
    - ë§ì…ˆ íŒŒë¼ë¯¸í„°(ğ›½)ë¡œ ì´ë™.
- **ìˆ˜ì¹˜ ì•ˆì •ì„±**: ë¶„ëª¨ê°€ 0ì— ê°€ê¹Œì›Œì§€ì§€ ì•Šë„ë¡ ì‘ì€ ê°’(ğœ–)ì„ ì¶”ê°€.

# **Decoder**

![image.png](attachment:6652250a-202f-4c45-aa9b-ee7655386c38:d24733bb-5581-4346-b46c-0231f4880d7a.png)

## **Masked Multi-Head Attention**

![image.png](attachment:42b48f28-eedf-40dc-9eab-576412773ac9:image.png)

**íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¸ê³¼ì„± ë³´ì¥ ë° ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ìš”ì•½**

- **ì¸ê³¼ì„± ë³´ì¥**
    - ëª¨ë¸ì´ í˜„ì¬ ìœ„ì¹˜ì˜ ì¶œë ¥ì´ ì´ì „ ìœ„ì¹˜ì˜ ë‹¨ì–´ì—ë§Œ ì˜ì¡´í•˜ë„ë¡ í•¨.
    - ë¯¸ë˜ ë‹¨ì–´ë¥¼ ë³´ì§€ ëª»í•˜ê²Œ **ë§ˆìŠ¤í‚¹** ì ìš©.
    - ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì—ì„œ ì£¼ëŒ€ê°ì„  ìœ„ ê°’ë“¤ì„ **ìŒì˜ ë¬´í•œëŒ€**ë¡œ ëŒ€ì²´ í›„ ì†Œí”„íŠ¸ë§¥ìŠ¤ ì ìš©.
- **ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ìƒí˜¸ì‘ìš©**
    - ì¸ì½”ë” ì¶œë ¥: **í‚¤(Key)**ì™€ **ê°’(Value)** ìƒì„±.
    - ë””ì½”ë”ì˜ ë©€í‹°í—¤ë“œ ì–´í…ì…˜:
        - **í¬ë¡œìŠ¤ ì…€í”„ ì–´í…ì…˜** ì‚¬ìš©.
        - í‚¤ì™€ ê°’ì€ ì¸ì½”ë”ì—ì„œ, ì¿¼ë¦¬ í–‰ë ¬ì€ ë””ì½”ë” ì…ë ¥(ë§ˆìŠ¤í¬ëœ ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ì²˜ë¦¬ í›„)ì—ì„œ ìƒì„±.

## **Feed Forward**

**í”¼ë“œí¬ì›Œë“œ ë¸”ë¡ì˜ ë™ì‘ ìš”ì•½**

- **ì„ í˜• ë³€í™˜ 1**: ì…ë ¥ í…ì„œ(í˜•ìƒ: batch_size, seq, d_model)ë¥¼ ì™„ì „ ì—°ê²° ì„ í˜• ì¸µì— í†µê³¼ì‹œì¼œ ë” í° ì°¨ì›ì˜ í…ì„œ(í˜•ìƒ: batch_size, seq, d_ff)ë¡œ ë³€í™˜.
- **ReLU í™œì„±í™”**: ì²« ë²ˆì§¸ ì„ í˜• ì¸µì˜ ì¶œë ¥ì„ ReLU í•¨ìˆ˜ì— ìš”ì†Œë³„ë¡œ ì ìš©í•´ ë¹„ì„ í˜•ì„±ì„ ì¶”ê°€.
- **ì„ í˜• ë³€í™˜ 2**: ReLU ì¶œë ¥ í…ì„œë¥¼ ë˜ ë‹¤ë¥¸ ì„ í˜• ì¸µì— í†µê³¼ì‹œì¼œ ì›ë˜ ì°¨ì›(batch_size, seq, d_model)ìœ¼ë¡œ ë˜ëŒë¦¼.
- **íŠ¹ì§•**: í”¼ë“œí¬ì›Œë“œ ë¸”ë¡ì€ ë³µì¡í•œ ë¹„ì„ í˜• ë³€í™˜ì„ í•™ìŠµí•˜ë©°, ì‹œí€€ìŠ¤ì˜ ê° ìœ„ì¹˜ì— ë…ë¦½ì ìœ¼ë¡œ ì ìš©ë˜ì–´ ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥í•˜ê³  íš¨ìœ¨ì ì„.

# **Training**

![image.png](attachment:6874c962-3bf9-4ed5-9cb8-6e95ff246dab:image.png)

**ì˜ì–´-í”„ë‘ìŠ¤ì–´ ë²ˆì—­ì„ ìœ„í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ë™ì‘ ê³¼ì •**

- **ì…ë ¥ ì¤€ë¹„**
    - ì˜ì–´ ë¬¸ì¥(â€œI love you so muchâ€)ì— ì‹œì‘ í† í°(SOS)ê³¼ ì¢…ë£Œ í† í°(EOS)ì„ ì¶”ê°€.
    - ë¬¸ì¥ì„ ì…ë ¥ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ í›„, ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€.
- **ì¸ì½”ë” ì²˜ë¦¬**
    - ì…ë ¥ì„ ì¸ì½”ë”ì— ì „ë‹¬.
    - ì¶œë ¥: (ì‹œí€€ìŠ¤ ê¸¸ì´, d_model) í¬ê¸°ì˜ í–‰ë ¬.
    - ê° ë‹¨ì–´ì˜ ì˜ë¯¸, ìœ„ì¹˜, ë‹¨ì–´ ê°„ ê´€ê³„ë¥¼ í¬í•¨í•œ ì„ë² ë”© ìƒì„±.
- **ë””ì½”ë” ì²˜ë¦¬**
    - ì‹œì‘ í† í°(SOS)ì„ ë””ì½”ë”ì— ì…ë ¥.
    - ì¶œë ¥(ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™)ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜, ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€.
    - ë§ˆìŠ¤í¬ëœ ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ì— ì „ë‹¬.
    - ì¸ì½”ë” ì¶œë ¥(ì¿¼ë¦¬ì™€ í‚¤) + ë§ˆìŠ¤í¬ëœ ì–´í…ì…˜ ì¶œë ¥(ê°’)ì„ ë””ì½”ë”ì˜ ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ì— ì…ë ¥.
    - ì¶œë ¥: (ì‹œí€€ìŠ¤ ê¸¸ì´, d_model) í¬ê¸°ì˜ í–‰ë ¬.
- **ì¶œë ¥ ìƒì„±**
    - ë””ì½”ë” ì¶œë ¥ì„ ì„ í˜• ì¸µìœ¼ë¡œ (ì‹œí€€ìŠ¤ ê¸¸ì´, vocab_size)ë¡œ ë³€í™˜.
    - ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¡œ ê° í† í°ì˜ í™•ë¥  ê³„ì‚°.
    - ì˜ˆì¸¡ëœ í”„ë‘ìŠ¤ì–´ ë¬¸ì¥(â€œje tâ€™aime tellementâ€) ìƒì„±.
- **í•™ìŠµ**
    - ëª¨ë¸ ì¶œë ¥ê³¼ ëª©í‘œ í”„ë‘ìŠ¤ì–´ ë¬¸ì¥ì„ ë¹„êµí•´ ì†ì‹¤ ê³„ì‚°.
    - ì—­ì „íŒŒë¡œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸.
    - ì „ì²´ ê³¼ì •ì€ ë‹¨ì¼ ì‹œê°„ ë‹¨ê³„ì—ì„œ ìˆ˜í–‰.

# **Inference**

![image.png](attachment:da876da5-c058-454d-b62b-c71b2b9b3963:image.png)

**íŠ¸ëœìŠ¤í¬ë¨¸ ë²ˆì—­ ê³¼ì • ìš”ì•½**

- **ì¸ì½”ë” ë™ì‘**
    - ì…ë ¥: ë¬¸ì¥ì˜ ì‹œì‘(SOS)ê³¼ ë(EOS) í† í°ì„ í¬í•¨í•œ ì˜ì–´ ë¬¸ì¥.
    - ì¶œë ¥: ì¸ì½”ë”ëŠ” ì¶œë ¥ í–‰ë ¬ì„ ìƒì„±.
    - íŠ¹ì§•: ë™ì¼í•œ ì˜ì–´ ë¬¸ì¥ì´ë¯€ë¡œ ë§¤ íƒ€ì„ìŠ¤í…ì—ì„œ ì¬ê³„ì‚° ë¶ˆí•„ìš”.
- **ë””ì½”ë” ë™ì‘ (íƒ€ì„ìŠ¤í… 1)**
    - ì…ë ¥: SOS í† í°ë§Œ ë””ì½”ë”ì— ì…ë ¥.
    - ê³¼ì •: ë””ì½”ë” ì¶œë ¥ â†’ ì„ í˜• ì¸µ â†’ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ ì ìš© â†’ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ì–´íœ˜ í† í° ì„ íƒ.
    - ê²°ê³¼: ë²ˆì—­ëœ ë¬¸ì¥ì˜ ì²« ë²ˆì§¸ í† í° ìƒì„±.
- **ë””ì½”ë” ë™ì‘ (íƒ€ì„ìŠ¤í… 2 ì´í›„)**
    - ì…ë ¥: ì´ì „ íƒ€ì„ìŠ¤í…ì—ì„œ ì˜ˆì¸¡ëœ í† í°ì„ ë””ì½”ë” ì…ë ¥ì— ì¶”ê°€.
    - ê³¼ì •: ë””ì½”ë”ì— ì…ë ¥í•˜ì—¬ ë‹¤ìŒ í† í° ìƒì„±, EOS í† í°ì´ ë‚˜ì˜¬ ë•Œê¹Œì§€ ë°˜ë³µ.
- **ë””ì½”ë”© ì „ëµ**
    - **ê·¸ë¦¬ë”” ì„œì¹˜**: ê° ë‹¨ê³„ì—ì„œ ê°€ì¥ ë†’ì€ ì†Œí”„íŠ¸ë§¥ìŠ¤ í™•ë¥ ì˜ ë‹¨ì–´ë¥¼ ì„ íƒ.
        - ë‹¨ì : ìµœì ì˜ ê²°ê³¼ë¥¼ ë³´ì¥í•˜ì§€ ì•ŠìŒ.
    - **ë¹” ì„œì¹˜**: ê° ë‹¨ê³„ì—ì„œ ìƒìœ„ Bê°œì˜ ë‹¨ì–´ë¥¼ ê³ ë ¤, ê°€ëŠ¥í•œ ë‹¤ìŒ ë‹¨ì–´ ì¡°í•©ì„ í‰ê°€í•˜ì—¬ ìƒìœ„ Bê°œì˜ ì‹œí€€ìŠ¤ ìœ ì§€.
        - ì¥ì : ì—¬ëŸ¬ ì‹œí€€ìŠ¤ë¥¼ ë™ì‹œì— íƒìƒ‰í•´ ë” ë‚˜ì€ ê²°ê³¼ ìƒì„±.

# ì°¸ê³ ìë£Œ

[Mastering Transformer: Detailed Insights into Each Block](https://medium.com/@sayedebad.777/mastering-transformer-detailed-insights-into-each-block-and-their-math-4221c6ee0076)

##################################################  

# Transformer â€” Python ì½”ë“œ ì„¤ëª… (tformer > model.py)

![image.png](attachment:1a63e6eb-294d-441a-96a5-2b3884e4cd16:image.png)

# **Input Embeddings**

```python
class InputEmbeddings(nn.Module):
    def __init__(self, d_model: int, vocab_size: int):
        super().__init__()
        self.d_model = d_model
        self.vocab_size = vocab_size
        self.embedding = nn.Embedding(vocab_size, d_model)

    def forward(self, x):
        return self.embedding(x) * math.sqrt(self.d_model)
```

**ì…ë ¥ ì„ë² ë”© ìƒì„± ê³¼ì •**

- **ë¬¸ì¥ â†’ ì…ë ¥ ID ë³€í™˜**: ë¬¸ì¥ì„ ë‹¨ì–´ ì‚¬ì „ ë‚´ ê° ë‹¨ì–´ì˜ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ«ì(ì…ë ¥ ID)ë¡œ ë³€í™˜.
- **ì…ë ¥ ID â†’ ì„ë² ë”© ë§¤í•‘**: ê° ì…ë ¥ IDë¥¼ 512ì°¨ì› ë²¡í„°ì¸ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜.

### ğŸ”¹ ê¸°ë³¸ ê°œë…

`InputEmbeddings` í´ë˜ìŠ¤ëŠ” PyTorchì˜ `nn.Module`ì„ ìƒì†ë°›ì€ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

(â€» `nn.Module`ì€ ëª¨ë“  ì‹ ê²½ë§ ëª¨ë“ˆì˜ ê¸°ë³¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.)

---

### ğŸ”¸ ìƒì„±ì (`__init__` ë©”ì„œë“œ)

### ğŸ“¥ ì…ë ¥ íŒŒë¼ë¯¸í„°

1. **`d_model`**: ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì› (ì˜ˆ: 512, 768 ë“±)
2. **`vocab_size`**: ë‹¨ì–´ ì§‘í•©(ì–´íœ˜)ì˜ í¬ê¸° (ê³ ìœ í•œ í† í° ê°œìˆ˜)

### ğŸ§± ë‚´ë¶€ ì†ì„±

- `self.d_model`: ì„ë² ë”© ì°¨ì›ì„ ì €ì¥
- `self.vocab_size`: ì–´íœ˜ í¬ê¸°ë¥¼ ì €ì¥
- `self.embedding`: `nn.Embedding`ì„ ì´ìš©í•´ ì„ë² ë”© ë ˆì´ì–´ ì´ˆê¸°í™”
    
    â†’ ê° í† í°ì„ `d_model` ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜í•´ì¤Œ
    

---

### ğŸ”¸ `forward` ë©”ì„œë“œ (ìˆœì „íŒŒ)

### ğŸ“¥ ì…ë ¥

- **`x`**: í† í° ì¸ë±ìŠ¤(ì˜ˆ: ë¬¸ì¥ì„ í† í°í™”í•œ í›„ ì–»ì€ IDë“¤)

### âš™ï¸ ë™ì‘

- `self.embedding(x)`: ì…ë ¥ëœ í† í° ì¸ë±ìŠ¤ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
    
    â†’ ì¶œë ¥ í˜•íƒœ: `(ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´, d_model)`
    
- `math.sqrt(self.d_model)`: ì„ë² ë”© ë²¡í„°ì— **âˆšd_model**ì„ ê³±í•´ì¤Œ
    
    â†’ í•™ìŠµ ì‹œ **gradient ì•ˆì •í™”**ë¥¼ ìœ„í•´ ì‚¬ìš©ë˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¼ë°˜ì ì¸ ê¸°ë²•
    
    â†’ ìœ„ì¹˜ ì¸ì½”ë”© ë“± ë‹¤ë¥¸ ë²¡í„°ì™€ ë”í•  ë•Œ **ë¶„ì‚°ì„ ì¼ì •í•˜ê²Œ ìœ ì§€**í•˜ë ¤ëŠ” ëª©ì 
    

### âœ… í•µì‹¬ ìš”ì•½

- ì…ë ¥ í† í° IDë¥¼ ê³ ì°¨ì› ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
- ë³€í™˜ëœ ë²¡í„°ëŠ” âˆšd_modelë¡œ ìŠ¤ì¼€ì¼ë§ë¨
- íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì—ì„œ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš©ë¨

# **Positional Encoding**

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, seq: int, dropout: float) -> None:
        super().__init__()
        self.d_model = d_model
        self.seq = seq
        self.dropout = nn.Dropout(dropout)

        # Create a matrix of shape (seq, d_model)
        pe = torch.zeros(seq, d_model)

        # Create a vector of shape (seq)
        position = torch.arange(0, seq, dtype=torch.float).unsqueeze(1) # (seq, 1)

        # Create a vector of shape (d_model)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)

        # Apply sine to even indices
        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))

        # Apply cosine to odd indices
        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))

        # Add a batch dimension to the positional encoding
        pe = pe.unsqueeze(0) # (1, seq, d_model)

        # Register the positional encoding as a buffer
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq, d_model)
        return self.dropout(x)
```

**ìœ„ì¹˜ ì¸ì½”ë”©(Position Encoding)ì˜ ì—­í• ê³¼ êµ¬í˜„**

- **ëª©ì **: ë¬¸ì¥ ë‚´ ê° ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ëª¨ë¸ì— ì „ë‹¬.
- **ë°©ë²•**: ë‹¨ì–´ ì„ë² ë”©ê³¼ ë™ì¼í•œ í¬ê¸°ì˜ ë²¡í„°ë¥¼ ì¶”ê°€í•´ ìœ„ì¹˜ ì •ë³´ í‘œí˜„.
    - **ë²¡í„° ìƒì„±**: íŠ¸ëœìŠ¤í¬ë¨¸ ë…¼ë¬¸ì˜ ë‘ ê³µì‹ ì‚¬ìš©.
        - ì§ìˆ˜ ìœ„ì¹˜: ì²« ë²ˆì§¸ ê³µì‹ ì ìš©.
        - í™€ìˆ˜ ìœ„ì¹˜: ë‘ ë²ˆì§¸ ê³µì‹ ì ìš©.
    - **ë²¡í„° í¬ê¸°**: ì˜ˆ: ë‹¨ì–´ 3ê°œì¼ ë•Œ, ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ê¹Œì§€ ê° ìœ„ì¹˜ì— ëŒ€í•´ 512ì°¨ì› ë²¡í„° ìƒì„±.
- **ìˆ˜ì¹˜ ì•ˆì •ì„±**: ë¡œê·¸ ê³µê°„(log space) ì‚¬ìš© (e^{ln(n)} = n).

### ğŸ”¹ ê¸°ë³¸ ê°œë…

ì´ í´ë˜ìŠ¤ëŠ” **ì…ë ¥ ì„ë² ë”©ì— ìœ„ì¹˜ ì •ë³´ë¥¼ ë”í•´ì£¼ëŠ” ì—­í• **ì„ í•©ë‹ˆë‹¤.

íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ìˆœì„œ ì •ë³´ê°€ ì—†ê¸° ë•Œë¬¸ì—, ì´ ìœ„ì¹˜ ì¸ì½”ë”©ì´ í•„ìˆ˜ì…ë‹ˆë‹¤.

---

### ğŸ”¸ ìƒì„±ì (`__init__` ë©”ì„œë“œ)

### ğŸ“¥ ì…ë ¥ íŒŒë¼ë¯¸í„°

1. **`d_model`**: ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›
2. **`seq`**: ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ ê¸¸ì´ (ì˜ˆ: ë¬¸ì¥ì˜ ìµœëŒ€ í† í° ìˆ˜)
3. **`dropout`**: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ (ê³¼ì í•© ë°©ì§€)

### ğŸ§± ë‚´ë¶€ ì†ì„±

- `self.d_model`: ì„ë² ë”© ì°¨ì›ì„ ì €ì¥
- `self.seq`: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì €ì¥
- `self.dropout`: ì§€ì •ëœ ë¹„ìœ¨ë¡œ ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´ ìƒì„±

---

### ğŸ”¸ ìœ„ì¹˜ ì¸ì½”ë”© í–‰ë ¬ ìƒì„±

### ğŸ“ í–‰ë ¬ êµ¬ì„±

- `pe`: `(seq, d_model)` í¬ê¸°ì˜ 0ìœ¼ë¡œ ì±„ì›Œì§„ í…ì„œ (ìœ„ì¹˜ ì¸ì½”ë”© ì €ì¥ìš©)
- `position`: `0 ~ seq-1`ê¹Œì§€ì˜ ìœ„ì¹˜ ì¸ë±ìŠ¤ë¥¼ (seq, 1) í˜•íƒœë¡œ ì €ì¥
- `div_term`: ìŠ¤ì¼€ì¼ë§ ìš©ë„ë¡œ ì‚¬ìš©ë˜ëŠ” ê°’
    - ê³„ì‚°ì‹: `10000^(2i / d_model)`
    - ì—¬ê¸°ì„œ iëŠ” ì„ë² ë”© ì°¨ì› ì¸ë±ìŠ¤

### ğŸµ ê³µì‹ ì ìš© (ë…¼ë¬¸ ê¸°ë°˜)

- `pe[:, 0::2]`: ì§ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” **ì‚¬ì¸(sin)** í•¨ìˆ˜ ê°’ ì €ì¥
- `pe[:, 1::2]`: í™€ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” **ì½”ì‚¬ì¸(cos)** í•¨ìˆ˜ ê°’ ì €ì¥

---

### ğŸ”¸ ë°°ì¹˜ ì°¨ì› ì¶”ê°€ ë° ë²„í¼ ë“±ë¡

- `pe = pe.unsqueeze(0)`
    
    â†’ í¬ê¸°ë¥¼ `(1, seq, d_model)`ë¡œ ë³€ê²½ â†’ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´
    
- `self.register_buffer('pe', pe)`
    
    â†’ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ëŠ” ì•„ë‹ˆì§€ë§Œ ëª¨ë¸ ìƒíƒœì— ì €ì¥ë¨
    
    â†’ GPU ë“±ìœ¼ë¡œ ëª¨ë¸ ì´ë™ ì‹œ ìë™ í¬í•¨ë¨
    

---

### ğŸ”¸ ìˆœì „íŒŒ (`forward` ë©”ì„œë“œ)

### âš™ï¸ ë™ì‘ ê³¼ì •

1. `x + positional encoding`
    - ì…ë ¥ ì„ë² ë”© xì— ìœ„ì¹˜ ì¸ì½”ë”©ì„ ë”í•¨
    - `requires_grad_(False)`ë¡œ ìœ„ì¹˜ ì¸ì½”ë”©ì€ **í•™ìŠµë˜ì§€ ì•Šë„ë¡ ì„¤ì •**
2. `return self.dropout(x)`
    - ë“œë¡­ì•„ì›ƒ ì ìš© â†’ **ê³¼ì í•© ë°©ì§€**
    - 

### âœ… í•µì‹¬ ìš”ì•½

- ìœ„ì¹˜ ì •ë³´ë¥¼ ì„ë² ë”©ì— ë”í•´ ìˆœì„œë¥¼ ì¸ì‹í•  ìˆ˜ ìˆê²Œ í•¨
- ì‚¬ì¸/ì½”ì‚¬ì¸ í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ê·œì¹™ì ì¸ íŒ¨í„´ìœ¼ë¡œ ì¸ì½”ë”©
- ë“œë¡­ì•„ì›ƒì„ í†µí•´ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ

# **Add Norm (Layer Normalization)**

```python
class LayerNormalization(nn.Module):

    def __init__(self, features: int, eps:float=10**-6) -> None:
        super().__init__()
        self.eps = eps
        self.alpha = nn.Parameter(torch.ones(features)) # alpha (multiplicative) is a learnable parameter
        self.bias = nn.Parameter(torch.zeros(features)) # bias (additive) is a learnable parameter

    def forward(self, x):
        # x: (batch, seq, hidden_size)
        # Keep the dimension for broadcasting
        mean = x.mean(dim = -1, keepdim = True) # (batch, seq, 1)
        # Keep the dimension for broadcasting
        std = x.std(dim = -1, keepdim = True) # (batch, seq, 1)
        # eps is to prevent dividing by zero or when std is very small
        return self.alpha * (x - mean) / (std + self.eps) + self.bias
```

### ğŸ”¹ ê¸°ë³¸ ê°œë…

- *ë ˆì´ì–´ ì •ê·œí™”(Layer Normalization)**ëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ ì •ê·œí™”í•˜ì—¬ í•™ìŠµì„ ë” ì•ˆì •ì ìœ¼ë¡œ ë§Œë“œëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œëŠ” **ëª¨ë“  ì„œë¸Œë ˆì´ì–´(ì˜ˆ: Self-Attention, Feedforward)** ë’¤ì— ë ˆì´ì–´ ì •ê·œí™”ë¥¼ ì ìš©í•©ë‹ˆë‹¤.

---

### ğŸ”¸ ë‚´ë¶€ ì†ì„± (Attributes)

1. **`self.eps`**
    - ì •ê·œí™” ê³¼ì •ì—ì„œ **0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒ ë°©ì§€**ë¥¼ ìœ„í•œ ì•„ì£¼ ì‘ì€ ê°’ (ì˜ˆ: 1e-5)
2. **`self.alpha`**
    - í•™ìŠµ ê°€ëŠ¥í•œ **ìŠ¤ì¼€ì¼(scale)** íŒŒë¼ë¯¸í„°
    - ì´ˆê¸°ê°’ì€ `1`ì´ê³ , ëª¨ì–‘ì€ `(features,)`
3. **`self.bias`**
    - í•™ìŠµ ê°€ëŠ¥í•œ **ì´ë™(shift)** íŒŒë¼ë¯¸í„°
    - ì´ˆê¸°ê°’ì€ `0`, ëª¨ì–‘ì€ `(features,)`

---

### ğŸ”¸ ìˆœì „íŒŒ (`forward` ë©”ì„œë“œ) ê³¼ì •

### ğŸ“¥ ì…ë ¥

- `x`: ëª¨ì–‘ì´ `(batch_size, seq_len, hidden_size)`ì¸ í…ì„œ
    
    (í•˜ë‚˜ì˜ ë°°ì¹˜ ë‚´ ë¬¸ì¥ë“¤ì˜ ì„ë² ë”© ë²¡í„°)
    

---

### ğŸ“Š í‰ê·  ê³„ì‚°

```python
mean = x.mean(dim=-1, keepdim=True)
```

- **ë§ˆì§€ë§‰ ì°¨ì›(hidden_size)** ê¸°ì¤€ìœ¼ë¡œ í‰ê·  ê³„ì‚°
- ëª¨ì–‘: `(batch_size, seq_len, 1)`
- `keepdim=True`ëŠ” **ë¸Œë¡œë“œìºìŠ¤íŒ…ì„ ì‰½ê²Œ í•˜ê¸° ìœ„í•´** ì°¨ì›ì„ ìœ ì§€

---

### ğŸ“‰ í‘œì¤€í¸ì°¨ ê³„ì‚°

```python
std = x.std(dim=-1, keepdim=True)
```

- ë§ˆì§€ë§‰ ì°¨ì› ê¸°ì¤€ìœ¼ë¡œ í‘œì¤€í¸ì°¨ ê³„ì‚°
- ëª¨ì–‘ì€ í‰ê· ê³¼ ë™ì¼í•˜ê²Œ `(batch_size, seq_len, 1)`

---

### ğŸ”„ ì •ê·œí™”

```python
normed = (x - mean) / (std + self.eps)
```

- í‰ê· ì„ ë¹¼ê³ , í‘œì¤€í¸ì°¨ + Îµë¡œ ë‚˜ëˆ„ì–´ ì •ê·œí™”
- **ë¶„ì‚°ì´ ë„ˆë¬´ ì‘ì„ ë•Œì˜ ë¶ˆì•ˆì •ì„±**ì„ `eps`ë¡œ ë°©ì§€

---

### ğŸ”§ ìŠ¤ì¼€ì¼ & ì‹œí”„íŠ¸

```python
output = self.alpha * normed + self.bias
```

- ì •ê·œí™”ëœ ê²°ê³¼ë¥¼ `self.alpha`ë¡œ **ìŠ¤ì¼€ì¼ ì¡°ì •**
- `self.bias`ë¡œ **ê°’ ì´ë™**
- ì´ ë‘˜ì€ **í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°**ë¼ ëª¨ë¸ì´ ìµœì ì˜ ì •ê·œí™” í˜•íƒœë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŒ
- 

### âœ… í•µì‹¬ ìš”ì•½

- ë§ˆì§€ë§‰ ì°¨ì›(hidden_size)ì„ ê¸°ì¤€ìœ¼ë¡œ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ê³„ì‚°í•˜ì—¬ ì •ê·œí™”
- ì •ê·œí™”ëœ ê°’ì— **ìŠ¤ì¼€ì¼(alpha)** ê³¼ **ì´ë™(bias)** ì„ ì ìš©
- í•™ìŠµ ì•ˆì •ì„±ê³¼ ì†ë„ í–¥ìƒì— ë§¤ìš° ì¤‘ìš”

# **Feed Forward Layer**

```python
class FeedForwardBlock(nn.Module):

    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:
        super().__init__()
        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1
        self.dropout = nn.Dropout(dropout)
        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2

    def forward(self, x):
        # (batch, seq, d_model) --> (batch, seq, d_ff) --> (batch, seq, d_model)
        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))
```

**íŠ¸ëœìŠ¤í¬ë¨¸ ë…¼ë¬¸ì˜ í–‰ë ¬ ì„¤ëª…**

- **í–‰ë ¬ W1**: ì°¨ì› 512 â†’ 2048 (ì…ë ¥ ì„ë² ë”©ì„ í™•ì¥).
- **í–‰ë ¬ W2**: ì°¨ì› 2048 â†’ 512 (í™•ì¥ëœ í‘œí˜„ì„ ë‹¤ì‹œ ì¶•ì†Œ).

### ğŸ”¹ ê¸°ë³¸ ê°œë…

íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ê° ìœ„ì¹˜(position)ì˜ í† í°ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ ì ìš©ë˜ëŠ” **ë‘ ê°œì˜ ì„ í˜• ê³„ì¸µ(Fully Connected Layers)**

â†’ **ë¹„ì„ í˜• ë³€í™˜**ì„ í†µí•´ ëª¨ë¸ì˜ í‘œí˜„ë ¥ì„ ë†’ì—¬ì¤Œ.

---

### ğŸ”¸ ë‚´ë¶€ ì†ì„± (Attributes)

1. **`self.linear_1`**
    - ì²« ë²ˆì§¸ ì„ í˜• ê³„ì¸µ
    - ì…ë ¥ ì°¨ì› `d_model` â†’ ì€ë‹‰ ì°¨ì› `d_ff`ë¡œ ë³€í™˜
    - ì—­í• : **íŠ¹ì§• í™•ì¥ (feature expansion)**
2. **`self.linear_2`**
    - ë‘ ë²ˆì§¸ ì„ í˜• ê³„ì¸µ
    - `d_ff` â†’ ë‹¤ì‹œ `d_model`ë¡œ ì°¨ì› ì¶•ì†Œ
    - ì—­í• : **ì›ë˜ ì°¨ì›ìœ¼ë¡œ ë³µì›**

---

### ğŸ”¸ ìˆœì „íŒŒ (`forward` ë©”ì„œë“œ) ê³¼ì •

### â‘  ì²« ë²ˆì§¸ ì„ í˜• ë³€í™˜

```python
self.linear_1(x)
```

- ì…ë ¥ í…ì„œì˜ shape: `(batch, seq_len, d_model)`
- ì¶œë ¥: `(batch, seq_len, d_ff)`
- ì˜ë¯¸: **ì°¨ì›ì„ ë„“í˜€ ë” í’ë¶€í•œ í‘œí˜„ ìƒì„±**

---

### â‘¡ ReLU í™œì„±í™”

```python
torch.relu(self.linear_1(x))
```

- ë¹„ì„ í˜• í•¨ìˆ˜ ì ìš© (ìŒìˆ˜ëŠ” 0ìœ¼ë¡œ, ì–‘ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ)
- í‘œí˜„ì— **ë¹„ì„ í˜•ì„±**ì„ ë”í•´ì¤Œ â†’ ë” ë³µì¡í•œ í•¨ìˆ˜ ê·¼ì‚¬ ê°€ëŠ¥

---

### â‘¢ ë“œë¡­ì•„ì›ƒ (Dropout) *(ìƒëµë˜ì–´ ìˆì—ˆì§€ë§Œ ëŒ€ë¶€ë¶„ í¬í•¨ë¨)*

```python
self.dropout(...)
```

- í›ˆë ¨ ì‹œ ì¼ë¶€ ë‰´ëŸ°ì„ ì„ì‹œë¡œ êº¼ì„œ **ê³¼ì í•© ë°©ì§€**

---

### â‘£ ë‘ ë²ˆì§¸ ì„ í˜• ë³€í™˜

```python
self.linear_2(...)
```

- ì°¨ì›ì„ ë‹¤ì‹œ `d_model`ë¡œ ì¶•ì†Œ
- shape: `(batch, seq_len, d_model)`
- ì›ë˜ ì°¨ì›ìœ¼ë¡œ ë˜ëŒë¦¬ë©´ì„œ ì •ë³´ ì •ì œ

---

### ğŸ”š ìµœì¢… ì¶œë ¥

```python
return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))
```

- ì¶œë ¥ ëª¨ì–‘: `(batch_size, seq_len, d_model)`
- ê° í† í° ìœ„ì¹˜ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ë¨

---

### âœ… í•µì‹¬ ìš”ì•½

- `d_model â†’ d_ff â†’ d_model` êµ¬ì¡°ì˜ **ë‘ ê°œì˜ ì„ í˜• ê³„ì¸µ**
- ReLUë¡œ **ë¹„ì„ í˜•ì„± ë¶€ì—¬**
- ë“œë¡­ì•„ì›ƒìœ¼ë¡œ **ê³¼ì í•© ë°©ì§€**
- íŠ¸ëœìŠ¤í¬ë¨¸ì˜ **í‘œí˜„ë ¥ì„ ë†’ì´ëŠ” í•µì‹¬ êµ¬ì„± ìš”ì†Œ**

# **Multi-Head Attention**

```python
class MultiHeadAttentionBlock(nn.Module):

    def __init__(self, d_model: int, h: int, dropout: float) -> None:
        super().__init__()
        self.d_model = d_model # Embedding vector size
        self.h = h # Number of heads
        # Make sure d_model is divisible by h
        assert d_model % h == 0, "d_model is not divisible by h"

        self.d_k = d_model // h # Dimension of vector seen by each head
        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq
        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk
        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv
        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo
        self.dropout = nn.Dropout(dropout)

    @staticmethod
    def attention(query, key, value, mask, dropout: nn.Dropout):
        d_k = query.shape[-1]
        # Just apply the formula from the paper
        # (batch, h, seq, d_k) --> (batch, h, seq, seq)
        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)
        if mask is not None:
            # Write a very low value (indicating -inf) to the positions where mask == 0
            attention_scores.masked_fill_(mask == 0, -1e9)
        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq, seq) # Apply softmax
        if dropout is not None:
            attention_scores = dropout(attention_scores)
        # (batch, h, seq, seq) --> (batch, h, seq, d_k)
        # return attention scores which can be used for visualization
        return (attention_scores @ value), attention_scores

    def forward(self, q, k, v, mask):
        query = self.w_q(q) # (batch, seq, d_model) --> (batch, seq, d_model)
        key = self.w_k(k) # (batch, seq, d_model) --> (batch, seq, d_model)
        value = self.w_v(v) # (batch, seq, d_model) --> (batch, seq, d_model)

        # (batch, seq, d_model) --> (batch, seq, h, d_k) --> (batch, h, seq, d_k)
        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)
        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)
        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)

        # Calculate attention
        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)

        # Combine all the heads together
        # (batch, h, seq, d_k) --> (batch, seq, h, d_k) --> (batch, seq, d_model)
        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)

        # Multiply by Wo
        # (batch, seq, d_model) --> (batch, seq, d_model)
        return self.w_o(x)
```

### ğŸ”¹ ê¸°ë³¸ ê°œë…

ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì€ **ì…ë ¥ ì •ë³´ë¥¼ ì—¬ëŸ¬ ê´€ì (head)** ì—ì„œ ë™ì‹œì— ì£¼ì˜(attention)í•˜ì—¬ ë” í’ë¶€í•œ í‘œí˜„ì„ ë§Œë“œëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.

íŠ¸ëœìŠ¤í¬ë¨¸ì˜ í•µì‹¬ êµ¬ì„±ìš”ì†Œ ì¤‘ í•˜ë‚˜ë¡œ, **Self-Attention**ì„ ì—¬ëŸ¬ ë²ˆ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.

---

### ğŸ”¸ ë‚´ë¶€ ì†ì„± (Attributes)

1. **`self.d_model`**
    - ì „ì²´ ì„ë² ë”© ì°¨ì› (ì˜ˆ: 512, 768 ë“±)
2. **`self.h`**
    - ì–´í…ì…˜ í—¤ë“œ ìˆ˜ (ì˜ˆ: 8ê°œ, 12ê°œ)
    - ì…ë ¥ ë²¡í„°ë¥¼ ì´ ìˆ˜ë§Œí¼ ë‚˜ëˆ„ì–´ ë³‘ë ¬ ì²˜ë¦¬í•¨
3. **`self.d_k = d_model // h`**
    - **ê° í—¤ë“œê°€ ë‹´ë‹¹í•˜ëŠ” ì°¨ì› ìˆ˜**
    - ì˜ˆ: d_modelì´ 512ì´ê³  hê°€ 8ì´ë©´, ê° í—¤ë“œëŠ” 64ì°¨ì› ì²˜ë¦¬
4. **`self.w_q, self.w_k, self.w_v`**
    - ê°ê° ì¿¼ë¦¬(Q), í‚¤(K), ë°¸ë¥˜(V)ë¡œ ì…ë ¥ì„ ë³€í™˜í•˜ëŠ” ì„ í˜• ë ˆì´ì–´
5. **`self.w_o`**
    - ëª¨ë“  í—¤ë“œì˜ ì¶œë ¥ì„ ê²°í•©í•œ ë’¤, ìµœì¢… ì¶œë ¥ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì„ í˜• ë ˆì´ì–´

---

### ğŸ”¸ ì–´í…ì…˜ ê³„ì‚° ê³¼ì • (`attention` ë©”ì„œë“œ)

### ğŸ“¥ ì…ë ¥

- **query, key, value**: ê° ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ë³€í™˜ëœ ë²¡í„°
- **mask (ì„ íƒ ì‚¬í•­)**: íŠ¹ì • ìœ„ì¹˜ë¥¼ ë¬´ì‹œí•  ë•Œ ì‚¬ìš© (ex. padding, causal masking)

---

### â‘  ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°

```python
scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)
```

- Qì™€ Kì˜ ë‚´ì ì„ í†µí•´ ìœ ì‚¬ë„ ê³„ì‚°
- `âˆšd_k`ë¡œ ë‚˜ëˆ„ì–´ **ê·¸ë˜ë””ì–¸íŠ¸ ì•ˆì •í™”**

---

### â‘¡ ë§ˆìŠ¤í¬ ì ìš© (ì„ íƒ)

- maskê°€ ì£¼ì–´ì§€ë©´, **ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜ì— í° ìŒìˆ˜(-inf)**ë¥¼ ë„£ì–´ softmax í›„ ë¬´ì‹œë˜ê²Œ ë§Œë“¦

---

### â‘¢ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ & ì¶œë ¥ ê³„ì‚°

```python
weights = softmax(scores)
output = weights @ value
```

- ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚° í›„, Vì™€ ê³±í•´ ìµœì¢… ì–´í…ì…˜ ì¶œë ¥ ìƒì„±

---

### ğŸ”¸ ë©€í‹°í—¤ë“œ ì²˜ë¦¬ íë¦„

### â‘  ì…ë ¥ ë¶„í• 

```python
(batch, seq_len, d_model) â†’ (batch, h, seq_len, d_k)
```

- Q, K, Vë¥¼ ê°ê° `h`ê°œì˜ í—¤ë“œë¡œ ë‚˜ëˆ”
- `.view()`ì™€ `.transpose()` ë“±ì„ í†µí•´ shape ì¡°ì •

### â‘¡ ë³‘ë ¬ë¡œ ì–´í…ì…˜ ìˆ˜í–‰

- ê° í—¤ë“œì—ì„œ ë…ë¦½ì ìœ¼ë¡œ attention ê³„ì‚°

### â‘¢ í—¤ë“œ ê²°í•©

```python
(batch, h, seq_len, d_k) â†’ (batch, seq_len, d_model)
```

- ì—¬ëŸ¬ í—¤ë“œì˜ ì¶œë ¥ì„ concat (ë³‘í•©)
- `.transpose()` í›„ `.contiguous().view()`ë¡œ ì›ë˜ ì°¨ì› ë³µì›

### â‘£ ìµœì¢… ì„ í˜• ë³€í™˜

- `self.w_o`ë¥¼ ì‚¬ìš©í•´ ìµœì¢… ì¶œë ¥ ìƒì„±
- shape: `(batch, seq_len, d_model)`

---

### âœ… í•µì‹¬ ìš”ì•½

| ë‹¨ê³„ | ì„¤ëª… |
| --- | --- |
| ì…ë ¥ ì„ í˜•ë³€í™˜ | Q, K, V ìƒì„± (linear layers) |
| ë¶„í•  | ì—¬ëŸ¬ í—¤ë“œë¡œ ë‚˜ëˆ” (hê°œ) |
| ì–´í…ì…˜ ìˆ˜í–‰ | ê° í—¤ë“œì—ì„œ scaled dot-product attention |
| ê²°í•© | ëª¨ë“  í—¤ë“œ ê²°ê³¼ë¥¼ í•©ì³ í•˜ë‚˜ë¡œ |
| ìµœì¢… ë³€í™˜ | ì„ í˜• ë ˆì´ì–´ë¥¼ í†µí•´ ì¶œë ¥ |

# **Residual Connection**

```python
class ResidualConnection(nn.Module):

        def __init__(self, features: int, dropout: float) -> None:
            super().__init__()
            self.dropout = nn.Dropout(dropout)
            self.norm = LayerNormalization(features)

        def forward(self, x, sublayer):
            return x + self.dropout(sublayer(self.norm(x)))
```

### ğŸ”¹ ê¸°ë³¸ ê°œë…

íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œëŠ” **ì”ì°¨ ì—°ê²°(Residual Connection)** ê³¼ **ë ˆì´ì–´ ì •ê·œí™”(LayerNorm)** ë¥¼ ê²°í•©í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

- êµ¬ì¡°:
    
    **`x â†’ LayerNorm â†’ Sublayer â†’ Dropout â†’ + x (Residual)`**
    

---

### ğŸ”¸ `forward` ë©”ì„œë“œ ê°œìš”

### ğŸ“¥ ì…ë ¥

- `x`: ì…ë ¥ í…ì„œ (ì˜ˆ: ì–´í…ì…˜ ë˜ëŠ” FFN ì´ì „ì˜ ì…ë ¥)
- `sublayer`: í•¨ìˆ˜ í˜•íƒœë¡œ ì „ë‹¬ë˜ëŠ” **ì„œë¸Œë ˆì´ì–´**
    
    ì˜ˆ: `MultiHeadAttention`, `FeedForward`, ë“±
    

---

### ğŸ”¸ ë‹¨ê³„ë³„ ì²˜ë¦¬ ê³¼ì •

---

### â‘  ì •ê·œí™”

```python
self.norm(x)
```

- ì…ë ¥ `x`ì— ëŒ€í•´ **Layer Normalization** ìˆ˜í–‰
- í•™ìŠµ ì•ˆì •í™” ë° ì†ë„ í–¥ìƒ

---

### â‘¡ ì„œë¸Œë ˆì´ì–´ ì ìš©

```python
sublayer(self.norm(x))

```

- ì •ê·œí™”ëœ ê°’ì„ **sublayer** (ì˜ˆ: ì–´í…ì…˜ ë˜ëŠ” FFN)ì— ì „ë‹¬
- ì„œë¸Œë ˆì´ì–´ëŠ” ì…ë ¥ì„ ë³€í™˜í•¨

---

### â‘¢ ë“œë¡­ì•„ì›ƒ

```python
self.dropout(sublayer(...))
```

- **ê³¼ì í•© ë°©ì§€**ë¥¼ ìœ„í•œ ë“œë¡­ì•„ì›ƒ ì ìš©
- í•™ìŠµ ì¤‘ ì¼ë¶€ ë‰´ëŸ° ë¬´ì‘ìœ„ ì œê±°

---

### â‘£ ì”ì°¨ ì—°ê²° (Residual Connection)

```python
x + self.dropout(...)
```

- **ì›ë˜ ì…ë ¥ x**ì™€ **ì„œë¸Œë ˆì´ì–´ì˜ ì¶œë ¥**ì„ ë”í•¨
- ëª¨ë¸ì´ í•„ìš” ì‹œ **"ì•„ë¬´ ê²ƒë„ í•˜ì§€ ì•Šê¸°" (identity mapping)** ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤Œ
- **ë”¥ëŸ¬ë‹ì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤(Vanishing Gradient)** ë¬¸ì œë¥¼ ì¤„ì—¬ì¤Œ

---

### ğŸ”š ìµœì¢… ì¶œë ¥

- ì •ê·œí™” â†’ ì„œë¸Œë ˆì´ì–´ â†’ ë“œë¡­ì•„ì›ƒ â†’ ì”ì°¨ ì—°ê²° í›„ ê²°ê³¼ ë°˜í™˜

```python
return x + self.dropout(sublayer(self.norm(x)))
```

---

### âœ… í•µì‹¬ ìš”ì•½

| ë‹¨ê³„ | ì„¤ëª… |
| --- | --- |
| `self.norm(x)` | ì •ê·œí™”ë¡œ í•™ìŠµ ì•ˆì •í™” |
| `sublayer(...)` | ì–´í…ì…˜ or FFN ë“± ë³€í™˜ ìˆ˜í–‰ |
| `self.dropout(...)` | ì¼ë¶€ ë‰´ëŸ° ì œê±°ë¡œ ê³¼ì í•© ë°©ì§€ |
| `x + ...` | ì…ë ¥ + ì¶œë ¥ â†’ **ì”ì°¨ ì—°ê²°ë¡œ ì„±ëŠ¥ í–¥ìƒ** |

# Encoder Block

```python
class EncoderBlock(nn.Module):

    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:
        super().__init__()
        self.self_attention_block = self_attention_block
        self.feed_forward_block = feed_forward_block
        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])

    def forward(self, x, src_mask):
        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))
        x = self.residual_connections[1](x, self.feed_forward_block)
        return x
```

### ğŸ”¹ ê¸°ë³¸ ê°œë…

íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ì˜ í•œ ë¸”ë¡ì€ ë‹¤ìŒ ë‘ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤:

1. **ë©€í‹°í—¤ë“œ ìê¸°ì–´í…ì…˜ (Multi-Head Self-Attention)**
2. **í¬ì§€ì…˜-ì™€ì´ì¦ˆ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ (Feed-Forward Network)**

ê° êµ¬ì„± ìš”ì†Œì—ëŠ” **ì •ê·œí™”(LayerNorm)** ì™€ **ì”ì°¨ ì—°ê²°(Residual Connection)** ì´ í•¨ê»˜ ì‚¬ìš©ë©ë‹ˆë‹¤.

---

### ğŸ”¸ ì†ì„± ì„¤ëª… (Attributes)

1. **`self.self_attention_block`**
    - ë©€í‹°í—¤ë“œ **ìê¸°ì–´í…ì…˜** ë¸”ë¡ ì €ì¥
2. **`self.feed_forward_block`**
    - **í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬** ì €ì¥ (2ê°œì˜ ì„ í˜• ê³„ì¸µ + ReLU)
3. **`self.residual_connections`**
    - **ResidualConnection ì¸ìŠ¤í„´ìŠ¤ 2ê°œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³´ê´€**
    - í•˜ë‚˜ëŠ” ì–´í…ì…˜ìš©, í•˜ë‚˜ëŠ” FFNìš©
    - í˜•ì‹: `[ResidualConnection, ResidualConnection]`

---

### ğŸ”¸ ìˆœì „íŒŒ ê³¼ì • (`forward` ë©”ì„œë“œ)

### âœ… 1ë‹¨ê³„: ìê¸°ì–´í…ì…˜ + ì”ì°¨ ì—°ê²°

```python
x = self.residual_connections[0](
    x,
    lambda x: self.self_attention_block(x, x, x, src_mask)
)
```

- `x`ëŠ” ì…ë ¥ í…ì„œ
- ë¨¼ì € **LayerNorm â†’ Self-Attention â†’ Dropout** ì ìš©
- ì›ë˜ ì…ë ¥ `x`ì™€ ê²°ê³¼ë¥¼ ë”í•´ì„œ **ì”ì°¨ ì—°ê²°**

ğŸ”¹ `lambda x: ...` ë¥¼ ì“°ëŠ” ì´ìœ :

- `ResidualConnection` ì•ˆì—ì„œ ì •ê·œí™”ëœ `x`ë¥¼ ë°›ì•„ ì–´í…ì…˜ ë¸”ë¡ì— ë„˜ê¸°ê¸° ìœ„í•´
- `query = key = value = x` í˜•íƒœë¡œ ìê¸°ì–´í…ì…˜ ìˆ˜í–‰

---

### âœ… 2ë‹¨ê³„: í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ + ì”ì°¨ ì—°ê²°

```python
x = self.residual_connections[1](x, self.feed_forward_block)
```

- ì²« ë²ˆì§¸ ì¶œë ¥ `x`ë¥¼ ë‹¤ì‹œ ì •ê·œí™” í›„ FFNì— í†µê³¼
- ê²°ê³¼ë¥¼ ì›ë˜ ì…ë ¥ `x`ì— ë”í•¨ (ì”ì°¨ ì—°ê²°)

---

### ğŸ”š ìµœì¢… ì¶œë ¥

- ì–´í…ì…˜ â†’ ì”ì°¨ ì—°ê²°
- FFN â†’ ì”ì°¨ ì—°ê²°
- ë‘ ë‹¨ê³„ë¥¼ ê±°ì¹œ ìµœì¢… í…ì„œë¥¼ ë°˜í™˜

```python
return x
```

---

### âœ… í•µì‹¬ ìš”ì•½

| ìˆœì„œ | êµ¬ì„± ìš”ì†Œ | ì„¤ëª… |
| --- | --- | --- |
| â‘  | **ìê¸°ì–´í…ì…˜** | ì…ë ¥ ê°„ ìƒí˜¸ì‘ìš© í•™ìŠµ |
| â‘¡ | **ì”ì°¨ + ì •ê·œí™”** | í•™ìŠµ ì•ˆì •í™” |
| â‘¢ | **í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬** | ìœ„ì¹˜ë³„ ë…ë¦½ì  ë¹„ì„ í˜• ë³€í™˜ |
| â‘£ | **ì”ì°¨ + ì •ê·œí™”** | ì •ë³´ ë³´ì¡´ ë° ì•ˆì •í™” |

# Encoder (Stack)

```python
class Encoder(nn.Module):

    def __init__(self, features: int, layers: nn.ModuleList) -> None:
        super().__init__()
        self.layers = layers
        self.norm = LayerNormalization(features)

    def forward(self, x, mask):
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)
```

### ğŸ”¹ ê¸°ë³¸ ê°œë…

ì „ì²´ **ì¸ì½”ë”**ëŠ” ì—¬ëŸ¬ ê°œì˜ **ì¸ì½”ë” ë ˆì´ì–´(ë¸”ë¡)** ë¥¼ **ìˆœì°¨ì ìœ¼ë¡œ ìŒ“ì€ êµ¬ì¡°**ì…ë‹ˆë‹¤.

ê° ë ˆì´ì–´ëŠ” ë‹¤ìŒì„ í¬í•¨í•©ë‹ˆë‹¤:

- ë©€í‹°í—¤ë“œ ìê¸°ì–´í…ì…˜
- í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬
- ë ˆì´ì–´ ì •ê·œí™” + ì”ì°¨ ì—°ê²°

---

### ğŸ”¸ Feed Forward ê³¼ì • (Encoderì˜ forward ë©”ì„œë“œ)

### âœ… 1ë‹¨ê³„: ì¸ì½”ë” ë ˆì´ì–´ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ í†µê³¼

```python
for layer in self.layers:
    x = layer(x, mask)
```

- `self.layers`: ì—¬ëŸ¬ ê°œì˜ ì¸ì½”ë” ë ˆì´ì–´ê°€ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸
- ê° ë ˆì´ì–´ì—ì„œ ì…ë ¥ `x`ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤:
    - LayerNorm â†’ MultiHeadAttention â†’ Residual
    - LayerNorm â†’ FFN â†’ Residual
- ë ˆì´ì–´ë¥¼ í†µê³¼í•  ë•Œë§ˆë‹¤ `x`ê°€ ê°±ì‹ ë¨

---

### âœ… 2ë‹¨ê³„: ìµœì¢… ë ˆì´ì–´ ì •ê·œí™”

```python
x = self.norm(x)
```

- ëª¨ë“  ì¸ì½”ë” ë ˆì´ì–´ë¥¼ í†µê³¼í•œ í›„, **ë§ˆì§€ë§‰ìœ¼ë¡œ í•œ ë²ˆ ë” ì •ê·œí™”(LayerNorm)**
- ëª¨ë¸ì´ ì¶œë ¥ ì „ì— ì•ˆì •ëœ í‘œí˜„ì„ ìƒì„±í•˜ë„ë¡ ë•ëŠ” ì—­í• 

---

### âœ… 3ë‹¨ê³„: ì¶œë ¥ ë°˜í™˜

```python
return x
```

- ì •ê·œí™”ëœ ìµœì¢… ì¸ì½”ë” ì¶œë ¥ì„ ë°˜í™˜
- ì´ ì¶œë ¥ì€ ì´í›„ ë””ì½”ë”(decoder)ë‚˜ ë‹¤ë¥¸ íƒœìŠ¤í¬(ì˜ˆ: ë¶„ë¥˜)ì— ì‚¬ìš©ë¨

---

### âœ… í•µì‹¬ ìš”ì•½

| ë‹¨ê³„ | ì„¤ëª… |
| --- | --- |
| `for layer in self.layers:` | ì¸ì½”ë” ë ˆì´ì–´ë¥¼ ë°˜ë³µí•˜ë©´ì„œ `x`ë¥¼ ì—…ë°ì´íŠ¸ |
| `x = self.norm(x)` | ìµœì¢… ì¶œë ¥ê°’ ì •ê·œí™” |
| `return x` | ì¸ì½”ë”ì˜ ìµœì¢… ì¶œë ¥ ë°˜í™˜ |

# Decoder Block

```python
class DecoderBlock(nn.Module):

    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:
        super().__init__()
        self.self_attention_block = self_attention_block
        self.cross_attention_block = cross_attention_block
        self.feed_forward_block = feed_forward_block
        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])

    def forward(self, x, encoder_output, src_mask, tgt_mask):
        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))
        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))
        x = self.residual_connections[2](x, self.feed_forward_block)
        return x
```

### ğŸ”¹ ê¸°ë³¸ ê°œë…

ë””ì½”ë”ì˜ í•œ ë¸”ë¡ì€ ë‹¤ìŒ **ì„¸ ê°€ì§€ ì„œë¸Œë ˆì´ì–´**ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

1. **ìê¸°ì–´í…ì…˜ (Self-Attention)**
2. **í¬ë¡œìŠ¤ì–´í…ì…˜ (Cross-Attention)** â€“ ì¸ì½”ë” ì¶œë ¥ì— ëŒ€í•œ ì–´í…ì…˜
3. **í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ (Feed Forward Network)**

ê° ì„œë¸Œë ˆì´ì–´ëŠ” **ì”ì°¨ ì—°ê²°(Residual Connection)** ê³¼ **ì •ê·œí™”(LayerNorm)** ì™€ í•¨ê»˜ ì‚¬ìš©ë©ë‹ˆë‹¤.

---

### ğŸ”¸ íŒŒë¼ë¯¸í„° ì„¤ëª…

| ì´ë¦„ | ì„¤ëª… |
| --- | --- |
| `self_attention_block` | íƒ€ê²Ÿ ì‹œí€€ìŠ¤ ë‚´ë¶€ì—ì„œ ìê¸°ì–´í…ì…˜ì„ ìˆ˜í–‰ |
| `cross_attention_block` | ì¸ì½”ë”ì˜ ì¶œë ¥ì— ëŒ€í•´ ì–´í…ì…˜ ìˆ˜í–‰ (í¬ë¡œìŠ¤ì–´í…ì…˜) |
| `feed_forward_block` | í¬ì§€ì…˜-ì™€ì´ì¦ˆ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ |
| `src_mask` | ì†ŒìŠ¤ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ë§ˆìŠ¤í¬ |
| `tgt_mask` | íƒ€ê²Ÿ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ë§ˆìŠ¤í¬ (ë¯¸ë˜ í† í° ê°€ë¦¬ê¸° ë“±) |

---

### ğŸ”¸ ì†ì„± ì„¤ëª… (Attributes)

- `self.self_attention_block`: ìê¸°ì–´í…ì…˜ ë¸”ë¡ ì €ì¥
- `self.cross_attention_block`: ì¸ì½”ë” ì¶œë ¥ì— ëŒ€í•œ í¬ë¡œìŠ¤ì–´í…ì…˜ ë¸”ë¡ ì €ì¥
- `self.feed_forward_block`: FFN ì €ì¥
- `self.residual_connections`: ResidualConnection 3ê°œ (ê° ì„œë¸Œë ˆì´ì–´ìš©)

---

### ğŸ”¸ ìˆœì „íŒŒ ê³¼ì • (`forward` ë©”ì„œë“œ)

### âœ… 1ë‹¨ê³„: **ìê¸°ì–´í…ì…˜ + ì”ì°¨ ì—°ê²°**

```python
x = self.residual_connections[0](
    x,
    lambda x: self.self_attention_block(x, x, x, tgt_mask)
)
```

- ìê¸° ì‹œí€€ìŠ¤ ë‚´ë¶€ í† í° ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµ
- ë¯¸ë˜ ì •ë³´ ë°©ì§€ë¥¼ ìœ„í•´ **tgt_mask** ì ìš©
- LayerNorm â†’ Self-Attention â†’ Dropout â†’ Residual

---

### âœ… 2ë‹¨ê³„: **í¬ë¡œìŠ¤ì–´í…ì…˜ + ì”ì°¨ ì—°ê²°**

```python
x = self.residual_connections[1](
    x,
    lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask)
)
```

- ì¸ì½”ë” ì¶œë ¥(ì†ŒìŠ¤ ì‹œí€€ìŠ¤)ì— ëŒ€í•œ ì–´í…ì…˜
- ê° ë””ì½”ë” ìœ„ì¹˜ê°€ ì¸ì½”ë”ì˜ ëª¨ë“  ìœ„ì¹˜ë¥¼ ì°¸ê³  ê°€ëŠ¥
- LayerNorm â†’ Cross-Attention â†’ Dropout â†’ Residual

---

### âœ… 3ë‹¨ê³„: **í”¼ë“œí¬ì›Œë“œ + ì”ì°¨ ì—°ê²°**

```python
x = self.residual_connections[2](x, self.feed_forward_block)
```

- ìœ„ì¹˜ë³„ ë…ë¦½ì  ë³€í™˜ ìˆ˜í–‰
- LayerNorm â†’ FFN â†’ Dropout â†’ Residual

---

### âœ… ì¶œë ¥ ë°˜í™˜

```python
return x
```

- ì„¸ ì„œë¸Œë ˆì´ì–´ë¥¼ ëª¨ë‘ ê±°ì¹œ ìµœì¢… ë””ì½”ë” ë¸”ë¡ ì¶œë ¥ ë°˜í™˜

---

### âœ… í•µì‹¬ ìš”ì•½

| ìˆœì„œ | êµ¬ì„± ìš”ì†Œ | ì„¤ëª… |
| --- | --- | --- |
| â‘  | **Self-Attention + Residual** | ë¯¸ë˜ í† í°ì„ ê°€ë¦¬ëŠ” ìê¸°ì–´í…ì…˜ |
| â‘¡ | **Cross-Attention + Residual** | ì¸ì½”ë” ì¶œë ¥ì— ëŒ€í•œ ì–´í…ì…˜ |
| â‘¢ | **Feed Forward + Residual** | ê° ìœ„ì¹˜ë³„ ë¹„ì„ í˜• ë³€í™˜ |
| ğŸ”„ | ê° ë‹¨ê³„ë§ˆë‹¤ LayerNorm â†’ Dropout â†’ Residual ì ìš© |  |

# Decoder (Stack)

```python
class Decoder(nn.Module):

    def __init__(self, features: int, layers: nn.ModuleList) -> None:
        super().__init__()
        self.layers = layers
        self.norm = LayerNormalization(features)

    def forward(self, x, encoder_output, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, encoder_output, src_mask, tgt_mask)
        return self.norm(x)
```

### ğŸ”¹ ê¸°ë³¸ ê°œë…

ì „ì²´ **ë””ì½”ë”**ëŠ” ì—¬ëŸ¬ ê°œì˜ **ë””ì½”ë” ë¸”ë¡(DecoderBlock)** ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°,

ê° ë¸”ë¡ì€ ë‹¤ìŒì„ í¬í•¨í•©ë‹ˆë‹¤:

- ìê¸°ì–´í…ì…˜ (Self-Attention)
- í¬ë¡œìŠ¤ì–´í…ì…˜ (Cross-Attention)
- í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ (Feed Forward)
- ì”ì°¨ ì—°ê²° + ì •ê·œí™”

---

### ğŸ”¸ ìˆœì „íŒŒ ê³¼ì • (forward ë©”ì„œë“œ)

### âœ… 1ë‹¨ê³„: ë””ì½”ë” ë¸”ë¡ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ í†µê³¼

```python
for layer in self.layers:
    x = layer(x, encoder_output, src_mask, tgt_mask)
```

- `self.layers`: ì—¬ëŸ¬ ê°œì˜ ë””ì½”ë” ë ˆì´ì–´ê°€ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸
- ê° ë””ì½”ë” ë ˆì´ì–´ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰:
    - íƒ€ê²Ÿ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ìê¸°ì–´í…ì…˜ (`tgt_mask` ì‚¬ìš© â†’ ë¯¸ë˜ í† í° ê°€ë¦¬ê¸°)
    - ì¸ì½”ë” ì¶œë ¥ì— ëŒ€í•œ í¬ë¡œìŠ¤ì–´í…ì…˜ (`src_mask` ì‚¬ìš©)
    - í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬
    - ê° ê³¼ì •ë§ˆë‹¤ **ì •ê·œí™” + ì”ì°¨ ì—°ê²°**

---

### âœ… 2ë‹¨ê³„: ìµœì¢… ë ˆì´ì–´ ì •ê·œí™”

```python
x = self.norm(x)
```

- ëª¨ë“  ë””ì½”ë” ë ˆì´ì–´ë¥¼ í†µê³¼í•œ í›„ ë§ˆì§€ë§‰ìœ¼ë¡œ LayerNorm ì ìš©
- ëª¨ë¸ì´ ì¶œë ¥ ì „ì— ì•ˆì •ëœ í‘œí˜„ì„ ìƒì„±í•˜ë„ë¡ ë•ëŠ” ì—­í• 

---

### âœ… 3ë‹¨ê³„: ì¶œë ¥ ë°˜í™˜

```python
return x
```

- ì •ê·œí™”ëœ ìµœì¢… ë””ì½”ë” ì¶œë ¥ ë°˜í™˜
- ì´ ì¶œë ¥ì€ ìµœì¢…ì ìœ¼ë¡œ **ì†Œí”„íŠ¸ë§¥ìŠ¤ â†’ ë‹¤ìŒ í† í° ì˜ˆì¸¡** ë“±ì— ì‚¬ìš©ë¨

---

### âœ… í•µì‹¬ ìš”ì•½

| ë‹¨ê³„ | ì„¤ëª… |
| --- | --- |
| `for layer in self.layers:` | ë””ì½”ë” ë ˆì´ì–´ë“¤ì„ ë°˜ë³µí•˜ë©´ì„œ `x`ë¥¼ ìˆœì°¨ ì²˜ë¦¬ |
| `tgt_mask` | ë””ì½”ë”ê°€ ë¯¸ë˜ í† í°ì„ ë³´ì§€ ëª»í•˜ë„ë¡ ë§ˆìŠ¤í‚¹ |
| `src_mask` | ì¸ì½”ë” ì¶œë ¥ ì¤‘ ì¼ë¶€ í† í°ì— ëŒ€í•œ ì–´í…ì…˜ì„ ì°¨ë‹¨ |
| `self.norm(x)` | ë§ˆì§€ë§‰ ì •ê·œí™” |
| `return x` | ë””ì½”ë”ì˜ ìµœì¢… ì¶œë ¥ ë°˜í™˜ |

# **Projection Layer**

```python
class ProjectionLayer(nn.Module):

    def __init__(self, d_model, vocab_size) -> None:
        super().__init__()
        self.proj = nn.Linear(d_model, vocab_size)

    def forward(self, x) -> None:
        # (batch, seq, d_model) --> (batch, seq, vocab_size)
        return self.proj(x)
```

**ProjectionLayerì˜ ì—­í• **

- **ê¸°ëŠ¥**: íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ë””ì½”ë” ì¶œë ¥ì„ ì–´íœ˜ ê³µê°„(vocabulary space)ìœ¼ë¡œ ë§¤í•‘.
- **ëª©ì **: ë‹¤ìŒ ë‹¨ì–´ ìƒì„± ë˜ëŠ” ì˜ˆì¸¡ ê°€ëŠ¥í•˜ë„ë¡ í•¨.
- **íŠ¹ì§•**: ê°„ë‹¨í•˜ì§€ë§Œ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ í•µì‹¬ì ì¸ êµ¬ì„± ìš”ì†Œ.

**íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¶œë ¥ì¸µ(Output Layer) ìš”ì•½:**

- **`self.proj`**:
    - **ì—­í• **: íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ê²°ê³¼(`d_model` ì°¨ì›)ë¥¼ ì „ì²´ ë‹¨ì–´ì˜ ê°œìˆ˜(`vocab_size`)ë¡œ ë³€í™˜í•´ì£¼ëŠ” ì„ í˜• ê³„ì¸µ(nn.Linear).
    - **ê¸°ëŠ¥**: ì…ë ¥ ë°ì´í„°ë¥¼ ì–´íœ˜(ë‹¨ì–´) ê³µê°„ìœ¼ë¡œ íˆ¬ì˜(projection)í•˜ì—¬ ê° ë‹¨ì–´ì— ëŒ€í•œ ì ìˆ˜(logit)ë¥¼ ê³„ì‚°.
- **`forward` ë©”ì†Œë“œ (ìˆœì „íŒŒ ê³¼ì •)**:
    - **ì…ë ¥**: ë””ì½”ë”ë¡œë¶€í„° ë°›ì€ í…ì„œ `x`.
        - **í˜•íƒœ**: `(batch, seq, d_model)` (ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´, ëª¨ë¸ ì°¨ì›).
    - **ì²˜ë¦¬**: ì…ë ¥ í…ì„œ `x`ê°€ `self.proj` ì„ í˜• ê³„ì¸µì„ í†µê³¼.
        - **ê³¼ì •**: ì…ë ¥ í…ì„œì™€ `self.proj`ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê³±í•˜ê³  í¸í–¥(bias)ì„ ë”í•˜ëŠ” ì„ í˜• ë³€í™˜ ìˆ˜í–‰.
    - **ì¶œë ¥**: ë³€í™˜ëœ í…ì„œ.
        - **í˜•íƒœ**: `(batch, seq, vocab_size)` (ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´, ì–´íœ˜ í¬ê¸°).
        - **ì˜ë¯¸**: ì‹œí€€ìŠ¤ ë‚´ ê° ìœ„ì¹˜ë§ˆë‹¤ ì „ì²´ ì–´íœ˜ì— ìˆëŠ” ê° ë‹¨ì–´ì— ëŒ€í•œ **ì ìˆ˜(logit)** ë²¡í„°ê°€ ìƒì„±ë¨. ì´ ì ìˆ˜ë“¤ì„ í†µí•´ ì–´ë–¤ ë‹¨ì–´ê°€ ë‹¤ìŒì— ì˜¬ í™•ë¥ ì´ ë†’ì€ì§€ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# **Transformer**

```python
class Transformer(nn.Module):

    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.src_pos = src_pos
        self.tgt_pos = tgt_pos
        self.projection_layer = projection_layer

    def encode(self, src, src_mask):
        # (batch, seq, d_model)
        src = self.src_embed(src)
        src = self.src_pos(src)
        return self.encoder(src, src_mask)

    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):
        # (batch, seq, d_model)
        tgt = self.tgt_embed(tgt)
        tgt = self.tgt_pos(tgt)
        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)

    def project(self, x):
        # (batch, seq, vocab_size)
        return self.projection_layer(x)
```

**íŠ¸ëœìŠ¤í¬ë¨¸ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ ë° ì‘ë™ ë°©ì‹ ìš”ì•½:**

---

### **1. ì´ˆê¸°í™” (`__init__`)**

- **ì—­í• **: íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ì‚¬ìš©í•  ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤.
- **í¬í•¨**: **ì¸ì½”ë”**, **ë””ì½”ë”**, **ì„ë² ë”©(Embedding) ê³„ì¸µ**, **ìœ„ì¹˜ ì¸ì½”ë”©(Positional Encoding) ê³„ì¸µ**, ê·¸ë¦¬ê³  **íˆ¬ì˜(Projection) ê³„ì¸µ**ê³¼ ê°™ì€ í•„ìˆ˜ êµ¬ì„± ìš”ì†Œë“¤ì„ ë¯¸ë¦¬ ì„¤ì •í•©ë‹ˆë‹¤.
- **ëª©ì **: ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì¸ì½”ë”©í•˜ê³  ë””ì½”ë”©í•˜ëŠ” ì „ì²´ ê³¼ì •ì„ ìœ„í•œ ê¸°ë³¸ ê³¨ê²©ì„ ë§ˆë ¨í•©ë‹ˆë‹¤.

---

### **2. ì¸ì½”ë”© (`encode`)**

- **ì—­í• **: ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¨ **ì›ë³¸ ì‹œí€€ìŠ¤**ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
- **ê³¼ì •**:
    1. ì…ë ¥ëœ ë‹¨ì–´(í† í°)ë“¤ì„ ìˆ«ìë¡œ ë³€í™˜í•˜ê³ , ì´ ìˆ«ìë“¤ì„ ì˜ë¯¸ ìˆëŠ” **ì„ë² ë”© ë²¡í„°**ë¡œ ë°”ê¿‰ë‹ˆë‹¤.
    2. ë‹¨ì–´ì˜ **ìˆœì„œ ì •ë³´**ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ **ìœ„ì¹˜ ì¸ì½”ë”©**ì„ ì„ë² ë”©ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    3. ì´ ì •ë³´ë¥¼ **ì¸ì½”ë”**ì— ë„£ì–´ ì›ë³¸ ì‹œí€€ìŠ¤ì˜ ì••ì¶•ëœ **í‘œí˜„(encoded representation)**ì„ ë§Œë“­ë‹ˆë‹¤.

---

### **3. ë””ì½”ë”© (`decode`)**

- **ì—­í• **: **ëª©í‘œ ì‹œí€€ìŠ¤** (ë²ˆì—­í•˜ë ¤ëŠ” ê²°ê³¼ë¬¼ ë˜ëŠ” ë‹¤ìŒ ë‹¨ì–´)ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
- **ê³¼ì •**:
    1. ëª©í‘œ ì‹œí€€ìŠ¤ì˜ ë‹¨ì–´ë“¤ì„ **ì„ë² ë”©**í•˜ê³  **ìœ„ì¹˜ ì¸ì½”ë”©**ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    2. ì´ ì •ë³´ì™€ í•¨ê»˜ **ì¸ì½”ë”ì˜ ì¶œë ¥**ì„ **ë””ì½”ë”**ì— ì…ë ¥í•©ë‹ˆë‹¤.
    3. ë””ì½”ë”ëŠ” ì´ ë‘ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì˜ˆì¸¡ì„ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë  **ë””ì½”ë”©ëœ í‘œí˜„(decoded representation)**ì„ ì¶œë ¥í•©ë‹ˆë‹¤.

---

### **4. íˆ¬ì˜ (`project`)**

- **ì—­í• **: **ë””ì½”ë”ì˜ ì¶œë ¥**ì„ ì‹¤ì œ ë‹¨ì–´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
- **ê³¼ì •**: ë””ì½”ë”ì—ì„œ ë‚˜ì˜¨ ì—°ì†ì ì¸ ìˆ«ì ê°’ë“¤ì„ **ì–´íœ˜(ë‹¨ì–´) ê³µê°„**ì— ìˆëŠ” íŠ¹ì • ë‹¨ì–´ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.
- **ëª©ì **: ë””ì½”ë”ì˜ ì¶”ìƒì ì¸ ê²°ê³¼ë¬¼ì„ ì‚¬ëŒì´ ì´í•´í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ **ë‹¨ì–´ ì˜ˆì¸¡**ìœ¼ë¡œ ë°”ê¾¸ëŠ” í•µì‹¬ ë‹¨ê³„ì…ë‹ˆë‹¤.

# **Build Transformer**

```python
def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq: int, tgt_seq: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:
    # Create the embedding layers
    src_embed = InputEmbeddings(d_model, src_vocab_size)
    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)

    # Create the positional encoding layers
    src_pos = PositionalEncoding(d_model, src_seq, dropout)
    tgt_pos = PositionalEncoding(d_model, tgt_seq, dropout)

    # Create the encoder blocks
    encoder_blocks = []
    for _ in range(N):
        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)
        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)
        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)
        encoder_blocks.append(encoder_block)

    # Create the decoder blocks
    decoder_blocks = []
    for _ in range(N):
        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)
        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)
        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)
        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)
        decoder_blocks.append(decoder_block)

    # Create the encoder and decoder
    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))
    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))

    # Create the projection layer
    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)

    # Create the transformer
    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)

    # Initialize the parameters
    for p in transformer.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform_(p)

    return transformer
```

**íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ êµ¬ì¶• (`build_transformer`) ìš”ì•½:**

- **ì—­í• **: ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤(Sequence-to-Sequence) ì‘ì—…ì„ ìœ„í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
- **ì…ë ¥ ë§¤ê°œë³€ìˆ˜**:
    - **ì…ë ¥(ì›ë³¸) ë° ì¶œë ¥(ëª©í‘œ) ì–´íœ˜ í¬ê¸°**: ëª¨ë¸ì´ ë‹¤ë£° ìˆ˜ ìˆëŠ” ë‹¨ì–´ì˜ ì´ ê°œìˆ˜.
    - **ì‹œí€€ìŠ¤ ê¸¸ì´**: ì²˜ë¦¬í•  ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´.
    - **ëª¨ë¸ ì°¨ì› (d_model)**: ëª¨ë¸ ë‚´ë¶€ì—ì„œ ë°ì´í„°ë¥¼ í‘œí˜„í•˜ëŠ” ë²¡í„°ì˜ í¬ê¸°.
    - **ê³„ì¸µ ìˆ˜ (num_layers)**: ì¸ì½”ë”ì™€ ë””ì½”ë”ì— ìŒ“ì„ ë¸”ë¡ì˜ ê°œìˆ˜.
    - **ì–´í…ì…˜ í—¤ë“œ ìˆ˜ (num_attention_heads)**: ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì—ì„œ ë³‘ë ¬ë¡œ ì •ë³´ë¥¼ ì²˜ë¦¬í•  í—¤ë“œì˜ ê°œìˆ˜.
    - **ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ (dropout_rate)**: ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ë“œë¡­ì•„ì›ƒ ì ìš© ë¹„ìœ¨.
    - **í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§ ì°¨ì› (feed_forward_dimensions)**: ì¸ì½”ë”/ë””ì½”ë” ë¸”ë¡ ë‚´ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ì˜ ì°¨ì›.
- **ìƒì„± êµ¬ì„± ìš”ì†Œ**:
    - **ì„ë² ë”© ê³„ì¸µ**: ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜.
    - **ìœ„ì¹˜ ì¸ì½”ë”© ê³„ì¸µ**: ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ì¶”ê°€.
    - **ì¸ì½”ë” ë° ë””ì½”ë” ë¸”ë¡**: ì‹¤ì œ ì¸ì½”ë”© ë° ë””ì½”ë”© ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” í•µì‹¬ ëª¨ë“ˆ.
    - **íˆ¬ì˜ ê³„ì¸µ**: ë””ì½”ë” ì¶œë ¥ì„ ìµœì¢… ë‹¨ì–´ ì˜ˆì¸¡ìœ¼ë¡œ ë³€í™˜.
- **ìµœì¢… ì‘ì—…**: ëª¨ë¸ì˜ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ë¥¼ ì´ˆê¸°í™”í•˜ê³ , ì™„ì„±ëœ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

# ì°¸ê³ ìë£Œ

[Building a Transformer from Scratch: A Step-by-Step Guide](https://medium.com/@sayedebad.777/building-a-transformer-from-scratch-a-step-by-step-guide-a3df0aeb7c9a)

https://github.com/ES7/Transformer-from-Scratch


################################## 

# Transformer â€” Python ì½”ë“œ ì„¤ëª… (Training)

# **Dataset (tformer > dataset.py)**

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset

class TranslationDataset(Dataset):

    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq):
        super().__init__()
        self.seq = seq

        self.ds = ds
        self.tokenizer_src = tokenizer_src
        self.tokenizer_tgt = tokenizer_tgt
        self.src_lang = src_lang
        self.tgt_lang = tgt_lang

        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id("[SOS]")], dtype=torch.int64)
        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id("[EOS]")], dtype=torch.int64)
        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id("[PAD]")], dtype=torch.int64)

    def __len__(self):
        return len(self.ds)

    def __getitem__(self, idx):
        src_target_pair = self.ds[idx]
        src_text = src_target_pair['translation'][self.src_lang]
        tgt_text = src_target_pair['translation'][self.tgt_lang]

        # Transform the text into tokens
        enc_input_tokens = self.tokenizer_src.encode(src_text).ids
        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids

        # Add sos, eos and padding to each sentence
        enc_num_padding_tokens = self.seq - len(enc_input_tokens) - 2  # We will add <s> and </s>
        # We will only add <s>, and </s> only on the label
        dec_num_padding_tokens = self.seq - len(dec_input_tokens) - 1

        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long
        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:
            raise ValueError("Sentence is too long")

        # Add <s> and </s> token
        encoder_input = torch.cat(
            [
                self.sos_token,
                torch.tensor(enc_input_tokens, dtype=torch.int64),
                self.eos_token,
                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),
            ],
            dim=0,
        )

        # Add only <s> token
        decoder_input = torch.cat(
            [
                self.sos_token,
                torch.tensor(dec_input_tokens, dtype=torch.int64),
                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),
            ],
            dim=0,
        )

        # Add only </s> token
        label = torch.cat(
            [
                torch.tensor(dec_input_tokens, dtype=torch.int64),
                self.eos_token,
                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),
            ],
            dim=0,
        )

        # Double check the size of the tensors to make sure they are all seq long
        assert encoder_input.size(0) == self.seq
        assert decoder_input.size(0) == self.seq
        assert label.size(0) == self.seq

        return {
            "encoder_input": encoder_input,  # (seq)
            "decoder_input": decoder_input,  # (seq)
            "encoder_mask": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq)
            "decoder_mask": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq) & (1, seq, seq),
            "label": label,  # (seq)
            "src_text": src_text,
            "tgt_text": tgt_text,
        }

def causal_mask(size):
    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)
    return mask == 0
```

**íŠ¸ëœìŠ¤í¬ë¨¸ í•™ìŠµìš© ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ìš”ì•½:**

ì´ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ëŠ” ë²ˆì—­ ì‘ì—…ì„ ìœ„í•´ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì— í•„ìš”í•œ ë°ì´í„°ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤. í† í°í™”, íŒ¨ë”©, íŠ¹ìˆ˜ í† í° ì²˜ë¦¬, ë§ˆìŠ¤í‚¹ ë“±ì„ í†µí•´ ëª¨ë¸ì´ ì˜¬ë°”ë¥¸ í˜•ì‹ì˜ ì…ë ¥ì„ ë°›ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.

---

### **1. ì´ˆê¸°í™” (`__init__`)**

- **ì—­í• **: ë°ì´í„°ì…‹ì„ ì„¤ì •í•˜ê³  í•„ìš”í•œ ë„êµ¬ë“¤ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
- **ë§¤ê°œë³€ìˆ˜**:
    1. `ds`: ì›ë³¸ ë° ëª©í‘œ ì–¸ì–´ ìŒì„ í¬í•¨í•˜ëŠ” ì‹¤ì œ ë°ì´í„°ì…‹.
    2. `tokenizer_src`: ì›ë³¸ ì–¸ì–´ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ëŠ” ë„êµ¬.
    3. `tokenizer_tgt`: ëª©í‘œ ì–¸ì–´ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ëŠ” ë„êµ¬.
    4. `src_lang`: ì›ë³¸ ì–¸ì–´ ì‹ë³„ì (ì˜ˆ: 'en' for English).
    5. `tgt_lang`: ëª©í‘œ ì–¸ì–´ ì‹ë³„ì (ì˜ˆ: 'it' for Italian).
    6. `seq`: ëª¨ë“  ë¬¸ì¥ì„ ê³ ì •ëœ ê¸¸ì´ë¡œ ë§ì¶”ê¸° ìœ„í•œ ì‹œí€€ìŠ¤(ë¬¸ì¥) ê¸¸ì´.
- **ì¶”ê°€ ì„¤ì •**: ëª©í‘œ ì–¸ì–´ë¥¼ ìœ„í•œ íŠ¹ìˆ˜ í† í° (`[SOS]` (ë¬¸ì¥ ì‹œì‘), `[EOS]` (ë¬¸ì¥ ë), `[PAD]` (íŒ¨ë”©))ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

---

### **2. ê¸¸ì´ ë°˜í™˜ (`__len__`)**

- **ì—­í• **: ë°ì´í„°ì…‹ì— ìˆëŠ” ìƒ˜í”Œì˜ ì´ ê°œìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

---

### **3. ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸° (`__getitem__`)**

- **ì—­í• **: ì£¼ì–´ì§„ ì¸ë±ìŠ¤ `idx`ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° ìƒ˜í”Œì„ ê°€ì ¸ì™€ ì²˜ë¦¬í•©ë‹ˆë‹¤.
- **ì²˜ë¦¬ ê³¼ì •**:
    - **í…ìŠ¤íŠ¸ ì¶”ì¶œ**: ë°ì´í„°ì…‹ì—ì„œ ì›ë³¸ ë° ëª©í‘œ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    - **í…ìŠ¤íŠ¸ í† í°í™”**: ê° í…ìŠ¤íŠ¸ë¥¼ í•´ë‹¹ í† í¬ë‚˜ì´ì €ë¡œ ì˜ê²Œ ìª¼ê°œì–´ í† í°ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
    - **íŒ¨ë”© ê³„ì‚°**: ì •í•´ì§„ `self.seq` ê¸¸ì´ì— ë§ì¶”ê¸° ìœ„í•´ í•„ìš”í•œ íŒ¨ë”© í† í°ì˜ ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    - **ë¬¸ì¥ ê¸¸ì´ ê²€ì¦**: íŠ¹ìˆ˜ í† í° ì¶”ê°€ í›„ì—ë„ ë¬¸ì¥ì´ `self.seq` ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    - **ì¸ì½”ë” ì…ë ¥ ìƒì„±**: `[SOS]` í† í° + ì¸ì½”ë”©ëœ ì›ë³¸ í† í°ë“¤ + `[EOS]` í† í° + íŒ¨ë”© í† í°ë“¤ì„ ì—°ê²°í•˜ì—¬ ë§Œë“­ë‹ˆë‹¤.
    - **ë””ì½”ë” ì…ë ¥ ìƒì„±**: `[SOS]` í† í° + ì¸ì½”ë”©ëœ ëª©í‘œ í† í°ë“¤ + íŒ¨ë”© í† í°ë“¤ì„ ì—°ê²°í•˜ì—¬ ë§Œë“­ë‹ˆë‹¤.
    - **ë ˆì´ë¸” ìƒì„±**: ì¸ì½”ë”©ëœ ëª©í‘œ í† í°ë“¤ + `[EOS]` í† í° + íŒ¨ë”© í† í°ë“¤ì„ ì—°ê²°í•˜ì—¬ ë§Œë“­ë‹ˆë‹¤ (ëª¨ë¸ì´ ì˜ˆì¸¡í•´ì•¼ í•  ì •ë‹µ).
    - **ë§ˆìŠ¤í¬ ìƒì„±**:
        - ì‹¤ì œ í† í°ê³¼ íŒ¨ë”© í† í°ì„ êµ¬ë¶„í•˜ê¸° ìœ„í•œ **ì¸ì½”ë”/ë””ì½”ë” ì…ë ¥ ë§ˆìŠ¤í¬**ë¥¼ ë§Œë“­ë‹ˆë‹¤.
        - ë””ì½”ë”ê°€ ë¯¸ë˜ì˜ ì •ë³´ë¥¼ ë¯¸ë¦¬ ë³´ì§€ ëª»í•˜ë„ë¡ **ì¸ê³¼(causal) ë§ˆìŠ¤í¬**ë¥¼ ë””ì½”ë” ì…ë ¥ì— ì ìš©í•©ë‹ˆë‹¤.
- **ë°˜í™˜**: ì¸ì½”ë” ì…ë ¥, ë””ì½”ë” ì…ë ¥, ë§ˆìŠ¤í¬ë“¤, ë ˆì´ë¸”, ê·¸ë¦¬ê³  ì›ë³¸ í…ìŠ¤íŠ¸ë“¤ì„ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

---

### **4. ì¸ê³¼ ë§ˆìŠ¤í¬ í•¨ìˆ˜ (`causal_mask`)**

- **ì—­í• **: ë””ì½”ë”ê°€ í›ˆë ¨ ì¤‘ì— í˜„ì¬ ìœ„ì¹˜ ì´ì „ì˜ í† í°ë“¤ë§Œ ì°¸ì¡°í•˜ë„ë¡ ì œí•œí•˜ëŠ” ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- **ìƒì„± ë°©ë²•**: ëŒ€ê°ì„ ì„ ì œì™¸í•œ ìƒì‚¼ê° í–‰ë ¬ì„ 1ë¡œ ì±„ìš´ ë‹¤ìŒ, 0ì´ í—ˆìš©ëœ ìœ„ì¹˜(ì´ì „ ìœ„ì¹˜ë¥¼ ì°¸ì¡°í•  ìˆ˜ ìˆëŠ” ìœ„ì¹˜)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¶ˆë¦¬ì–¸ ë§ˆìŠ¤í¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ëŠ” ì¸ê³¼ ê´€ê³„ë¥¼ ê°•ì œí•˜ì—¬ ë””ì½”ë”ê°€ ìˆœì°¨ì ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ìƒì„±í•˜ë„ë¡ ë•ìŠµë‹ˆë‹¤.

# **Configuration (tformer > config.py)**

```python
from pathlib import Path

def get_config():
    return {
        "batch_size": 8,
        "num_epochs": 20,
        "lr": 10**-4,
        "seq": 350,
        "d_model": 512,
        "datasource": 'opus_books',
        "lang_src": "en",
        "lang_tgt": "it",
        "model_folder": "weights",
        "model_basename": "tmodel_",
        "preload": "latest",
        "tokenizer_file": "tokenizer_{0}.json",
        "experiment_name": "runs/tmodel"
    }

def get_weights_file_path(config, epoch: str):
    model_folder = f"{config['datasource']}_{config['model_folder']}"
    model_filename = f"{config['model_basename']}{epoch}.pt"
    return str(Path('.') / model_folder / model_filename)

# Find the latest weights file in the weights folder
def latest_weights_file_path(config):
    model_folder = f"{config['datasource']}_{config['model_folder']}"
    model_filename = f"{config['model_basename']}*"
    weights_files = list(Path(model_folder).glob(model_filename))
    if len(weights_files) == 0:
        return None
    weights_files.sort()
    return str(weights_files[-1])
```

### **1. ì„¤ì • ê°€ì ¸ì˜¤ê¸° (`get_config`) í•¨ìˆ˜**

- **ì—­í• **: ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ ëª¨ë“  ì„¤ì • ë§¤ê°œë³€ìˆ˜ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
- **ì£¼ìš” ë§¤ê°œë³€ìˆ˜ ì„¤ëª…**:
    - `batch_size`: í•œ ë²ˆì˜ í•™ìŠµ ë°˜ë³µì—ì„œ ì²˜ë¦¬ë˜ëŠ” ìƒ˜í”Œì˜ ìˆ˜.
    - `num_epochs`: ì „ì²´ í•™ìŠµ ë°ì´í„°ì…‹ì´ ëª¨ë¸ì„ í†µê³¼í•˜ëŠ” íšŸìˆ˜.
    - `lr`: ì˜µí‹°ë§ˆì´ì €(optimizer)ì˜ í•™ìŠµë¥ (learning rate).
    - `seq`: ì…ë ¥ ë°ì´í„°ì˜ ì‹œí€€ìŠ¤(ë¬¸ì¥) ê¸¸ì´.
    - `d_model`: ëª¨ë¸ì˜ ì°¨ì› (ì˜ˆ: ì…ë ¥ì—ì„œ íŠ¹ì§•ì˜ ìˆ˜).
    - `datasource`: ë°ì´í„°ì…‹ ì†ŒìŠ¤ì˜ ì´ë¦„ (ì˜ˆ: â€˜opus_booksâ€™).
    - `lang_src`: ì›ë³¸ ì–¸ì–´ ì½”ë“œ (ì˜ˆ: â€˜enâ€™ for English).
    - `lang_tgt`: ëª©í‘œ ì–¸ì–´ ì½”ë“œ (ì˜ˆ: â€˜itâ€™ for Italian).
    - `model_folder`: ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ì €ì¥ë  í´ë”.
    - `model_basename`: ì €ì¥ë  ëª¨ë¸ íŒŒì¼ì˜ ê¸°ë³¸ ì´ë¦„.
    - `preload`: ê°€ì¥ ìµœì‹  ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ë¡œë“œí• ì§€ ì—¬ë¶€ (ì˜ˆ: â€˜latestâ€™).
    - `tokenizer_file`: í† í¬ë‚˜ì´ì € íŒŒì¼ ì´ë¦„ì˜ í…œí”Œë¦¿.
    - `experiment_name`: ì‹¤í—˜ ì‹¤í–‰ ì´ë¦„ (ë¡œê¹… ë° ì¶”ì ì— ì‚¬ìš©ë¨).

---

### **2. ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° (`get_weights_file_path`) í•¨ìˆ˜**

- **ì—­í• **: ì„¤ì • ì •ë³´ì™€ ì—í¬í¬ ë²ˆí˜¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- **ì…ë ¥**:
    - `config`: `get_config` í•¨ìˆ˜ì—ì„œ ë°˜í™˜ëœ ì„¤ì • ë”•ì…”ë„ˆë¦¬.
    - `epoch`: ì—í¬í¬ ë²ˆí˜¸ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ìì—´ (ì˜ˆ: â€˜1â€™, â€˜2â€™ ë“±).
- **ê³¼ì •**: í˜„ì¬ ë””ë ‰í† ë¦¬ (`Path('.')`), ëª¨ë¸ í´ë” (ë°ì´í„° ì†ŒìŠ¤ ë° ëª¨ë¸ í´ë” ì´ë¦„ìœ¼ë¡œ êµ¬ì„±), ëª¨ë¸ íŒŒì¼ ì´ë¦„ (ëª¨ë¸ ê¸°ë³¸ ì´ë¦„ê³¼ ì—í¬í¬ ë²ˆí˜¸ë¡œ êµ¬ì„±)ì„ ì¡°í•©í•˜ì—¬ íŒŒì¼ ê²½ë¡œë¥¼ ë§Œë“­ë‹ˆë‹¤.
- **ë°˜í™˜**: ê²°ê³¼ íŒŒì¼ ê²½ë¡œë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

---

### **3. ìµœì‹  ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œ ì°¾ê¸° (`latest_weights_file_path`) í•¨ìˆ˜**

- **ì—­í• **: ì§€ì •ëœ ëª¨ë¸ í´ë”ì—ì„œ ê°€ì¥ ìµœì‹  ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.
- **ì…ë ¥**:
    - `config`: `get_config` í•¨ìˆ˜ì—ì„œ ë°˜í™˜ëœ ì„¤ì • ë”•ì…”ë„ˆë¦¬.
- **ê³¼ì •**:
    1. ëª¨ë¸ í´ë” ê²½ë¡œì™€ íŒŒì¼ ì´ë¦„ íŒ¨í„´(ì™€ì¼ë“œì¹´ë“œ â€˜*â€™ í¬í•¨)ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
    2. `Path.glob`ë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ëŠ” ëª¨ë“  íŒŒì¼ì„ ëª¨ë¸ í´ë”ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    3. ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ì—†ìœ¼ë©´ `None`ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    4. íŒŒì¼ì´ ë°œê²¬ë˜ë©´, íŒŒì¼ ì´ë¦„ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•œ í›„, ê°€ì¥ ìµœì‹  íŒŒì¼(ì •ë ¬ëœ ëª©ë¡ì˜ ë§ˆì§€ë§‰ íŒŒì¼)ì˜ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
- **ë°˜í™˜**: ê°€ì¥ ìµœì‹  ê°€ì¤‘ì¹˜ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

# **Training (tformer > trainpy)**

```python
from model import build_transformer
from dataset import TranslationDataset, causal_mask
from config import get_config, get_weights_file_path, latest_weights_file_path

import torchtext.datasets as datasets
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
from torch.optim.lr_scheduler import LambdaLR

import warnings
from tqdm import tqdm
import os
from pathlib import Path

# Huggingface datasets and tokenizers
from datasets import load_dataset
from tokenizers import Tokenizer
from tokenizers.models import WordLevel
from tokenizers.trainers import WordLevelTrainer
from tokenizers.pre_tokenizers import Whitespace

import torchmetrics
from torch.utils.tensorboard import SummaryWriter
```

### `get_all_sentences` í•¨ìˆ˜

```python
def get_all_sentences(ds, lang):
    for item in ds:
        yield item['translation'][lang]
```

**`get_all_sentences` í•¨ìˆ˜ ìš”ì•½:**

- **ì—­í• **: ì£¼ì–´ì§„ ë°ì´í„°ì…‹(`ds`)ì—ì„œ íŠ¹ì • ì–¸ì–´(`lang`)ì˜ ë¬¸ì¥ë“¤ì„ í•˜ë‚˜ì”© ë°˜í™˜í•˜ëŠ” ì œë„ˆë ˆì´í„°(generator) í•¨ìˆ˜ì…ë‹ˆë‹¤.
- **ì‘ë™ ë°©ì‹**:
    - ë°ì´í„°ì…‹ì˜ ê° í•­ëª©ì„ ë°˜ë³µí•©ë‹ˆë‹¤.
    - ê° í•­ëª©ì€ `translation`ì´ë¼ëŠ” í‚¤ë¥¼ ê°€ì§€ê³  ìˆì–´ì•¼ í•˜ë©°, ì´ í‚¤ ì•„ë˜ì— ì—¬ëŸ¬ ì–¸ì–´ë¡œ ëœ ë²ˆì—­ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    - ì§€ì •ëœ `lang`ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì¥ì„ **`yield`** í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í•˜ë‚˜ì”© "ìƒì„±"í•©ë‹ˆë‹¤.
- **ì¥ì **: `yield`ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨, ëª¨ë“  ë¬¸ì¥ì„ í•œêº¼ë²ˆì— ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ëŠ” ëŒ€ì‹  í•„ìš”í•  ë•Œë§ˆë‹¤ í•˜ë‚˜ì”© ìƒì„±í•˜ì—¬ **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**ì„ ë†’ì…ë‹ˆë‹¤.

### `get_or_build_tokenizer` í•¨ìˆ˜

```python
def get_or_build_tokenizer(config, ds, lang):
    tokenizer_path = Path(config['tokenizer_file'].format(lang))
    if not Path.exists(tokenizer_path):
        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour
        tokenizer = Tokenizer(WordLevel(unk_token="[UNK]"))
        tokenizer.pre_tokenizer = Whitespace()
        trainer = WordLevelTrainer(special_tokens=["[UNK]", "[PAD]", "[SOS]", "[EOS]"], min_frequency=2)
        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)
        tokenizer.save(str(tokenizer_path))
    else:
        tokenizer = Tokenizer.from_file(str(tokenizer_path))
    return tokenizer
```

`get_or_build_tokenizer` í•¨ìˆ˜ ìš”ì•½:

- **ì—­í• **: ì£¼ì–´ì§„ ì–¸ì–´ì— ëŒ€í•œ í† í¬ë‚˜ì´ì € íŒŒì¼ì„ í™•ì¸í•˜ê³ , ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒˆë¡œ êµ¬ì¶• ë° í•™ìŠµí•˜ì—¬ ì €ì¥í•˜ê³ , ì¡´ì¬í•˜ë©´ ê¸°ì¡´ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
- **ì£¼ìš” êµ¬ì„± ìš”ì†Œ ë° ê³¼ì •**:
    1. **í† í¬ë‚˜ì´ì € ê°ì²´ ìƒì„±**:
        - `Tokenizer(models.WordLevel(unk_token="[UNK]"))`: ìƒˆë¡œìš´ `Tokenizer` ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ë‹¨ì–´ ìˆ˜ì¤€(WordLevel) ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©°, ëª¨ë¥´ëŠ” ë‹¨ì–´ëŠ” `[UNK]` í† í°ìœ¼ë¡œ ì²˜ë¦¬í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.
    2. **ì „ì²˜ë¦¬(Pre-tokenizer) ì„¤ì •**:
        - `tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()`: í…ìŠ¤íŠ¸ë¥¼ ê³µë°±(`whitespace`) ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬í•˜ë„ë¡ ì „ì²˜ë¦¬ê¸°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    3. **í•™ìŠµ ë„êµ¬(Trainer) ì„¤ì •**:
        - `WordLevelTrainer(special_tokens=["[UNK]", "[PAD]", "[SOS]", "[EOS]"], min_frequency=2)`: `WordLevelTrainer`ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            - **íŠ¹ìˆ˜ í† í°**: `[UNK]` (ì•Œ ìˆ˜ ì—†ëŠ” ë‹¨ì–´), `[PAD]` (íŒ¨ë”©), `[SOS]` (ë¬¸ì¥ ì‹œì‘), `[EOS]` (ë¬¸ì¥ ë)ì„ ì •ì˜í•©ë‹ˆë‹¤.
            - **ìµœì†Œ ë¹ˆë„**: ìµœì†Œ 2ë²ˆ ì´ìƒ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ë§Œ ì–´íœ˜ì— í¬í•¨ì‹œí‚¤ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.
    4. **í† í¬ë‚˜ì´ì € í•™ìŠµ (íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°)**:
        - `tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)`: `get_all_sentences` í•¨ìˆ˜ë¥¼ í†µí•´ ë°ì´í„°ì…‹ì—ì„œ í•´ë‹¹ ì–¸ì–´ì˜ ë¬¸ì¥ë“¤ì„ í•˜ë‚˜ì”© ê°€ì ¸ì™€ í† í¬ë‚˜ì´ì €ë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
        - `tokenizer.save(str(tokenizer_path))`: í•™ìŠµëœ í† í¬ë‚˜ì´ì €ë¥¼ ì§€ì •ëœ íŒŒì¼ ê²½ë¡œì— ì €ì¥í•©ë‹ˆë‹¤.
    5. **í† í¬ë‚˜ì´ì € ë¡œë“œ (íŒŒì¼ì´ ìˆëŠ” ê²½ìš°)**:
        - `Tokenizer.from_file(str(tokenizer_path))`: ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í† í¬ë‚˜ì´ì € íŒŒì¼ì—ì„œ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
- **ë°˜í™˜ ê°’**: ìƒˆë¡œ êµ¬ì¶•í•˜ê³  í•™ìŠµí–ˆê±°ë‚˜ ê¸°ì¡´ íŒŒì¼ì—ì„œ ë¡œë“œëœ í† í¬ë‚˜ì´ì € ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ í† í¬ë‚˜ì´ì €ëŠ” ì´í›„ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‘ì—…(ì˜ˆ: ë‹¨ì–´ë¥¼ ìˆ«ìë¡œ, ìˆ«ìë¥¼ ë‹¨ì–´ë¡œ ë³€í™˜)ì— ì‚¬ìš©ë©ë‹ˆë‹¤.

### `get_ds` í•¨ìˆ˜

```python
def get_ds(config):
    # It only has the train split, so we divide it overselves
    ds_raw = load_dataset(f"{config['datasource']}", f"{config['lang_src']}-{config['lang_tgt']}", split='train')

    # Build tokenizers
    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])
    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])

    # Keep 90% for training, 10% for validation
    train_ds_size = int(0.9 * len(ds_raw))
    val_ds_size = len(ds_raw) - train_ds_size
    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])

    train_ds = TranslationDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq'])
    val_ds = TranslationDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq'])

    # Find the maximum length of each sentence in the source and target sentence
    max_len_src = 0
    max_len_tgt = 0

    for item in ds_raw:
        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids
        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids
        max_len_src = max(max_len_src, len(src_ids))
        max_len_tgt = max(max_len_tgt, len(tgt_ids))

    print(f'Max length of source sentence: {max_len_src}')
    print(f'Max length of target sentence: {max_len_tgt}')

    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)
    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)

    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt
```

**ë°ì´í„° ì¤€ë¹„ ë° ë¡œë“œ ê³¼ì • ìš”ì•½:**

---

### **1. ë°ì´í„°ì…‹ ë¡œë“œ**

- **ì„¤ì •**: ì„¤ì •(`config`)ì—ì„œ ì§€ì •ëœ ë°ì´í„° ì†ŒìŠ¤ ì´ë¦„ê³¼ ë²ˆì—­ ì‘ì—…ì„ ìœ„í•œ ì–¸ì–´ ìŒ(ì˜ˆ: 'en-it')ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
- **ì‹¤í–‰**: í•´ë‹¹ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤. ì´ë•Œ, `split='train'`ì„ ì§€ì •í•˜ì—¬ **í•™ìŠµìš© ë¶„í• (training split)**ë§Œ ë¡œë“œí•©ë‹ˆë‹¤.

---

### **2. í† í¬ë‚˜ì´ì € êµ¬ì¶•**

- **ì„¤ì •**: ì„¤ì • ë”•ì…”ë„ˆë¦¬, ì´ì „ì— ë¡œë“œëœ ì›ë³¸ ë°ì´í„°ì…‹, ê·¸ë¦¬ê³  ì›ë³¸ ë° ëª©í‘œ ì–¸ì–´ë¥¼ `get_or_build_tokenizer` í•¨ìˆ˜ì— ì „ë‹¬í•©ë‹ˆë‹¤.
- **ì‹¤í–‰**: ì´ í•¨ìˆ˜ëŠ” í•´ë‹¹ ì–¸ì–´ì— ëŒ€í•œ í† í¬ë‚˜ì´ì €ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ìƒˆë¡œ êµ¬ì¶•í•˜ê³  í•™ìŠµì‹œì¼œ ì €ì¥í•˜ë©°, ì¡´ì¬í•˜ë©´ ê¸°ì¡´ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.

---

### **3. ë°ì´í„°ì…‹ ë¶„í• **

- **ì‹¤í–‰**: ë¡œë“œëœ ì›ë³¸ ë°ì´í„°ì…‹ì„ ê³„ì‚°ëœ í¬ê¸°(ì˜ˆ: í›ˆë ¨ìš© 90%, ê²€ì¦ìš© 10%)ì— ë”°ë¼ **í›ˆë ¨ ì„¸íŠ¸(training set)**ì™€ **ê²€ì¦ ì„¸íŠ¸(validation set)**ë¡œ ë¬´ì‘ìœ„ ë¶„í• í•©ë‹ˆë‹¤.

---

### **4. ë²ˆì—­ ë°ì´í„°ì…‹ ìƒì„±**

- **í´ë˜ìŠ¤ í™œìš©**: `dataset.py` íŒŒì¼ì˜ `TranslationDataset` í´ë˜ìŠ¤ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
- **ë§¤ê°œë³€ìˆ˜ ì „ë‹¬**: ë¶„í• ëœ ì›ë³¸ í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„°ì…‹, ì›ë³¸ ë° ëª©í‘œ ì–¸ì–´ í† í¬ë‚˜ì´ì €ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤. ë˜í•œ, ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¨ ì›ë³¸ ì–¸ì–´, ëª©í‘œ ì–¸ì–´, ê·¸ë¦¬ê³  ì‹œí€€ìŠ¤ ê¸¸ì´(ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´)ë„ í•¨ê»˜ ì „ë‹¬í•©ë‹ˆë‹¤.

---

### **5. ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ ì°¾ê¸°**

- **ê³¼ì •**: ì›ë³¸ ë°ì´í„°ì…‹ì˜ ê° í•­ëª©ì„ ë°˜ë³µí•˜ë©´ì„œ ì›ë³¸ ë° ëª©í‘œ ë¬¸ì¥ì„ ì¸ì½”ë”©í•˜ì—¬ í† í° IDë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
- **ëª©ì **: í˜„ì¬ ë¬¸ì¥ ê¸¸ì´ê°€ ê¸°ì¡´ì˜ ìµœëŒ€ ê¸¸ì´ë³´ë‹¤ ê¸¸ë©´ ì—…ë°ì´íŠ¸í•˜ë©°, ìµœì¢…ì ìœ¼ë¡œ ì›ë³¸ ë° ëª©í‘œ ë¬¸ì¥ì˜ **ìµœëŒ€ ê¸¸ì´**ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. (ì´ëŠ” ëª¨ë¸ ì„¤ê³„ ë˜ëŠ” ë””ë²„ê¹…ì— ìœ ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)

---

### **6. ë°ì´í„° ë¡œë” ìƒì„±**

- **ì—­í• **: í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„°ì…‹ì„ ìœ„í•œ ë°˜ë³µ ê°€ëŠ¥í•œ ë°ì´í„° ë¡œë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- **ì„¤ì •**: ì„¤ì •ì—ì„œ í›ˆë ¨ ë°ì´í„° ë¡œë”ì˜ ë°°ì¹˜ í¬ê¸°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
- **ì„±ëŠ¥ í–¥ìƒ**: í•™ìŠµ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ë°ì´í„°ì…‹ì„ **ì„ìŠµë‹ˆë‹¤(shuffle)**.

---

### **7. ë°˜í™˜ ê°’**

- **ê²°ê³¼**: ìµœì¢…ì ìœ¼ë¡œ í›ˆë ¨ ë°ì´í„° ë¡œë”, ê²€ì¦ ë°ì´í„° ë¡œë”, ê·¸ë¦¬ê³  ì›ë³¸ ë° ëª©í‘œ ì–¸ì–´ í† í¬ë‚˜ì´ì €ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ ê²°ê³¼ë¬¼ë“¤ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ í•™ìŠµì— ì§ì ‘ ì‚¬ìš©ë©ë‹ˆë‹¤.

### `get_model` í•¨ìˆ˜

```python
def get_model(config, vocab_src_len, vocab_tgt_len):
    model = build_transformer(vocab_src_len, vocab_tgt_len, config["seq"], config['seq'], d_model=config['d_model'])
    return model
```

**íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ë¹Œë“œ í•¨ìˆ˜ ìš”ì•½:**

---

`build_transformer` í•¨ìˆ˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ë§Œë“œëŠ” ê³¼ì •ì„ ê°„ë‹¨í•˜ê²Œ ê°ì‹¸ëŠ”(wrapping) í•¨ìˆ˜ì…ë‹ˆë‹¤.

- **ì—­í• **: ëª¨ë¸ ìƒì„±ì˜ ë³µì¡í•œ ì„¸ë¶€ ì‚¬í•­ì„ ìˆ¨ê¸°ê³ , ì„¤ì •(configuration)ê³¼ ì–´íœ˜ í¬ê¸°(vocabulary sizes)ë§Œ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ì¤€ë¹„ì‹œí‚µë‹ˆë‹¤.
- **ë°˜í™˜**: í•™ìŠµ ë° í‰ê°€ì— ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

### `train_model` í•¨ìˆ˜

```python
def train_model(config):
    # Define the device
    device = "cuda" if torch.cuda.is_available() else "mps" if torch.has_mps or torch.backends.mps.is_available() else "cpu"
    print("Using device:", device)
    if (device == 'cuda'):
        print(f"Device name: {torch.cuda.get_device_name(device.index)}")
        print(f"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB")
    elif (device == 'mps'):
        print(f"Device name: <mps>")
    else:
        print("NOTE: If you have a GPU, consider using it for training.")
        print("      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc")
        print("      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu")
    device = torch.device(device)

    # Make sure the weights folder exists
    Path(f"{config['datasource']}_{config['model_folder']}").mkdir(parents=True, exist_ok=True)

    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)
    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)
    # Tensorboard
    writer = SummaryWriter(config['experiment_name'])

    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)

    # If the user specified a model to preload before training, load it
    initial_epoch = 0
    global_step = 0
    preload = config['preload']
    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None
    if model_filename:
        print(f'Preloading model {model_filename}')
        state = torch.load(model_filename)
        model.load_state_dict(state['model_state_dict'])
        initial_epoch = state['epoch'] + 1
        optimizer.load_state_dict(state['optimizer_state_dict'])
        global_step = state['global_step']
    else:
        print('No model to preload, starting from scratch')

    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)

    for epoch in range(initial_epoch, config['num_epochs']):
        torch.cuda.empty_cache()
        model.train()
        batch_iterator = tqdm(train_dataloader, desc=f"Processing Epoch {epoch:02d}")
        for batch in batch_iterator:

            encoder_input = batch['encoder_input'].to(device) # (b, seq)
            decoder_input = batch['decoder_input'].to(device) # (B, seq)
            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq)
            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq, seq)

            # Run the tensors through the encoder, decoder and the projection layer
            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq, d_model)
            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq, d_model)
            proj_output = model.project(decoder_output) # (B, seq, vocab_size)

            # Compare the output with the label
            label = batch['label'].to(device) # (B, seq)

            # Compute the loss using a simple cross entropy
            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))
            batch_iterator.set_postfix({"loss": f"{loss.item():6.3f}"})

            # Log the loss
            writer.add_scalar('train loss', loss.item(), global_step)
            writer.flush()

            # Backpropagate the loss
            loss.backward()

            # Update the weights
            optimizer.step()
            optimizer.zero_grad(set_to_none=True)

            global_step += 1

        # Run validation at the end of every epoch
        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq'], device, lambda msg: batch_iterator.write(msg), global_step, writer)

        # Save the model at the end of every epoch
        model_filename = get_weights_file_path(config, f"{epoch:02d}")
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'global_step': global_step
        }, model_filename)
```

**íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ í•µì‹¬ í•™ìŠµ ë¡œì§ ìš”ì•½:**

ì´ í•¨ìˆ˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ í•µì‹¬ í•™ìŠµ ë¡œì§ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤. ì¥ì¹˜(CPU/GPU) ì„ íƒ, ëª¨ë¸ ì´ˆê¸°í™”, ì˜µí‹°ë§ˆì´ì € ì„¤ì •, í•™ìŠµ ë£¨í”„ ì‹¤í–‰, ê·¸ë¦¬ê³  Tensorboardë¥¼ ì´ìš©í•œ í•™ìŠµ ì§„í–‰ ìƒí™© ë¡œê¹… ë“±ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

---

### **1. ì¥ì¹˜ í™•ì¸ ë° ì„¤ì •**

- **íŒë‹¨**: "cuda"(GPU), "mps"(Multi-Process Service), ë˜ëŠ” "cpu" ì¤‘ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì¥ì¹˜ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
- **ì •ë³´ ì¶œë ¥**: "cuda"ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ì¥ì¹˜ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

---

### **2. ëª¨ë¸ ê°€ì¤‘ì¹˜ í´ë” ìƒì„±**

- **ëª©ì **: ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•  í´ë”ê°€ ì—†ëŠ” ê²½ìš° ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.

---

### **3. ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°**

- **ì²˜ë¦¬**: ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³ , ì´ë¥¼ í•™ìŠµ ì„¸íŠ¸ì™€ ê²€ì¦ ì„¸íŠ¸ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
- **ë„êµ¬ ì¤€ë¹„**: ì›ë³¸ ë° ëª©í‘œ ì–¸ì–´ì— ëŒ€í•œ í† í¬ë‚˜ì´ì €ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

---

### **4. ëª¨ë¸ ì´ˆê¸°í™”**

- **ìƒì„±**: `get_model` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
- **ì¥ì¹˜ ì´ë™**: ìƒì„±ëœ ëª¨ë¸ì„ ì„¤ì •ëœ ì¥ì¹˜(GPU/CPU)ë¡œ ì´ë™ì‹œí‚µë‹ˆë‹¤.

---

### **5. Tensorboard ì´ˆê¸°í™”**

- **ë„êµ¬ ìƒì„±**: í•™ìŠµ ì§„í–‰ ìƒí™©ì„ ì‹œê°í™”í•˜ê¸° ìœ„í•´ `SummaryWriter`ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

---

### **6. ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”**

- **ì„ íƒ**: `Adam` ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•˜ë©°, ì„¤ì •ëœ í•™ìŠµë¥ (learning rate)ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

---

### **7. ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ (ì„ íƒ ì‚¬í•­)**

- **ì¡°ê±´ë¶€ ì‹¤í–‰**: ì„¤ì •(`config`)ì— ì§€ì •ëœ ê²½ìš°, ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ì™€ ì˜µí‹°ë§ˆì´ì € ìƒíƒœë¥¼ ë¡œë“œí•˜ì—¬ ì´ì–´ì„œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

---

### **8. ì†ì‹¤ í•¨ìˆ˜ ì •ì˜**

- **ì„ íƒ**: `CrossEntropyLoss`ë¥¼ ì‚¬ìš©í•˜ë©°, ë ˆì´ë¸” ìŠ¤ë¬´ë”©(label smoothing)ì„ ì ìš©í•©ë‹ˆë‹¤.

---

### **9. í•™ìŠµ ë£¨í”„ (`Training Loop`)**

- **ì—í¬í¬ ë°˜ë³µ**: ì „ì²´ ì—í¬í¬ ìˆ˜ë§Œí¼ ë°˜ë³µí•©ë‹ˆë‹¤.
- **ë°°ì¹˜ ë°˜ë³µ**: ê° ì—í¬í¬ ë‚´ì—ì„œ í•™ìŠµ ë°ì´í„° ë¡œë”ì˜ ë°°ì¹˜ë“¤ì„ ë°˜ë³µ ì²˜ë¦¬í•©ë‹ˆë‹¤.
- **ìˆœì „íŒŒ**: ì¸ì½”ë”, ë””ì½”ë”, ê·¸ë¦¬ê³  íˆ¬ì˜(projection) ê³„ì¸µì„ í†µí•´ ìˆœì „íŒŒ(forward pass)ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- **ì†ì‹¤ ê³„ì‚° ë° ì—­ì „íŒŒ**: ì†ì‹¤ì„ ê³„ì‚°í•˜ê³ , ì—­ì „íŒŒ(backpropagation)ë¥¼ í†µí•´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
- **ë¡œê¹…**: Tensorboardë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ì‹¤ ê°’ì„ ë¡œê¹…í•©ë‹ˆë‹¤.
- **ì—í¬í¬ ì¢…ë£Œ ì‹œ**:
    - ê° ì—í¬í¬ê°€ ëë‚  ë•Œë§ˆë‹¤ **ê²€ì¦(validation)**ì„ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
    - ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

# TensorBoard ì‹¤í–‰

```bash
tensorboard --logdir=./BasicModel/Transformer/runs
```

# Tformer_Tutorial.ipynb ì‹¤ìŠµ

# ì°¸ê³ ìë£Œ

[Training a Transformer Model from Scratch](https://medium.com/@sayedebad.777/training-a-transformer-model-from-scratch-25bb270f5888)

https://github.com/ES7/Transformer-from-Scratch

```bash
cd /home/kime/ws_openvla ; /usr/bin/env /home/kime/miniconda3/envs/ws_mdl/bin/python /home/kime/ws_openvla/KIMe_BasicModel/Transformer/tformer/train.py 
```

##################################  

# Transformer â€” Python ì½”ë“œ ê¸°ë°˜ ì‹¤ìŠµ (í›ˆë ¨) Check Point

[Transformer_Train_CheckPT.zip](attachment:9de9c604-b327-46c2-8283-9ad828ec597b:Transformer_Train_CheckPT.zip)

í´ë” `lemon-mint/weights`

[tmodel_00.pt](attachment:9fed9a68-d6bd-41ff-892d-1471f519304d:tmodel_00.pt)

[tmodel_10.pt](attachment:4ef90b1e-fb63-4df0-a2ba-ecf458c442a7:tmodel_10.pt)

[tmodel_20.pt](attachment:36574036-c477-4bbc-b0ba-c20e53058c9c:tmodel_10.pt)

[tmodel_29.pt](attachment:c55f499b-133e-4367-a78d-2dcd36dcb71b:tmodel_29.pt)

í´ë” `runs/tmodel`

[events.out.tfevents.1748068267.kime-A6000.3472412.0](attachment:d89e2f0e-a605-4258-acbd-d3d26290db56:events.out.tfevents.1748068267.kime-A6000.3472412.0)

###############################  

# Diffusion ëª¨ë¸ ê¸°ì´ˆ ì´ë¡ 

**ì‘ì„±:** `ì´ˆê¸°` July 30, 2025 

# **Diffusion ëª¨ë¸ ê°œìš”**

## **Generative vs. Discriminative**

- Generative models learn the data distribution

![image.png](attachment:66b704c2-6653-4a3a-83d8-e9bff8df1d1d:image.png)

## **Generative Models â€” Learning to generate data**

![image.png](attachment:832e53cd-6bca-443e-8dd7-cec9462bbc01:image.png)

## Different **Generative Models**

![image.png](attachment:c678bb9c-cf32-47eb-84fe-bcaa28ee33d5:image.png)

# **Diffusion ëª¨ë¸ ê¸°ì´ˆ ì´ë¡ **

## Diffusion Models as Stacking VAEs

![image.png](attachment:1c53c049-398f-463a-8428-13da91daabd8:image.png)

![image.png](attachment:f6278021-546f-489b-9f22-d0ba44c79dad:image.png)

### Forward diffusion process â€” stacking fixed VAE encoders

- **Adding Gaussian noise according to schedule $ğ›½_t$:**
    
    ![image.png](attachment:381605c6-7545-4aea-bd69-f4433341cade:image.png)
    
    **Forward Process (Step):** $q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t}\mathbf{x}_{t-1}, \beta_t \mathbf{I})$
    
    - $\mathcal{N}(\mathbf{x}t; \sqrt{1 - \beta_t}\mathbf{x}{t-1}, \beta_t \mathbf{I})$
        - ë‹¤ë³€ëŸ‰ ì •ê·œ ë¶„í¬(Multivariate Normal Distribution)
        - xtëŠ” ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ë³€ìˆ˜
        - Î¼ëŠ” ë¶„í¬ì˜ í‰ê· (Mean) ë²¡í„°
        - Î£ëŠ” ë¶„í¬ì˜ ê³µë¶„ì‚°(Covariance) í–‰ë ¬
    
    **Forward Process (Total):** $q(\mathbf{x}_{1:T} | \mathbf{x}0) = \prod_{t=1}^{T} q(\mathbf{x}_t | \mathbf{x}_{t-1})$
    

- **Sampling of $x_ğ‘¡$ at arbitrary timestep ğ‘¡ in closed form:**
    
    ![image.png](attachment:eedbac94-248c-40c3-b662-6c7ec5f5542c:c989a06f-72c0-4050-892e-e8c02bf6d569.png)
    
    ![image.png](attachment:584c34fa-8a32-44d6-947e-0b485eeccb5d:image.png)
    
    ![image.png](attachment:0161afdd-96b3-4517-aa52-6cfad97a7dcc:image.png)
    
    ![image.png](attachment:f643e7a6-8724-4ca1-8721-ebd22019b2a2:image.png)
    
    ![image.png](attachment:d99fb4f3-ea4b-4b2d-aa84-15af6ef6e592:image.png)
    

### Reverse denoising process â€” stacking `learnable` VAE decoders

![image.png](attachment:dac7f402-8b29-47ef-9794-8683e80374ef:image.png)

- Sample
    
    ![image.png](attachment:6e17c0e2-3855-4fb1-bdc7-6eb8394f2a11:image.png)
    
- Iteratively sample
    
    ![image.png](attachment:c0c5d3af-bfcb-46a7-8417-cf336b52417d:image.png)
    
- $q(\textbf{x}_{t-1}|\textbf{x}_{t})$ not directly tractable â€” But can be estimated with a Gaussian distribution if $ğ›½_ğ‘¡$ is small at each step
    
    ![image.png](attachment:77d45f03-da32-4f27-b01d-1a77d57dc0b4:image.png)
    
    ![image.png](attachment:9dda50c1-8c89-4b8f-9ed7-cfe6e7c52c65:image.png)
    
    ![image.png](attachment:06bdeaea-0a72-433a-b2ed-b51125deb740:image.png)
    
    ![image.png](attachment:d0988e32-2d48-48c2-871b-fbc7a696bab5:image.png)
    

### **Recap: Variational Autoencoders**

- **Solution:** having intermediate latent variables to reduce the gap of each step
    - Hierarchical VAEs (Stacking VAEs)
        - Each step, the decoder removes part of the noise
        - Provides a seed model closer to final distribution
            
            ![image.png](attachment:0a9b58ec-73ff-44db-bfcb-ca77fca5f8d3:image.png)
            
            ![image.png](attachment:5716afac-7559-41d2-a678-1880ad55c6be:image.png)
            
        - Each step incrementally recovers the final distribution
            
            ![image.png](attachment:cec49299-d73d-4f71-8565-cd233f45fbde:image.png)
            
- **Limitations of VAEs**
    - Decoder must transform a standard Gaussian all the way to the target distribution in one-step
        
        ![image.png](attachment:e3707ce3-4c97-4956-b42b-0ba1eb27ce43:image.png)
        
        - Often too large a gap
        - Blurry results are generated
            
            ![image.png](attachment:e023fdf3-358c-4fe8-a7c7-0792cc95d7a3:image.png)
            
- Training: maximize the ELBO (Evidence Lower Bound)
    
    ![image.png](attachment:8efb52de-60fc-419d-be30-cf0662188500:image.png)
    
    ![image.png](attachment:05c242e4-df15-42b1-9a55-ebd50a91c08a:image.png)
    
    ![image.png](attachment:ef21f705-7709-4efa-a448-bc59789af889:image.png)
    
- Decoder: a generative model that transforms a Gaussian variable z to real data
- Encoder: an inference model that approximates the posterior ğ‘(ğ‘§|ğ‘¥)
- VAEs: a likelihood-based generative model

### Diffusion models are special cases of Stacking VAEs

![image.png](attachment:799d8ab2-fbaa-41b9-9ce0-052a8090efd9:image.png)

- Diffusion models use fixed inference encoders
    
    ![image.png](attachment:82d2dfee-ab70-421c-8d6f-dbf87e117c7d:image.png)
    
- In VAEs, encoders are learned with KL-divergence between the posterior and the prior
- Suffers from the â€˜posterior-collapseâ€™ issue :
    - Posterior collapseëŠ” ì ì¬ ë³€ìˆ˜ $z$ì˜ ê·¼ì‚¬ í›„ë°© ë¶„í¬ $q(z|x)$ê°€ ì‚¬ì „ ë¶„í¬ $p(z)$ì— ì§€ë‚˜ì¹˜ê²Œ ê°€ê¹Œì›Œì ¸ì„œ ì ì¬ ë³€ìˆ˜ê°€ ë°ì´í„°ì˜ ì •ë³´ë¥¼ ê±°ì˜ ë°˜ì˜í•˜ì§€ ì•Šê²Œ ë˜ëŠ” í˜„ìƒì…ë‹ˆë‹¤.
    - ì ì¬ ë³€ìˆ˜ $z$ê°€ ë°ì´í„° $x$ì— ëŒ€í•´ ë…ë¦½ì ì´ ë˜ì–´ ì˜ë¯¸ ìˆëŠ” í‘œí˜„ì„ í•™ìŠµí•˜ì§€ ëª»í•©ë‹ˆë‹¤.
    - ëª¨ë¸ì´ ìƒì„± í’ˆì§ˆì´ ë–¨ì–´ì§€ê±°ë‚˜, ì ì¬ ê³µê°„ì´ í™œìš©ë˜ì§€ ì•Šì•„ ë‹¤ì–‘ì„±ì´ ê°ì†Œí•©ë‹ˆë‹¤.

---

## Diffusion Models: Training, Sampling

- Final Objective
    
    ![image.png](attachment:dc8d42ee-9a04-425b-9fb5-121d8e561614:image.png)
    
- In DDPM, simplified version
    
    ![image.png](attachment:85811f63-0c74-4a1f-a542-fc36b1e5b008:image.png)
    
    ![image.png](attachment:833dd5ab-d7a2-4017-bb3b-cd9b6ba8dcb3:image.png)
    

---

## Many Steps in Diffusion (Sampling)

- Slow in generation
- In Training, we randomly sample one time step
- But in inference, we must transit from T to 0
    - 1000 steps
    - extremely slow for raw images/signals
        
        ![image.png](attachment:fe5bac93-8d07-4df4-868b-5886803f95d7:image.png)
        
        ![image.png](attachment:d93ba831-a496-47f3-a59d-4be123696d90:image.png)
        
        ![image.png](attachment:166f718c-2f7e-4d23-8808-caea88ce102b:image.png)
        
- DDIM with Fewer Steps Sampling
    
    ![image.png](attachment:9ab7b49f-d2aa-48f4-a7ae-184fec0732f3:image.png)
    

### DDPM vs DDIM ì°¨ì´ì 

Diffusion ëª¨ë¸ì—ì„œ ìƒ˜í”Œ ìƒì„±ì„ ìœ„í•œ **reverse diffusion process**ë¥¼ ë°©ë²•

- **DDPM**(Denoising Diffusion Probabilistic Models)
- **DDIM**(Denoising Diffusion Implicit Models)

í•˜ì§€ë§Œ ë‘ ëª¨ë¸ì€ **ìƒ˜í”Œë§ ê³¼ì •ì˜ ë°©ì‹**ê³¼ **ì†ë„**, ê·¸ë¦¬ê³  **ìƒ˜í”Œ ë‹¤ì–‘ì„±** ì¸¡ë©´ì—ì„œ ì¤‘ìš”í•œ ì°¨ì´ë¥¼ ê°€ì§€ê³  ìˆìŒ.

---

### âœ… 1. DDPM (Denoising Diffusion Probabilistic Models)

**ì œì•ˆì**: Ho et al. (2020)

**í•µì‹¬ ì•„ì´ë””ì–´**:

DDPMì€ forward ê³¼ì •ì—ì„œ ì ì  ë” ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•´ê°€ë©° ë°ì´í„°ë¥¼ ì •ê·œ ë¶„í¬ë¡œ ë§Œë“  ë’¤, ì´ë¥¼ ì—­ìœ¼ë¡œ ì œê±°í•˜ëŠ” **í™•ë¥ ì  reverse process**ë¥¼ í†µí•´ ìƒ˜í”Œì„ ìƒì„±í•¨.

### ğŸ” Forward Process:

- ë°ì´í„° $x_0$ì— ì ì§„ì ìœ¼ë¡œ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•´ì„œ $x_1, x_2, ..., x_T$ê¹Œì§€ ìƒì„±
- ë…¸ì´ì¦ˆëŠ” ë‹¤ìŒ ì‹ìœ¼ë¡œ ì¶”ê°€ë¨:
    
    $q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)$
    

### ğŸ”„ Reverse Process:

- í•™ìŠµëœ ì‹ ê²½ë§ $\epsilon_\theta(x_t, t)$ì´ $x_t$ì—ì„œ ì›ë˜ì˜ noise $\epsilon$ì„ ì˜ˆì¸¡
- ì—­ë°©í–¥ ìƒ˜í”Œë§ì€ ë‹¤ìŒê³¼ ê°™ì€ **í™•ë¥ ì ** í˜•íƒœ:
    
    $p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$
    
- í•œ ìŠ¤í…ë§ˆë‹¤ Gaussian ìƒ˜í”Œë§ì´ ìˆìŒ â†’ **ëŠë¦¼**

### â—ï¸â­íŠ¹ì§•:

- **ë†’ì€ ìƒ˜í”Œ í’ˆì§ˆ**
- **ìƒ˜í”Œë§ì´ ëŠë¦¼** (ë³´í†µ 1000 ìŠ¤í… ì´ìƒ í•„ìš”)
- **ë‹¤ì–‘ì„± ë†’ìŒ** (í™•ë¥ ì  ê³¼ì •ì´ë¯€ë¡œ ë‹¤ì–‘í•œ ìƒ˜í”Œ ê°€ëŠ¥)

---

### âœ… 2. DDIM (Denoising Diffusion Implicit Models)

**ì œì•ˆì**: Song et al. (2021)

**í•µì‹¬ ì•„ì´ë””ì–´**:

DDIMì€ DDPMê³¼ ë™ì¼í•œ í•™ìŠµ ë°©ì‹ì„ ìœ ì§€í•˜ë©´ì„œë„, ìƒ˜í”Œë§ì„ **ê²°ì •ë¡ ì (Deterministic)**ìœ¼ë¡œ ë°”ê¿”ì„œ ë” ë¹ ë¥´ê²Œ ë§Œë“œëŠ” ë°©ì‹

### ğŸ” Forward Process:

- DDPMê³¼ ë™ì¼í•˜ê²Œ ì§„í–‰

### ğŸ”„ Reverse Process:

- í™•ë¥ ì  Gaussian ìƒ˜í”Œë§ì´ ì•„ë‹Œ, **ODE ê¸°ë°˜ì˜ ê²°ì •ë¡ ì  ì—­ë°©í–¥ ê³¼ì •**
- ë‹¤ìŒê³¼ ê°™ì´ noise-free trajectoryë¥¼ í†µí•´ ìƒ˜í”Œë§:
    
    $x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \left( \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \cdot \epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}} \right) + \sqrt{1 - \bar{\alpha}_{t-1}} \cdot \epsilon_\theta(x_t, t)$
    
- **Gaussian noiseë¥¼ ì œê±°**í•´ ê²°ì •ë¡ ì  ê²°ê³¼ ìƒì„±
- **ìƒ˜í”Œë§ ìŠ¤í… ìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŒ** (e.g., 1000 â†’ 50 ìŠ¤í… ë“±)

### â—ï¸â­íŠ¹ì§•:

- **ìƒ˜í”Œë§ ì†ë„ ë¹ ë¦„** (ìˆ˜ì‹­ ìŠ¤í…ìœ¼ë¡œë„ ê³ í’ˆì§ˆ ìƒ˜í”Œ ìƒì„± ê°€ëŠ¥)
- **ê²°ì •ë¡ ì  ìƒì„±** (ê°™ì€ ì¡°ê±´ì´ë©´ í•­ìƒ ê°™ì€ ìƒ˜í”Œ)
- **ë‹¤ì–‘ì„± ë¶€ì¡±** â†’ í•˜ì§€ë§Œ optional noise ì¶”ê°€ë¡œ ë‹¤ì–‘ì„± íšŒë³µ ê°€ëŠ¥

---

### ğŸ” í•µì‹¬ ë¹„êµ ìš”ì•½

| í•­ëª© | **DDPM** | **DDIM** |
| --- | --- | --- |
| ìƒ˜í”Œë§ ë°©ì‹ | í™•ë¥ ì  (Stochastic) | ê²°ì •ë¡ ì  (Deterministic) |
| ìƒ˜í”Œë§ ì†ë„ | ëŠë¦¼ (ë³´í†µ 1000 ìŠ¤í…) | ë¹ ë¦„ (10~50 ìŠ¤í… ê°€ëŠ¥) |
| ë‹¤ì–‘ì„± | ë†’ìŒ (ë§¤ë²ˆ ë‹¤ë¥¸ ê²°ê³¼) | ë‚®ìŒ (ê°™ì€ ì¡°ê±´ì´ë©´ ê°™ì€ ê²°ê³¼) |
| í•™ìŠµ ê³¼ì • | ë™ì¼ (noise prediction ë°©ì‹) | ë™ì¼ |
| í™œìš©ì„± | ê³ í’ˆì§ˆì´ì§€ë§Œ ëŠë¦¼ | íŠ¸ë ˆì´ë“œì˜¤í”„ ì¡°ì ˆ ê°€ëŠ¥ (í’ˆì§ˆ vs ì†ë„) |

# **Diffusion ëª¨ë¸ â€” ì‹¤ìŠµ í‚¤ í¬ì¸íŠ¸**

- Sampling
- Network (UNet)
- Training
- Controling (Context)
- Speeding Up (DDPM vs DDIM)

## **Diffusion ëª¨ë¸ â€” Why?**

![image.png](attachment:fdf8c017-2fca-4f7a-a937-697b95414794:image.png)

![image.png](attachment:2d29c96a-9caa-4173-9c77-c0621676ae8b:image.png)

![image.png](attachment:ae147e44-ba18-44e7-9d52-078d7d1d4e30:image.png)

![image.png](attachment:324fd25f-73a9-4422-bb64-e438d4f529d7:image.png)

## **Diffusion ëª¨ë¸ â€”** Sampling

![image.png](attachment:1257f4db-7363-45e2-95e6-59144b668d75:image.png)

![image.png](attachment:75ee2741-a7f8-4efd-98ad-668d44a2126d:image.png)

![image.png](attachment:39381f54-7fe0-44eb-a86a-53013cb5b427:image.png)

## **Diffusion ëª¨ë¸ â€”** Neural Network

![image.png](attachment:05a3b71a-0043-4c20-be4b-e7054a51a404:image.png)

![image.png](attachment:fae5b6f9-244e-4084-af0d-500345053d2f:image.png)

## **Diffusion ëª¨ë¸ â€”** Training

![image.png](attachment:2238cf84-fc8c-4621-96de-170ae92e3ab7:image.png)

![image.png](attachment:edbced9f-d42a-4564-8395-1258a9083be3:image.png)

![image.png](attachment:c7a4961e-e048-4391-b458-5d8e2b55ae8a:image.png)

![image.png](attachment:fdc0143d-c60c-433a-a778-58c4bbc77828:image.png)

## **Diffusion ëª¨ë¸ â€”** Controlling

![image.png](attachment:3c7d545f-cd19-4e5e-8d78-8449ddc87286:image.png)

![image.png](attachment:5168403e-0f88-42a1-a659-7fe957365810:image.png)

![image.png](attachment:0576b914-c760-4618-95eb-31f2680b1dbc:image.png)

## **Diffusion ëª¨ë¸ â€”** Speeding Up

![image.png](attachment:c288281a-67fc-44f6-8891-cb88ba3d3ff0:image.png)

![image.png](attachment:8914f088-1821-40db-9bbf-6bb256a382fa:image.png)

---

# ì°¸ê³ ìë£Œ

cs.cmu(F24) â€” lec24.diffusion.pdf â€” https://deeplearning.cs.cmu.edu/F24/document/slides/lec24.diffusion.pdf

####################################  

# Diffusion â€” Python ì½”ë“œ ê¸°ë°˜ ì‹¤ìŠµ (Sprite & MNIST)

# **Lab 1 - Sampling (L1_Sampling.ipynb)**

![image.png](attachment:10a2c874-b8fa-441d-be1c-1e8bd050d84f:9cfd4a18-be19-4b70-a4d0-c0cb28c74acf.png)

```python
class ContextUnet(nn.Module):
    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):  # cfeat - context features
        super(ContextUnet, self).__init__()

        # number of input channels, number of intermediate feature maps and number of classes
        self.in_channels = in_channels
        self.n_feat = n_feat
        self.n_cfeat = n_cfeat
        self.h = height  #assume h == w. must be divisible by 4, so 28,24,20,16...

        # Initialize the initial convolutional layer
        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)

        # Initialize the down-sampling path of the U-Net with two levels
        self.down1 = UnetDown(n_feat, n_feat)        # down1 #[10, 256, 8, 8]
        self.down2 = UnetDown(n_feat, 2 * n_feat)    # down2 #[10, 256, 4, 4]
        
         # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())
        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())

        # Embed the timestep and context labels with a one-layer fully connected neural network
        self.timeembed1 = EmbedFC(1, 2*n_feat)
        self.timeembed2 = EmbedFC(1, 1*n_feat)
        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat)
        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat)

        # Initialize the up-sampling path of the U-Net with three levels
        self.up0 = nn.Sequential(
            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.h//4, self.h//4), # up-sample  
            nn.GroupNorm(8, 2 * n_feat), # normalize                       
            nn.ReLU(),
        )
        self.up1 = UnetUp(4 * n_feat, n_feat)
        self.up2 = UnetUp(2 * n_feat, n_feat)

        # Initialize the final convolutional layers to map to the same number of channels as the input image
        self.out = nn.Sequential(
            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0
            nn.GroupNorm(8, n_feat), # normalize
            nn.ReLU(),
            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1), # map to same number of channels as input
        )

    def forward(self, x, t, c=None):
        """
        x : (batch, n_feat, h, w) : input image
        t : (batch, n_cfeat)      : time step
        c : (batch, n_classes)    : context label
        """
        # x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on

        # pass the input image through the initial convolutional layer
        x = self.init_conv(x)
        # pass the result through the down-sampling path
        down1 = self.down1(x)       #[10, 256, 8, 8]
        down2 = self.down2(down1)   #[10, 256, 4, 4]
        
        # convert the feature maps to a vector and apply an activation
        hiddenvec = self.to_vec(down2)
        
        # mask out context if context_mask == 1
        if c is None:
            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)
            
        # embed context and timestep
        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)     # (batch, 2*n_feat, 1,1)
        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)
        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)
        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)
        #print(f"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}")

        up1 = self.up0(hiddenvec)
        up2 = self.up1(cemb1*up1 + temb1, down2)  # add and multiply embeddings
        up3 = self.up2(cemb2*up2 + temb2, down1)
        out = self.out(torch.cat((up3, x), 1))
        return out

```

## ğŸ§  ContextUnet í´ë˜ìŠ¤ ì •ì˜

ì´ ë¶€ë¶„ì€ **ì»¨í…ìŠ¤íŠ¸ ì¡°ê±´ë¶€ U-Net** êµ¬ì¡°ë¥¼ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤. í™•ì‚° ëª¨ë¸ì—ì„œ ì‚¬ìš©ë  í•µì‹¬ ì‹ ê²½ë§ì…ë‹ˆë‹¤.

### êµ¬ì¡° ìš”ì•½:

- `ResidualConvBlock`: ì”ì°¨ êµ¬ì¡°ë¡œ íŠ¹ì§• ì¶”ì¶œ
- `UnetDown`, `UnetUp`: ë‹¤ìš´ìƒ˜í”Œë§, ì—…ìƒ˜í”Œë§ ê²½ë¡œ
- `EmbedFC`: íƒ€ì„ìŠ¤í…(timestep) ë° ì¡°ê±´(context)ì„ ì„ë² ë”©
- ì»¨í…ìŠ¤íŠ¸ì™€ íƒ€ì„ìŠ¤í… ì •ë³´ë¥¼ ì„ë² ë”©í•˜ì—¬ ì¤‘ê°„ íŠ¹ì„±ê³¼ ê²°í•©
- ë§ˆì§€ë§‰ì— ì›ë³¸ ì´ë¯¸ì§€ì™€ ê°™ì€ ì°¨ì›ì˜ ì¶œë ¥ì„ ìƒì„±

## Hyper Parameter ì„¤ì •

```python
# hyperparameters

## diffusion hyperparameters
timesteps = 500
beta1 = 1e-4
beta2 = 0.02

## network hyperparameters
device = torch.device("cuda:0" if torch.cuda.is_available() else torch.device('cpu'))
n_feat = 64 # 64 hidden dimension feature
n_cfeat = 5 # context vector is of size 5
height = 16 # 16x16 image
save_dir = './weights/'

# training hyperparameters
batch_size = 100
n_epoch = 32
lrate=1e-3
```

## í™•ì‚° ìŠ¤ì¼€ì¤„ ê³„ì‚°

```python
## construct DDPM noise schedule
b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1
a_t = 1 - b_t
ab_t = torch.cumsum(a_t.log(), dim=0).exp()    
ab_t[0] = 1
```

ë…¸ì´ì¦ˆ ì¶”ê°€/ì œê±°ì— ì‚¬ìš©ë˜ëŠ” ë³€ìˆ˜ë“¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤:

- `b_t`, `a_t`, `ab_t`: ê°ê° ì‹œê°„ì— ë”°ë¥¸ ë…¸ì´ì¦ˆ ë¹„ìœ¨, ì”ì—¬ ì‹ í˜¸ ë¹„ìœ¨, ëˆ„ì  ì‹ í˜¸ ë³´ì¡´ ë¹„ìœ¨

## ë…¸ì´ì¦ˆ ì¶”ê°€

```python
# removes the predicted noise (but adds some noise back in to avoid collapse)
def denoise_add_noise(x, t, pred_noise, z=None):
    if z is None:
        z = torch.randn_like(x)
    noise = b_t.sqrt()[t] * z
    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()
    return mean + noise
```

## ìƒ˜í”Œë§ (DDPM)

```python
# sample using standard algorithm
@torch.no_grad()
def sample_ddpm(n_sample, save_rate=20):
    # x_T ~ N(0, 1), sample initial noise
    samples = torch.randn(n_sample, 3, height, height).to(device)  

    # array to keep track of generated steps for plotting
    intermediate = [] 
    for i in range(timesteps, 0, -1):
        print(f'sampling timestep {i:3d}', end='\r')

        # reshape time tensor
        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)

        # sample some random noise to inject back in. For i = 1, don't add back in noise
        **z = torch.randn_like(samples) if i > 1 else 0**

        eps = nn_model(samples, t)    # predict noise e_(x_t,t)
        samples = denoise_add_noise(samples, i, eps, z)
        if i % save_rate ==0 or i==timesteps or i<8:
            intermediate.append(samples.detach().cpu().numpy())

    intermediate = np.stack(intermediate)
    return samples, intermediate
```

## ì˜¬ë°”ë¥´ì§€ ì•Šì€ ìƒ˜í”Œë§ (DDPM)

```python
**# incorrectly sample without adding in noise**
@torch.no_grad()
def sample_ddpm_incorrect(n_sample):
    # x_T ~ N(0, 1), sample initial noise
    samples = torch.randn(n_sample, 3, height, height).to(device)  

    # array to keep track of generated steps for plotting
    intermediate = [] 
    for i in range(timesteps, 0, -1):
        print(f'sampling timestep {i:3d}', end='\r')

        # reshape time tensor
        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)

        # don't add back in noise
        **z = 0**

        eps = nn_model(samples, t)    # predict noise e_(x_t,t)
        samples = denoise_add_noise(samples, i, eps, z)
        if i%20==0 or i==timesteps or i<8:
            intermediate.append(samples.detach().cpu().numpy())

    intermediate = np.stack(intermediate)
    return samples, intermediate
```

# **Lab 2 - Training  (L2_Training.ipynb)**

## ë°ì´í„° ë¡œë“œ

```python
# load dataset and construct optimizer
dataset = CustomDataset("./dataset/L2_sprites_1788_16x16.npy", "./dataset/L2_sprite_labels_nc_1788_16x16.npy", transform, null_context=False)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)
optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)
```

## ì´ë¯¸ì§€ì— ë…¸ì´ì¦ˆ ì¶”ê°€

```python
# perturbs an image to a specified noise level
def perturb_input(x, t, noise):
    return ab_t.sqrt()[t, None, None, None] * x + (1 - ab_t[t, None, None, None]).sqrt() * noise
```

## Training í”„ë¡œì„¸ìŠ¤

```python
# training without context code (Need GPU)

# set into train mode
nn_model.train()

for ep in range(n_epoch):
    print(f'epoch {ep}')
    
    # linearly decay learning rate
    optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)
    
    pbar = tqdm(dataloader, mininterval=2 )
    for x, _ in pbar:   # x: images
        optim.zero_grad()
        x = x.to(device)
        
        # perturb data
        noise = torch.randn_like(x)
        t = torch.randint(1, timesteps + 1, (x.shape[0],)).to(device) 
        x_pert = perturb_input(x, t, noise)
        
        # use network to recover noise
        pred_noise = nn_model(x_pert, t / timesteps)
        
        # loss is mean squared error between the predicted and true noise
        loss = F.mse_loss(pred_noise, noise)
        loss.backward()
        
        optim.step()

    # save model periodically
    if ep%4==0 or ep == int(n_epoch-1):
        if not os.path.exists(save_dir):
            os.mkdir(save_dir)
        torch.save(nn_model.state_dict(), save_dir + f"model_{ep}.pth")
        print('saved model at ' + save_dir + f"model_{ep}.pth")
```

# **Lab 3 - Context**

**ì¡°ê±´ë¶€(Conditional) ì´ë¯¸ì§€ ìƒì„±**ì´ë©°, Contextë¥¼ ë°˜ì˜í•œ ìƒì„± ê²°ê³¼ë¥¼ ì–»ëŠ” ê²ƒì´ ëª©ì ì„.

## Contextë¥¼ ì¶”ê°€í•œ Training í”„ë¡œì„¸ìŠ¤

```python
# training with context code (Need GPU)
# set into train mode
nn_model.train()

for ep in range(n_epoch):
    print(f'epoch {ep}')
    
    # linearly decay learning rate
    optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)
    
    pbar = tqdm(dataloader, mininterval=2 )
    **for x, c in pbar:   # x: images  c: context**
        optim.zero_grad()
        x = x.to(device)
        **c = c.to(x)**
        
        # randomly mask out c
        context_mask = torch.bernoulli(torch.zeros(c.shape[0]) + 0.9).to(device)
        **c = c * context_mask.unsqueeze(-1)**
        
        # perturb data
        noise = torch.randn_like(x)
        t = torch.randint(1, timesteps + 1, (x.shape[0],)).to(device) 
        x_pert = perturb_input(x, t, noise)
        
        # use network to recover noise
        **pred_noise = nn_model(x_pert, t / timesteps, c=c)**
        
        # loss is mean squared error between the predicted and true noise
        loss = F.mse_loss(pred_noise, noise)
        loss.backward()
        
        optim.step()

    # save model periodically
    if ep%4==0 or ep == int(n_epoch-1):
        if not os.path.exists(save_dir):
            os.mkdir(save_dir)
        torch.save(nn_model.state_dict(), save_dir + f"context_model_{ep}.pth")
        print('saved model at ' + save_dir + f"context_model_{ep}.pth")
```

## Contextë¥¼ ì¶”ê°€í•œ Sampling í”„ë¡œì„¸ìŠ¤

```python
# sample with context using standard algorithm
@torch.no_grad()
def sample_ddpm_context(n_sample, context, save_rate=20):
    # x_T ~ N(0, 1), sample initial noise
    samples = torch.randn(n_sample, 3, height, height).to(device)  

    # array to keep track of generated steps for plotting
    intermediate = [] 
    for i in range(timesteps, 0, -1):
        print(f'sampling timestep {i:3d}', end='\r')

        # reshape time tensor
        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)

        # sample some random noise to inject back in. For i = 1, don't add back in noise
        z = torch.randn_like(samples) if i > 1 else 0

        **eps = nn_model(samples, t, c=context)**    # predict noise e_(x_t,t, ctx)
        samples = denoise_add_noise(samples, i, eps, z)
        if i % save_rate==0 or i==timesteps or i<8:
            intermediate.append(samples.detach().cpu().numpy())

    intermediate = np.stack(intermediate)
    return samples, intermediate
```

## íŠ¹ì • Context ê¸°ë°˜ Sampling

```python
# user defined context
ctx = torch.tensor([
    # hero, non-hero, food, spell, side-facing
    [1,0,0,0,0],  
    [1,0,0,0,0],    
    [0,0,0,0,1],
    [0,0,0,0,1],    
    [0,1,0,0,0],
    [0,1,0,0,0],
    [0,0,1,0,0],
    [0,0,1,0,0],
]).float().to(device)
samples, _ = sample_ddpm_context(ctx.shape[0], ctx)
show_images(samples)
```

## íŠ¹ì • Mixed Context ê¸°ë°˜ Sampling

```python
# mix of defined context
ctx = torch.tensor([
    # hero, non-hero, food, spell, side-facing
    [1,0,0,0,0],      #human
    [1,0,0.6,0,0],    
    [0,0,0.6,0.4,0],  
    [1,0,0,0,1],  
    [1,1,0,0,0],
    [1,0,0,1,0]
]).float().to(device)
samples, _ = sample_ddpm_context(ctx.shape[0], ctx)
show_images(samples)
```

## ğŸ”ëª¨ë¸ ì•„í‚¤í…ì²˜ â€“ `ContextUnet` í•µì‹¬ ì•„ì´ë””ì–´

- **U-Net ê¸°ë°˜ êµ¬ì¡°**ì— **timestep**ê³¼ **context label**ì„ embeddingí•˜ì—¬ ì´ë¯¸ì§€ ìƒì„±ì— ë°˜ì˜.
- `context`ëŠ” í´ë˜ìŠ¤ ì •ë³´ë¥¼ ë‹´ì€ ë²¡í„° (ì˜ˆ: one-hot)ì´ë©°, ì´ë¥¼ í†µí•´ ì›í•˜ëŠ” ì†ì„±ì„ ê°€ì§„ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŒ.

```python
x â†’ init_conv â†’ down1 â†’ down2 â†’ to_vec
                  â†“           â†“
              + time/context embeddings
                  â†“           â†“
               up0 â†’ up1 â†’ up2 â†’ output

```

- `EmbedFC`: Fully Connected layerë¡œ contextì™€ timestepì„ embedding
- `up1`, `up2`: contextì™€ timestepì˜ embeddingì„ ê³±/í•©í•˜ì—¬ ì¶”ê°€ ì •ë³´ ë°˜ì˜
- context ì—†ì´ ìƒì„±í•  ìˆ˜ë„ ìˆë„ë¡, `if c is None:` ì²˜ë¦¬

# **Lab 4 - Fast Sampling (DDIM)**

## DDIM Denois í”„ë¡œì„¸ìŠ¤

```python
# define sampling function for DDIM   
# removes the noise using ddim
def denoise_ddim(x, t, t_prev, pred_noise):
    ab = ab_t[t]
    ab_prev = ab_t[t_prev]
    
    x0_pred = ab_prev.sqrt() / ab.sqrt() * (x - (1 - ab).sqrt() * pred_noise)
    dir_xt = (1 - ab_prev).sqrt() * pred_noise

    return x0_pred + dir_xt
```

## DDIM Sampling í”„ë¡œì„¸ìŠ¤

```python
# sample quickly using DDIM
@torch.no_grad()
def sample_ddim(n_sample, n=20):
    # x_T ~ N(0, 1), sample initial noise
    samples = torch.randn(n_sample, 3, height, height).to(device)  

    # array to keep track of generated steps for plotting
    intermediate = [] 
    step_size = timesteps // n
    for i in range(timesteps, 0, -step_size):
        print(f'sampling timestep {i:3d}', end='\r')

        # reshape time tensor
        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)

        eps = nn_model(samples, t)    # predict noise e_(x_t,t)
        samples = denoise_ddim(samples, i, i - step_size, eps)
        intermediate.append(samples.detach().cpu().numpy())

    intermediate = np.stack(intermediate)
    return samples, intermediate
```

## Contextë¥¼ ì¶”ê°€í•œ DDIM Sampling í”„ë¡œì„¸ìŠ¤

```python
# fast sampling algorithm with context
@torch.no_grad()
def sample_ddim_context(n_sample, context, n=20):
    # x_T ~ N(0, 1), sample initial noise
    samples = torch.randn(n_sample, 3, height, height).to(device)  

    # array to keep track of generated steps for plotting
    intermediate = [] 
    step_size = timesteps // n
    for i in range(timesteps, 0, -step_size):
        print(f'sampling timestep {i:3d}', end='\r')

        # reshape time tensor
        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)

        eps = nn_model(samples, t, c=context)    # predict noise e_(x_t,t)
        samples = denoise_ddim(samples, i, i - step_size, eps)
        intermediate.append(samples.detach().cpu().numpy())

    intermediate = np.stack(intermediate)
    return samples, intermediate
```


############################  
# OpenVLA ë° LIBERO í™˜ê²½ êµ¬ì¶•

**ìµœì´ˆ ì‘ì„±:** May 19, 2025

**ì—…ë°ì´íŠ¸:** 1ì°¨ July 30, 2025

# **Installation â€” OpenVLA (Linux & Windows)**

- **í™˜ê²½ êµ¬ì„±:** PyTorch 2.2.0, torchvision 0.17.0, transformers 4.40.1, tokenizers 0.19.1, timm 0.9.10, and flash-attn 2.5.5

### Set-up: Conda Env

```bash
# Create and activate conda environment
conda create -n openvla python=3.10 -y
conda activate openvla

# Install PyTorch.
# https://pytorch.org/get-started/locally/

# CUDA 12.1 (Conda)
conda install pytorch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 pytorch-cuda=12.1 -c pytorch -c nvidia -y 
# OR
# CUDA 12.1 (Pip) -Optional
pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu121 -q
```

### Set-up: Openvla Repo

```bash
# Clone and install the openvla repo
git clone https://github.com/openvla/openvla.git
cd openvla
pip install -e .
# openvla pip ì„¤ì¹˜ì´í›„ Pytorch CPU ë²„ì „ìœ¼ë¡œ downgrade í˜„ìƒ ì²´í¬
# openvla pip ì„¤ì¹˜ì´í›„ CUDA 12.1 (Conda) ì„¤ì¹˜ ì¶”ì²œ
conda install nvidia/label/cuda-12.1.0::cuda-nvcc
```

### Set-up(for Ubuntu): Ninja & Flash Attention 2

```bash
# Install Flash Attention 2 for training (https://github.com/Dao-AILab/flash-attention)
#   =>> If you run into difficulty, try `pip cache remove flash_attn` first
pip install packaging ninja
# ninja --version; echo $?  # Verify Ninja --> should return exit code "0"
# Linux GCC í•„ìˆ˜ (ìœˆë„ìš°ì—ì„œëŠ” MSVC í•„ìš”) 
pip install "flash-attn==2.5.5" --no-build-isolation
```

### Set-up(for Windows): Ninja & Flash Attention 2

```bash
# ìœˆë„ìš° flash-attn ì„¤ì¹˜ (ì´ì „ ì„¤ì¹˜ ì§„í–‰ ì „)

# 1. nvcc ì„¤ì¹˜
# https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html
# nvcc ì‹¤í–‰ í™•ì¸

# 2. (Option) MS Visual Studio ì„¤ì¹˜ í•„ìš” ê°€ëŠ¥

```

![image.png](attachment:bd205b7c-0c87-4e40-91b6-6844d7d14631:image.png)

# Inference Test **â€” OpenVLA (Linux & Windows)**

**Source code: vla-scripts/extern/verify_openvla.py**

### VRAM 16GB ì •ë„ â€” (BFLOAT16 + FLASH-ATTN MODE)

```python
# === BFLOAT16 + FLASH-ATTN MODE ===
print("[*] Loading in BF16 with Flash-Attention Enabled")
vla = AutoModelForVision2Seq.from_pretrained(
    MODEL_PATH,
    attn_implementation="flash_attention_2",
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    trust_remote_code=True,
).to(device)
```

```python
# === BFLOAT16 MODE ===
inputs = processor(prompt, image).to(device, dtype=torch.bfloat16)
```

### VRAM 6GB ì •ë„ â€” (4-BIT QUANTIZATION MODE)

```bash
# If needed
sudo apt install python3-pip
# Install bitsandbytes
pip install bitsandbytes
```

```python
from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig
```

```python
# === 4-BIT QUANTIZATION MODE (`pip install bitsandbytes`) :: [~6GB of VRAM Passive || 7GB of VRAM Active] ===
print("[*] Loading in 4-Bit Quantization Mode")
vla = AutoModelForVision2Seq.from_pretrained(
    MODEL_PATH,
    attn_implementation="flash_attention_2",
    torch_dtype=torch.float16,
    quantization_config=BitsAndBytesConfig(load_in_4bit=True),
    low_cpu_mem_usage=True,
    trust_remote_code=True,
)
```

```python
# === 8-BIT/4-BIT QUANTIZATION MODE ===
inputs = processor(prompt, image).to(device, dtype=torch.float16)
```

<aside>
ğŸ’¡

**ERROR #1** - Related to â€œpip install bitsandbytesâ€

> A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.6 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.
> 

**SOL**

```bash
# pip install again (to downgrade as numpy 1.26.4)
cd openvla
pip install -e .
```

</aside>

<aside>
ğŸ’¡

**ERROR #2**

> ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.
> 

**SOL**

```bash
pip install accelerate==1.1.1
```

</aside>

---

# Install â€” **LIBERO**

### Clone and install the LIBERO repo

```bash
git clone https://github.com/Lifelong-Robot-Learning/LIBERO.git
cd LIBERO
pip install -e .
cd ..
```

### Install other required packages for OpenVLA

```bash
cd openvla
pip install -r experiments/robot/libero/libero_requirements.txt
```

<aside>
ğŸ’¡

**ERROR #1** - Related to â€œpip install bitsandbytesâ€

> A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.6 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.
> 

**SOL**

```bash
# pip install again (to downgrade as numpy 1.26.4)
cd openvla
pip install -e .
```

</aside>

# **Launching LIBERO Evaluations (Only Linux)**

### Set-up â€” 4-bit ì–‘ìí™” ëª¨ë“œ

`openvla/experiments/robot/libero`Â í´ë” >Â `run_libero_eval.py`Â 

GenerateConfig í´ë˜ìŠ¤ > `load_in_4bit: bool = False` â†’ `load_in_4bit: bool = True`

```bash
# Launch LIBERO-Spatial evals
python experiments/robot/libero/run_libero_eval.py \
  --model_family openvla \
  --pretrained_checkpoint openvla/openvla-7b-finetuned-libero-spatial \
  --task_suite_name libero_spatial \
  --center_crop True
  
python experiments/robot/libero/run_libero_eval.py --model_family openvla --pretrained_checkpoint openvla/openvla-7b-finetuned-libero-spatial --task_suite_name libero_spatial --center_crop True

# Launch LIBERO-Object evals
python experiments/robot/libero/run_libero_eval.py \
  --model_family openvla \
  --pretrained_checkpoint openvla/openvla-7b-finetuned-libero-object \
  --task_suite_name libero_object \
  --center_crop True

# Launch LIBERO-Goal evals
python experiments/robot/libero/run_libero_eval.py \
  --model_family openvla \
  --pretrained_checkpoint openvla/openvla-7b-finetuned-libero-goal \
  --task_suite_name libero_goal \
  --center_crop True

# Launch LIBERO-10 (LIBERO-Long) evals
python experiments/robot/libero/run_libero_eval.py \
  --model_family openvla \
  --pretrained_checkpoint openvla/openvla-7b-finetuned-libero-10 \
  --task_suite_name libero_10 \
  --center_crop True
```

<aside>
ğŸ’¡

Custom dataset folder

> Do you want to specify a custom path for the dataset folder? (Y/N): N
> 
</aside>

<aside>
ğŸ’¡

Error #1

```bash
pip install robosuite==1.4
```

</aside>

<aside>
ğŸ’¡

`robosuite` Installation for Windows ([ì°¸ê³ ](https://robosuite.ai/docs/installation.html#installing-on-windows))

https://robosuite.ai/docs/installation.html#installing-on-windows

</aside>

---

# ì°¸ê³ ìë£Œ

[GitHub - openvla/openvla: OpenVLA: An open-source vision-language-action model for robotic manipulation.](https://github.com/openvla/openvla?tab=readme-ov-file#getting-started)

[CUDA Installation Guide for Microsoft Windows â€” Installation Guide Windows 12.9 documentation](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html)


################################  

# OpenVLA ì†Œê°œ

ìµœì´ˆì‘ì„± : May 19, 2025 

[OpenVLA: An Open-Source Vision-Language-Action Model](https://openvla.github.io/)

[openvla_teaser_video.mp4](attachment:b5fbb0a8-8ea4-492e-9f2a-f8038a3f5076:openvla_teaser_video.mp4)

![image.png](attachment:e475d3cf-9573-4e59-9853-a85dea39d267:image.png)

![image.png](attachment:3a116deb-5858-4b79-b05d-d4400e95834d:image.png)

## **OpenVLA ì†Œê°œ ë° íŠ¹ì§•**

- **ëª¨ë¸ ê°œìš”**: 7B íŒŒë¼ë¯¸í„° ì˜¤í”ˆì†ŒìŠ¤ ë¹„ì „-ì–¸ì–´-ì•¡ì…˜ ëª¨ë¸(VLA)
- **ë°ì´í„°**: Open X-Embodiment ë°ì´í„°ì…‹ì˜ 970k ë¡œë´‡ ì—í”¼ì†Œë“œë¡œ ì‚¬ì „ í›ˆë ¨
- **ì„±ëŠ¥**: ì¼ë°˜ ë¡œë´‡ ì¡°ì‘ ì •ì±…ì—ì„œ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±
- **ë‹¤ê¸°ëŠ¥ì„±**: ì—¬ëŸ¬ ë¡œë´‡ ì œì–´ ê¸°ë³¸ ì§€ì›
- **ì ì‘ì„±**: íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •ìœ¼ë¡œ ìƒˆë¡œìš´ ë¡œë´‡ ì„¤ì •ì— ë¹ ë¥´ê²Œ ì ì‘
- **ì˜¤í”ˆì†ŒìŠ¤**: OpenVLA ì²´í¬í¬ì¸íŠ¸ ë° PyTorch í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì™„ì „ ê³µê°œ
- **ì ‘ê·¼ì„±**: HuggingFaceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¯¸ì„¸ ì¡°ì • ê°€ëŠ¥

###########################################  

# LIBERO ì†Œê°œ

ìµœì´ˆ ì‘ì„±: May 19, 2025 

[LIBERO â€“ LIBERO](https://libero-project.github.io/main.html)

![image.png](attachment:b0635f31-4c2a-4296-8ffa-a0cad642ae29:image.png)

## LIBERO ìš”ì•½

- **(ê¸°ë³¸)** ì‚¬ì „ í›ˆë ¨ëœ í›„ ë°°ì¹˜ **(ì¶”ê°€)** ì¸ê°„ ì‚¬ìš©ìì™€ í•¨ê»˜ **í‰ìƒ í•™ìŠµ**í•˜ì—¬ ê°œì¸í™”ëœ êµ¬í˜„ ì—ì´ì „íŠ¸ í•„ìˆ˜
- **ì—°êµ¬ ê³¼ì œ**: ì˜ì‚¬ê²°ì •ì—ì„œì˜ í‰ìƒ í•™ìŠµ(LLDM, lifelong learning in decision making) ì¤‘ìš”
- **ë¬¸ì œì **: LLDM ì—°êµ¬ë¥¼ ìœ„í•œ ì ì ˆí•œ í…ŒìŠ¤íŠ¸ë² ë“œ ë¶€ì¡±
- **ì œì•ˆ**: LIBERO, í‰ìƒ ë¡œë´‡ í•™ìŠµì— íŠ¹í™”ëœ ë²¤ì¹˜ë§ˆí¬ ì œê³µ
    - **íŠ¹ì§•**: ì§€ì†ì ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥í•œ ë²¤ì¹˜ë§ˆí¬
    - **ëª©í‘œ**: ì§€ì‹ ì „ì´ë¥¼ ì—°êµ¬í•˜ëŠ” ê³µí†µ í”Œë«í¼ ì œê³µ
- **ê¸°ëŒ€ íš¨ê³¼**: ê¸°ê³„í•™ìŠµ ë° ë¡œë³´í‹±ìŠ¤ ì»¤ë®¤ë‹ˆí‹°ê°€ ìƒˆë¡œìš´ í‰ìƒ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ ë° í‰ê°€ ê°€ëŠ¥

## LIBEROì´ LLDMì— íŠ¹í™”ëœ ì 

- **ì§€ì†ì  í•™ìŠµ í™˜ê²½ ì œê³µ**: LIBEROëŠ” ë¡œë´‡ì´ **ì¸ê°„ê³¼ì˜ ìƒí˜¸ì‘ìš©**ì„ í†µí•´ ìƒˆë¡œìš´ ê¸°ìˆ ê³¼ ì§€ì‹ì„ **ì§€ì†ì ìœ¼ë¡œ í•™ìŠµ**í•  ìˆ˜ ìˆëŠ” **ë™ì  í…ŒìŠ¤íŠ¸ë² ë“œ**ë¥¼ ì œê³µ. LLDMì˜ í•µì‹¬ì¸ ì‹œê°„ì— ê±¸ì¹œ ì ì‘ê³¼ ê°œì„ ì„ ì§€ì›.
- **ì§€ì‹ ì „ì´ ì´ˆì **: LIBEROëŠ” ë¡œë´‡ì´ **ê¸°ì¡´ ì§€ì‹**ì„ **ìƒˆë¡œìš´ ì‘ì—…**ì´ë‚˜ í™˜ê²½ì— **ì „ì´ì‹œí‚¤ëŠ” ëŠ¥ë ¥**ì„ **í‰ê°€**í•˜ë„ë¡ ì„¤ê³„ë¨. LLDMì—ì„œ í•„ìˆ˜ì ì¸ ìš”ì†Œë¡œ, ë¡œë´‡ì´ ë‹¤ì–‘í•œ ìƒí™©ì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë„ì›€.
- **í™•ì¥ ê°€ëŠ¥í•œ ë²¤ì¹˜ë§ˆí¬**: LIBEROëŠ” ìƒˆë¡œìš´ ì‘ì—…, ì‹œë‚˜ë¦¬ì˜¤, ë°ì´í„°ì…‹ì„ ì¶”ê°€í•˜ë©° ì§€ì†ì ìœ¼ë¡œ **í™•ì¥ ê°€ëŠ¥**í•¨.
- **í˜„ì‹¤ì  ë¬¸ì œ ë°˜ì˜**: LIBEROëŠ” ë¡œë´‡ì´ ì‹¤ì œ **ì¸ê°„ê³¼ì˜ í˜‘ì—…**ì—ì„œ ì§ë©´í•  ìˆ˜ ìˆëŠ” **ë³µì¡í•œ ì˜ì‚¬ê²°ì • ë¬¸ì œ**ë¥¼ í¬í•¨í•¨. ì‹¤ì„¸ê³„ì—ì„œì˜ í‰ìƒ í•™ìŠµ ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•˜ë„ë¡ ì„¤ê³„ë¨.
- **ì»¤ë®¤ë‹ˆí‹° í˜‘ì—… ì´‰ì§„**: ì•Œê³ ë¦¬ì¦˜ ë¹„êµÂ·í‰ê°€ë¥¼ ìœ„í•œ **í‘œì¤€í™”ëœ í”Œë«í¼** ì œê³µ.

##################################  

FastAPI êµ¬ì¶• ì‹¤ìŠµ
ìµœì´ˆ ì‘ì„±: @May 19, 2025â€‹
ì—…ë°ì´íŠ¸: 1ì°¨ @July 30, 2025â€‹
Serving OpenVLA models over a REST API (FastAPI) 
Install â€” FastAPI
pip install fastapi uvicorn
â€‹
Test @ 4-bit ì–‘ìí™” ëª¨ë“œ & FastAPI Server
Source code: vla-scripts/deploy.py [ì°¸ê³ ]
ìˆ˜ì •: Import êµ¬ë¬¸
from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig
â€‹
ìˆ˜ì •: class OpenVLAServer::__Init__
# self.vla = AutoModelForVision2Seq.from_pretrained(
#     self.openvla_path,
#     attn_implementation=attn_implementation,
#     torch_dtype=torch.bfloat16,
#     low_cpu_mem_usage=True,
#     trust_remote_code=True,
# ).to(self.device)

print("[*] Loading in 4-Bit Quantization Mode")
self.vla = AutoModelForVision2Seq.from_pretrained(
    self.openvla_path,
    attn_implementation=attn_implementation,
    torch_dtype=torch.float16,
    quantization_config=BitsAndBytesConfig(load_in_4bit=True),
    low_cpu_mem_usage=True,
    trust_remote_code=True,
)
â€‹
ìˆ˜ì •: class OpenVLAServer::predict_action
# inputs = self.processor(prompt, Image.fromarray(image).convert("RGB")).to(self.device, dtype=torch.bfloat16)

# === 8-BIT/4-BIT QUANTIZATION MODE ===
inputs = self.processor(prompt, Image.fromarray(image).convert("RGB")).to(self.device, dtype=torch.float16)
â€‹
ì‹¤í–‰
# at VS Code Terminal
conda activate openvla
cd ~/ws_openvla

python openvla/vla-scripts/deploy.py
â€‹
Install â€” gradio
pip install gradio
â€‹
Test @ Gradio ì‹œê°í™”
Source code: gradio_app.py
ì½”ë“œ â€” gradio_app.py
import gradio as gr
import requests
import json_numpy
import numpy as np
from PIL import Image

# Gradio í´ë¼ì´ì–¸íŠ¸ì™€ ì„œë²„ ê°„ ë°ì´í„° í¬ë§· ì²˜ë¦¬
json_numpy.patch()

# REST API ì„œë²„ ì—”ë“œí¬ì¸íŠ¸
API_URL = "http://localhost:8000/act"

def predict_action(image, instruction, unnorm_key=None):
    # ì—…ë¡œë“œëœ ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
    image_array = np.array(image)

    # ìš”ì²­ ë°ì´í„°(payload) ìƒì„±
    payload = {
        "image": image_array,
        "instruction": instruction,
    }

    if unnorm_key:
        payload["unnorm_key"] = unnorm_key

    # ì„œë²„ì— POST ìš”ì²­
    response = requests.post(API_URL, json=payload)
    
    # ì„œë²„ ì‘ë‹µ í™•ì¸
    if response.status_code == 200:
        return response.json()
    else:
        return f"Error {response.status_code}: {response.text}"

# Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì„±
with gr.Blocks() as gradio_app:
    gr.Markdown("# OpenVLA Robot Action Prediction")
    gr.Markdown(
        "Provide an image of the robot workspace and an instruction to predict the robot's action. "
        "You can either upload an image or provide a base64-encoded image via API."
    )

    with gr.Row():
        with gr.Column(scale=1):
            instruction_input = gr.Textbox(label="Instruction", placeholder="e.g., Pick up the remote")
            unnorm_key_input = gr.Textbox(label="Unnorm Key (Optional)", placeholder="e.g., bridge_orig")
            image_input = gr.Image(type="pil", label="Upload Image")
            submit_btn = gr.Button("Submit")

        with gr.Column(scale=1):
            output_action = gr.Textbox(label="Robot Action (X, Y, Z, Roll, Pitch, Yaw, Gripper)", interactive=False, lines=8)
    

    # ì˜ˆì¸¡ í•¨ìˆ˜ ì—°ê²°
    submit_btn.click(
        fn=predict_action,
        inputs=[image_input, instruction_input, unnorm_key_input],
        outputs=[output_action]
    )

    # ì˜ˆì œ ì œê³µ
    gr.Examples(
        examples=[
            ["Place the red vegetable in the silver pot.", "bridge_orig", "./KIMe_OpenVLA/images/example1.jpeg"],
            ["Pick up the remote", "bridge_orig", "./OpenVLA_Tutorial/images/example2.jpeg"]
        ],
        inputs=[instruction_input, unnorm_key_input, image_input]
    )

gradio_app.launch()
â€‹
ì‹¤í–‰
# at VS Code Terminal
conda activate openvla
cd ~/ws_openvla

python OpenVLA_Tutorial/gradio_app.py
â€‹
Test Images

Place the red vegetable in the silver pot.

Pick up the remote.
############################  

# LIBERO+OpenVLA Jupyter ì‹¤ìŠµ

# VSCode ì„¤ì • (lanuch.json & settings.json)

**lanuch.json**

```json
{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python Debugger: Current File",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "env": {"PYTHONPATH": "${workspaceFolder}${pathSeparator}${workspaceFolder}/openvla${pathSeparator}${env:PYTHONPATH}"}
        }
    ]
}
```

**settings.json**

```json
{
    "python.analysis.extraPaths": [
        "./openvla"
    ],
    "python.autoComplete.extraPaths": [
        "./openvla"
    ]
}
```

# Jupyter ì‹¤ìŠµ

**OpenVLA_Tutorial í´ë” ë‚´**

- 01_LIBERO.ipynb
- 02_OpenVLA_on_LIBERO.ipynb
    
    ```bash
    # ipywidgets ëª¨ë“ˆ ì˜¤ë¥˜ì‹œ
    conda install -c conda-forge ipywidgets
    ```

##################################  

# GR00T N1.5 Tutorial

**ì‘ì„± ì¼ì:** ìµœì´ˆ ì‘ì„± July 30, 2025

---

# **Installation Guide**

1. Git Repo ë³µì‚¬

```bash
git clone https://github.com/NVIDIA/Isaac-GR00T
cd Isaac-GR00T
```

1. Conda í™˜ê²½ êµ¬ì„± (Python 3.10)
    
    > âš ï¸CUDA 12.4 ë²„ì „ í•„ìˆ˜ (flash-attn ë•Œë¬¸ì—)
    > 

```bash
conda create -n gr00t python=3.10
conda activate gr00t
pip install --upgrade setuptools
pip install -e .[base]
conda install nvidia/label/cuda-12.4.0::cuda-nvcc
pip install --no-build-isolation flash-attn==2.7.1.post4
```

# **Getting started with this repo**

- **Jupyter ë…¸íŠ¸ë¶**ê³¼ ìƒì„¸ **ë¬¸ì„œ**ëŠ”Â [`./getting_started`](https://github.com/NVIDIA/Isaac-GR00T/blob/main/getting_started)Â í´ë”ì—ì„œ í™•ì¸ ê°€ëŠ¥
- **ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸**ëŠ”Â [`./scripts`](https://github.com/NVIDIA/Isaac-GR00T/blob/main/scripts)Â í´ë” ìˆìŒ.
- SO-101 ë¡œë´‡ì—ì„œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²• ([HuggingFace](https://huggingface.co/blog/nvidia/gr00t-n1-5-so101-tuning) ì°¸ê³ )
    
    [Post-Training Isaac GR00T N1.5 for LeRobot SO-101 Arm](https://huggingface.co/blog/nvidia/gr00t-n1-5-so101-tuning)
    

## **1. Data Format & Loading**

- ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì²˜ë¦¬í•˜ê¸° ìœ„í•´ [Hugging Faceì˜ LeRobot ë°ì´í„°](https://github.com/huggingface/lerobot)ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ë” ìƒì„¸í•œ ì–‘ì‹(modality) ë° ì£¼ì„ ìŠ¤í‚¤ë§ˆ("LeRobot í˜¸í™˜ ë°ì´í„° ìŠ¤í‚¤ë§ˆ", "LeRobot compatible data schema")ë¥¼ í™œìš©í•¨.
- LeRobot ë°ì´í„°ì…‹ì˜ ì˜ˆì‹œëŠ” `./demo_data/robot_sim.PickNPlace`ì— ì €ì¥ë˜ì–´ ìˆìŒ. ì¶”ê°€ì ì¸ [`modality.json`](https://www.google.com/search?q=%5Bhttps://github.com/NVIDIA/Isaac-GR00T/blob/main/demo_data/robot_sim.PickNPlace/meta/modality.json%5D(https://github.com/NVIDIA/Isaac-GR00T/blob/main/demo_data/robot_sim.PickNPlace/meta/modality.json)) íŒŒì¼ë„ í¬í•¨ë¨.
- ë°ì´í„°ì…‹ í˜•ì‹ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ [`getting_started/LeRobot_compatible_data_schema.md`](https://github.com/NVIDIA/Isaac-GR00T/blob/main/getting_started/LeRobot_compatible_data_schema.md) ì°¸ê³ 
- [`EmbodimentTag`](https://github.com/NVIDIA/Isaac-GR00T/blob/main/getting_started/4_deeper_understanding.md#embodiment-action-head-fine-tuning)ì„ í†µí•´ multiple embodiments  ì§€ì›
- `LeRobotSingleDataset` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë¡œë“œ ê°€ëŠ¥
    
    ```python
    from gr00t.data.dataset import LeRobotSingleDataset
    from gr00t.data.embodiment_tags import EmbodimentTag
    from gr00t.data.dataset import ModalityConfig
    from gr00t.experiment.data_config import DATA_CONFIG_MAP
    
    # get the data config
    data_config = DATA_CONFIG_MAP["fourier_gr1_arms_only"]
    
    # get the modality configs and transforms
    modality_config = data_config.modality_config()
    transforms = data_config.transform()
    
    # This is a LeRobotSingleDataset object that loads the data from the given dataset path.
    dataset = LeRobotSingleDataset(
        dataset_path="demo_data/robot_sim.PickNPlace",
        modality_configs=modality_config,
        transforms=None,  # we can choose to not apply any transforms
        embodiment_tag=EmbodimentTag.GR1, # the embodiment to use
    )
    
    # This is an example of how to access the data.
    dataset[5]
    ```
    
- GR00T N1.5 ëª¨ë¸ê³¼ ì—°ë™í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì²˜ë¦¬í•˜ëŠ” ë°©ë²• **íŠœí† ë¦¬ì–¼**
- [`getting_started/0_load_dataset.ipynb`](https://github.com/NVIDIA/Isaac-GR00T/blob/main/getting_started/0_load_dataset.ipynb)
- ìœ„ì™€ ë™ì¼í•œ ë‚´ìš©ì˜ **ì‹¤í–‰ ê°€ëŠ¥í•œ ìŠ¤í¬ë¦½íŠ¸**
    - [`scripts/load_dataset.py`](https://github.com/NVIDIA/Isaac-GR00T/blob/main/scripts/load_dataset.py)
- Dataset ë¡œë“œ ì‹¤í–‰
    
    ```bash
    python scripts/load_dataset.py --dataset-path ./demo_data/robot_sim.PickNPlace
    ```
    

## **2. Inference**

- GR00T N1.5 ëª¨ë¸: [Huggingface](https://huggingface.co/nvidia/GR00T-N1.5-3B)
- êµì°¨ êµ¬í˜„(cross embodiment) dataset ì˜ˆì‹œ: [demo_data/robot_sim.PickNPlace](https://github.com/NVIDIA/Isaac-GR00T/blob/main/demo_data/robot_sim.PickNPlace)

### **2.1 Inference with PyTorch**

```python
from gr00t.model.policy import Gr00tPolicy
from gr00t.data.embodiment_tags import EmbodimentTag

# 1. Load the modality config and transforms, or use above
modality_config = ComposedModalityConfig(...)
transforms = ComposedModalityTransform(...)

# 2. Load the dataset
dataset = LeRobotSingleDataset(.....<Same as above>....)

# 3. Load pre-trained model
policy = Gr00tPolicy(
    model_path="nvidia/GR00T-N1.5-3B",
    modality_config=modality_config,
    modality_transform=transforms,
    embodiment_tag=EmbodimentTag.GR1,
    device="cuda"
)

# 4. Run inference
action_chunk = policy.get_action(dataset[0])
```

- ì¶”ë¡  íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶• Jupyter ë…¸íŠ¸ë¶ íŠœí† ë¦¬ì–¼:
    - [`getting_started/1_gr00t_inference.ipynb`](https://github.com/NVIDIA/Isaac-GR00T/blob/main/getting_started/1_gr00t_inference.ipynb)
- **ì¶”ë¡  ì„œë¹„ìŠ¤** ì‹¤í–‰ ê°€ëŠ¥
    - **ì„œë²„ ëª¨ë“œ**
        
        ```
        python scripts/inference_service.py --model-path nvidia/GR00T-N1.5-3B --server
        ```
        
    - **í´ë¼ì´ì–¸íŠ¸ ëª¨ë“œ** (to send requests to the server)
        
        ```
        python scripts/inference_service.py  --client
        ```
        

### **2.2 Inference with Python TensorRT (Optional)**

- ONNX ë° TensorRTë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ í•˜ë ¤ë©´, [`deployment_scripts/README.md`](https://github.com/NVIDIA/Isaac-GR00T/blob/main/deployment_scripts/README.md) ì°¸ì¡°

## **3. Fine-Tuning**

- **ë¯¸ì„¸ ì¡°ì •(finetuning) Jupyter ë…¸íŠ¸ë¶ :** [`getting_started/2_finetuning.ipynb`](https://github.com/NVIDIA/Isaac-GR00T/blob/main/getting_started/2_finetuning.ipynb)
- ë¯¸ì„¸ ì¡°ì •(finetuning) ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰:
    
    ```bash
    # first run --help to see the available arguments
    python scripts/gr00t_finetune.py --help
    
    # then run the script
    python scripts/gr00t_finetune.py --dataset-path ./demo_data/robot_sim.PickNPlace --num-gpus 1
    ```
    
    **Note**: **RTX 4090** ê·¸ë˜í”½ ì¹´ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •(finetuning)í•˜ëŠ” ê²½ìš°, `gr00t_finetune.py`ë¥¼ ì‹¤í–‰í•  ë•Œ **`--no-tune_diffusion_model`** í”Œë˜ê·¸ë¥¼ ë°˜ë“œì‹œ ì¶”ê°€í•´ì•¼ í•¨. ì´ í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ **CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±(CUDA out of memory)** ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ.
    
- Hugging Face Sim ë°ì´í„° ë¦´ë¦¬ì¦ˆì—ì„œ **ìƒ˜í”Œ ë°ì´í„°ì…‹**ì„ [ë‹¤ìš´ë¡œë“œ](https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim)
    
    ```bash
    huggingface-cli download  nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim \
      --repo-type dataset \
      --include "gr1_arms_only.CanSort/**" \
      --local-dir $HOME/gr00t_dataset
    ```
    

- ëª¨ë¸ ë¯¸ì„¸ ì¡°ì • ì‹œ, **ë°°ì¹˜ í¬ê¸°ë¥¼ ìµœëŒ€ë¡œ ëŠ˜ë¦¬ê³  20k ìŠ¤í… ë™ì•ˆ í•™ìŠµ**ì„ ê¶Œì¥
- í•˜ë“œì›¨ì–´ ì„±ëŠ¥ ê³ ë ¤ ì‚¬í•­
    - **ë¯¸ì„¸ ì¡°ì • ì„±ëŠ¥**: ìµœì ì˜ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•´ **H100 ë…¸ë“œ 1ê°œ ë˜ëŠ” L40 ë…¸ë“œ 1ê°œ**ë¥¼ ì‚¬ìš©í•¨. A6000, RTX 4090ê³¼ ê°™ì€ ë‹¤ë¥¸ í•˜ë“œì›¨ì–´ êµ¬ì„±ë„ ì‘ë™í•˜ì§€ë§Œ, ìˆ˜ë ´í•˜ëŠ” ë° ë” ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ.
    - **LoRA ë¯¸ì„¸ ì¡°ì •**: LoRA ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•´ **A6000 GPU 2ê°œ ë˜ëŠ” RTX 4090 GPU 2ê°œ**ë¥¼ ì‚¬ìš©í•¨.
    - **ì¶”ë¡  ì„±ëŠ¥**: ì‹¤ì‹œê°„ ì¶”ë¡ ì˜ ê²½ìš°, ëŒ€ë¶€ë¶„ì˜ ìµœì‹  GPUëŠ” ë‹¨ì¼ ìƒ˜í”Œ ì²˜ë¦¬ì‹œ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì„. ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼, L40ê³¼ RTX 4090 ê°„ì˜ ì¶”ë¡  ì†ë„ ì°¨ì´ëŠ” ë¯¸ë¯¸í•¨.
- ìƒˆë¡œìš´ í˜•íƒœ(new embodiment)ì˜ ë¯¸ì„¸ ì¡°ì •ì— ëŒ€í•œ ë‚´ìš©ì€ [`getting_started/3_0_new_embodiment_finetuning.md`](https://github.com/NVIDIA/Isaac-GR00T/blob/main/getting_started/3_0_new_embodiment_finetuning.md) ì°¸ê³ 

### **Choosing the Right Embodiment Head**

![robots-banner](https://github.com/NVIDIA/Isaac-GR00T/raw/main/media/robots-banner.png)

**GR00T N1.5ëŠ” ë‹¤ì–‘í•œ ë¡œë´‡ êµ¬ì„±ì— ìµœì í™”ëœ ì„¸ ê°€ì§€ ì‚¬ì „ í•™ìŠµëœ embodiment headë¥¼ ì œê³µí•¨.**

- **`EmbodimentTag.GR1`**: ì ˆëŒ€ ì¡°ì¸íŠ¸ ê³µê°„ ì œì–´(absolute joint space control)ë¥¼ ì‚¬ìš©í•˜ëŠ” **ì†**ì„ ê°€ì§„ íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡
- **`EmbodimentTag.OXE_DROID`**: EEF(delta end-effector) ì œì–´ë¥¼ ì‚¬ìš©í•˜ëŠ” ë‹¨ì¼ ì•” ë¡œë´‡
- **`EmbodimentTag.AGIBOT_GENIE1`**: ì ˆëŒ€ ì¡°ì¸íŠ¸ ê³µê°„ ì œì–´ë¥¼ ì‚¬ìš©í•˜ëŠ” **ê·¸ë¦¬í¼**ë¥¼ ê°€ì§„ íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡
- **`EmbodimentTag.NEW_EMBODIMENT`**: (ì‚¬ì „ í•™ìŠµë˜ì§€ ì•ŠìŒ) ìƒˆë¡œìš´ ë¡œë´‡ embodimentì— ëŒ€í•œ ë¯¸ì„¸ ì¡°ì •
- ê´€ì¸¡ ë° ë™ì‘ ê³µê°„ì— ëŒ€í•œ ìì„¸í•œ ì •ë³´ëŠ”Â [`EmbodimentTag`](https://github.com/NVIDIA/Isaac-GR00T/blob/main/getting_started/4_deeper_understanding.md#embodiment-action-head-fine-tuning) ì°¸ê³ 

### **Sim Env:Â [robocasa-gr1-tabletop-tasks](https://github.com/robocasa/robocasa-gr1-tabletop-tasks)**

- Sample dataset for finetuning: [ë‹¤ìš´ë¡œë“œ](https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim)
- For Simulation Evaluation, [robocasa-gr1-tabletop-tasks](https://github.com/robocasa/robocasa-gr1-tabletop-tasks) ì°¸ê³ 

## **4. Evaluation**

- ëª¨ë¸ì˜ ì˜¤í”„ë¼ì¸ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´, ë°ì´í„°ì…‹ì— ëŒ€í•´ ëª¨ë¸ì„ í‰ê°€í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ìŠ¤í¬ë¦½íŠ¸
    
    ```bash
    python scripts/eval_policy.py --plot --model_path nvidia/GR00T-N1.5-3B
    ```
    
- ë˜ëŠ”, ìƒˆë¡œ í•™ìŠµëœ ëª¨ë¸ì„ **í´ë¼ì´ì–¸íŠ¸-ì„œë²„ ëª¨ë“œ**ë¡œ ì‹¤í–‰ ê°€ëŠ¥
    
    **ìƒˆë¡œ í›ˆë ¨ëœ ëª¨ë¸ ì‹¤í–‰:**
    
    ```
    python scripts/inference_service.py --server \
        --model-path <MODEL_PATH> \
        --embodiment-tag new_embodiment
        --data-config <DATA_CONFIG>
    ```
    
    **Offline Evaluation ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰:**
    
    ```
    python scripts/eval_policy.py --plot \
        --dataset-path <DATASET_PATH> \
        --embodiment-tag new_embodiment \
        --data-config <DATA_CONFIG>
    ```
    
    - Offline Evaluation ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ë©´ **ì‹¤ì œ ê°’ (Ground Truth)ê³¼ ì˜ˆì¸¡ëœ í–‰ë™ (Predicted actions) ê°„ì˜ ê·¸ë˜í”„**ì„ ë³¼ ìˆ˜ ìˆìŒ. **í–‰ë™ì˜ ì •ê·œí™”ë˜ì§€ ì•Šì€ MSE (í‰ê·  ì œê³± ì˜¤ì°¨)** ê°’ë„ ì œê³µë¨.
    - ì´ ê²°ê³¼ëŠ” **ì •ì±…(policy)ì´ í•´ë‹¹ ë°ì´í„°ì…‹ì—ì„œ ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í•˜ëŠ”ì§€**ì— ëŒ€í•œ ì¢‹ì€ ì§€í‘œê°€ ë¨.

# **Jetson Deployment**

A detailed guide for deploying GR00T N1.5 on Jetson is available inÂ [`deployment_scripts/README.md`](https://github.com/NVIDIA/Isaac-GR00T/blob/main/deployment_scripts/README.md).

Here's comparison of E2E performance between PyTorch and TensorRT on Orin

- Jetsonì— GR00T N1.5ë¥¼ ë°°í¬í•˜ëŠ” ìì„¸í•œ ê°€ì´ë“œëŠ” [`deployment_scripts/README.md`](https://github.com/NVIDIA/Isaac-GR00T/blob/main/deployment_scripts/README.md) ì°¸ê³ 
- Orinì—ì„œ **PyTorchì™€ TensorRT ê°„ì˜ E2E(End-to-End) ì„±ëŠ¥ ë¹„êµ**
    
    ![orin-perf](https://github.com/NVIDIA/Isaac-GR00T/raw/main/media/orin-perf.png)
    
- ëª¨ë¸ ì§€ì—° ì‹œê°„ì€ `trtexec`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ í¬ê¸° 1ë¡œ ì¸¡ì •ë˜ì—ˆìŒ.
    
    
    | **Model Name** | **Orin benchmark perf (ms)** | **Precision** |
    | --- | --- | --- |
    | Action_Head - process_backbone_output | 5.17 | FP16 |
    | Action_Head - state_encoder | 0.05 | FP16 |
    | Action_Head - action_encoder | 0.20 | FP16 |
    | Action_Head - DiT | 7.77 | FP16 |
    | Action_Head - action_decoder | 0.04 | FP16 |
    | VLM - ViT | 11.96 | FP16 |
    | VLM - LLM | 17.25 | FP16 |
    
    **ì°¸ê³ :** íŒŒì´í”„ë¼ì¸ ë‚´ **ëª¨ë“ˆ ì§€ì—° ì‹œê°„**(ì˜ˆ: DiT ë¸”ë¡)ì€ ìœ„ì˜ ë²¤ì¹˜ë§ˆí¬ í‘œì— ìˆëŠ” **ëª¨ë¸ ì§€ì—° ì‹œê°„ë³´ë‹¤ ì•½ê°„ ë” ê¸¸ìŒ.** ì´ëŠ” í•´ë‹¹ ëª¨ë“ˆ(ì˜ˆ: Action_Head - DiT)ì˜ ì§€ì—° ì‹œê°„ì´ í‘œì˜ ëª¨ë¸ ì§€ì—° ì‹œê°„ë¿ë§Œ ì•„ë‹ˆë¼ **PyTorchì—ì„œ TensorRTë¡œ ë°ì´í„°ë¥¼ ì „ì†¡í•˜ê³  ë‹¤ì‹œ TensorRTì—ì„œ PyTorchë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” ì˜¤ë²„í—¤ë“œ**ê¹Œì§€ í¬í•¨í•˜ê¸° ë•Œë¬¸ì„.
    

---

# ì°¸ê³ ìë£Œ

[NVIDIA Isaac GR00T](https://developer.nvidia.com/isaac/gr00t)

https://github.com/NVIDIA/Isaac-GR00T/

https://github.com/NVIDIA/Isaac-GR00T/

[Post-Training Isaac GR00T N1.5 for LeRobot SO-101 Arm](https://huggingface.co/blog/nvidia/gr00t-n1-5-so101-tuning)

  


