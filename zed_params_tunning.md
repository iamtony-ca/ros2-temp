제공해주신 파라미터들은 카메라의 **"눈(Sensor)"**이 빛을 어떻게 받아들이고, 어떻게 **"화장(ISP)"**해서 내보낼지를 결정하는 핵심 설정들입니다.

로봇의 **"점프하는 고스트 현상(매칭 실패)"**과 직결되는 아주 중요한 파라미터들이 포함되어 있습니다. 직관적으로 이해하시기 쉽게 **[개념]**, **[직관적 의미]**, **[조절 시 변화]** 순서로 설명해 드릴게요.

---

### 1. ZED X 전용 설정 (`zedx.yaml`)
이곳의 설정은 **하드웨어(센서) 레벨**의 정밀한 제어입니다. 가장 중요합니다.

#### **1) `exposure_time` (노출 시간)**
* **직관적 의미:** 카메라의 눈꺼풀을 얼마나 오래 열고 있을지 결정합니다. (단위: 마이크로초, $\mu s$)
* **설명:** 16000이면 16ms(0.016초) 동안 빛을 받습니다.
* **변화:**
    * **⬆️ 높이면:** 이미지가 **밝아집니다**. 하지만 로봇이 움직일 때 **잔상(Motion Blur)**이 심해져서 **Depth 매칭이 실패**하고 벽이 튀어나옵니다.
    * **⬇️ 낮추면:** 이미지가 **어두워집니다**. 대신 움직임이 아주 **선명(Crisp)**해져서 고스트 현상이 사라집니다. (어두워진 건 Gain으로 보상해야 함)
    * 

#### **2) `auto_exposure_time_range_min / max`**
* **직관적 의미:** 자동 노출(`auto_exposure_gain: true`)을 쓸 때, AI에게 허용된 "최소/최대 셔터 속도 범위"입니다.
* **변화:**
    * `max`를 **줄이면:** 아무리 어두워도 셔터를 길게 열지 못하게 막습니다. → **모션 블러 방지용 안전장치**.

#### **3) `analog_gain` (아날로그 게인)**
* **직관적 의미:** 센서가 받은 빛 신호를 전기적으로 "뻥튀기" 하는 정도입니다. (단위: mdB)
* **설명:** 스피커 볼륨을 올리는 것과 같습니다. `exposure_time`을 줄여서 어두워진 화면을 이걸로 다시 밝게 만듭니다.
* **변화:**
    * **⬆️ 높이면:** 화면이 **밝아집니다**. 하지만 **자글자글한 노이즈(Grain)**가 조금씩 생깁니다. (그래도 Digital Gain보다는 훨씬 깨끗합니다.)
    * **⬇️ 낮추면:** 화면이 깨끗하지만 **어둡습니다**.

#### **4) `digital_gain` (디지털 게인)**
* **직관적 의미:** 이미 찍힌 사진을 소프트웨어적으로 밝게 보정하는 것입니다. (포토샵 밝기 조절과 동일)
* **설명:** 아날로그 게인으로도 부족할 때 쓰는 "최후의 수단"입니다.
* **변화:**
    * **⬆️ 높이면:** 밝아지지만, 화질이 급격히 나빠지고 **노이즈가 매우 심해집니다**.
    * **추천:** 가능한 **1(사용 안 함)**로 두는 것이 좋습니다.

#### **5) `exposure_compensation` (노출 보정)**
* **직관적 의미:** 자동 노출이 맞춘 밝기보다 "조금 더 밝게/어둡게" 찍으라고 지시하는 바이어스(Bias)입니다.
* **설명:** 50이 "딱 중간(0)"입니다.
* **변화:**
    * **⬆️ 50보다 높이면:** 자동 노출 결과보다 더 **밝게(Over-expose)** 찍습니다.
    * **⬇️ 50보다 낮추면:** 자동 노출 결과보다 더 **어둡게(Under-expose)** 찍습니다.

#### **6) `denoising` (노이즈 제거)**
* **직관적 의미:** 피부 잡티 제거 필터와 같습니다. 자글자글한 노이즈를 뭉개서 부드럽게 만듭니다.
* **변화:**
    * **⬆️ 높이면:** 화면이 매끈해지지만, **디테일이 뭉개집니다(수채화 현상)**. 너무 높으면 특징점이 사라져 Depth 계산이 안 될 수 있습니다.
    * **⬇️ 낮추면:** 선명하지만 노이즈가 그대로 보입니다.

---

### 2. 공통 스테레오 설정 (`common_stereo.yaml`)
이곳은 주로 **이미지의 색감과 느낌(ISP)**을 다룹니다.

#### **1) `saturation` (채도)**
* **직관적 의미:** 색깔의 "진하기"입니다.
* **변화:**
    * **⬆️ 높이면:** 색이 아주 **진하고 화려해집니다(Vivid)**.
    * **⬇️ 낮추면:** 색이 빠져서 **흑백(Grayscale)**에 가까워집니다.
    * **팁:** 로봇 비전에서는 보통 **4** (기본값) 정도가 적당합니다. 너무 높으면 색 번짐이 생깁니다.

#### **2) `sharpness` (선명도)**
* **직관적 의미:** 물체의 테두리(Edge)를 얼마나 날카롭게 할지 결정합니다.
* **변화:**
    * **⬆️ 높이면:** 물체 윤곽이 뚜렷해집니다. 특징점이 잘 잡혀 Depth에 도움이 될 수 있지만, 과하면 테두리에 **하얀 띠(Halo effect)**가 생겨 노이즈가 됩니다.
    * **⬇️ 낮추면:** 이미지가 **흐릿(Soft)**해집니다.

#### **3) `gamma` (감마)**
* **직관적 의미:** 중간 밝기(Mid-tone) 영역의 밝기 조절입니다. (어두운 곳을 밝게 볼지 결정)
* **변화:**
    * **⬆️ 높이면:** 그림자(Shadow) 부분이 밝아져서 **어두운 곳이 잘 보입니다**. (대신 전체적으로 뿌옇게 보일 수 있음)
    * **⬇️ 낮추면:** 대비(Contrast)가 강해지고 그림자가 더 어둡게 묻힙니다.

#### **4) `auto_exposure_gain` (자동 노출/게인)**
* **직관적 의미:** 카메라가 알아서 밝기를 조절할지(Auto), 내가 시키는 대로 할지(Manual) 결정합니다.
* **변화:**
    * **True:** 밝은 곳, 어두운 곳을 로봇이 이동할 때 화면 밝기를 일정하게 유지하려고 노력합니다. (단점: 밝기가 변하는 순간 Depth가 튐)
    * **False:** 사용자가 설정한 `exposure`와 `gain` 값으로 고정됩니다. **조명이 일정한 실내 주행에서는 False가 훨씬 안정적입니다.**

#### **5) `whitebalance_temperature` (색온도)**
* **직관적 의미:** 화면의 "색감"을 따뜻하게 할지 차갑게 할지 결정합니다.
* **변화:**
    * **⬆️ 높이면(6500 쪽):** 화면이 **붉그스름(Warm/Orange)**해집니다.
    * **⬇️ 낮추면(2800 쪽):** 화면이 **파르스름(Cool/Blue)**해집니다.
    * **팁:** 형광등 아래에서 화면이 너무 파랗거나 노랗게 보이면 이 값을 조절해서 "흰색이 흰색으로 보이게" 맞춥니다.

---

### 💡 [요약] 사용자님의 "점프 현상" 해결을 위한 튜닝 가이드

로봇 회전 시 장애물이 튀어나오는 문제를 잡으려면 **`zedx.yaml`**의 다음 3가지 파라미터가 핵심입니다.

1.  **`exposure_time` (가장 중요):**
    * 현재 `16000`(16ms) → **`8000` (8ms)** 이하로 과감히 줄이세요.
    * **효과:** 모션 블러가 사라져 회전 시에도 "칼 같은" 이미지를 얻어 Depth가 튀지 않습니다.
2.  **`analog_gain`:**
    * Exposure를 줄여서 어두워진 화면을 다시 밝혀야 합니다.
    * 현재 `1255` → **`2500` ~ `3000`** 정도로 올리세요.
    * **효과:** 화면 밝기 회복. 노이즈가 약간 늘지만 모션 블러보다는 훨씬 낫습니다.
3.  **`auto_exposure_time_range_max`:**
    * 혹시 `auto_exposure_gain: true`를 쓰신다면, 이 값을 **`10000`** 정도로 제한하세요.
    * **효과:** 카메라가 어두운 곳을 봤을 때 셔터 속도를 맘대로 늘려서 블러를 만드는 것을 원천 봉쇄합니다.
###########################################################
################################################################3
################################################################3
사용자님의 직관이 아주 정확합니다. \*\*"바닥(Floor)을 ROI로 날려버리는 것"\*\*은 **연산 효율과 데이터 전송 측면에서 매우 유리**합니다. 하지만 **"위치 추정(Odometry)" 측면에서는 바닥이 필요**할 수 있습니다.

이 두 가지 관점(연산 효율 vs 위치 추정)을 비교해서 **가장 현명한 설정 방법**을 알려드립니다.

-----

### 1\. 바닥을 날리면 좋은 점 (연산 & 내비게이션 효율)

사용자님 말씀대로, 로봇 주행에서 **평평한 바닥의 Pointcloud**는 사실상 "쓰레기 데이터"에 가깝습니다.

1.  **CPU & 대역폭 절약:**
      * 이미지 하단 30%를 ROI로 날리면, 생성되는 Pointcloud의 개수가 **30% 즉시 감소**합니다.
      * 앞서 우리가 고민했던 \*\*`nvethernet` 에러(대역폭 초과)\*\*나 **ROS 2 Topic Delay**를 줄이는 데 직빵입니다.
2.  **Nav2 Costmap 부하 감소:**
      * Costmap은 들어오는 점군이 바닥인지 장애물인지 높이(`min_obstacle_height`)로 검사해서 바닥이면 버리는 연산을 수행합니다.
      * 애초에 바닥 데이터가 안 들어오면 Nav2가 할 일이 줄어듭니다.
3.  **고스트 노이즈 감소:**
      * 바닥의 텍스처가 부족하거나 반사가 심하면 바닥이 뚫리거나 솟아오르는 노이즈가 생기는데, 이를 원천 차단할 수 있습니다.

### 2\. 바닥이 필요한 경우 (위치 추정 안정성)

하지만 **`pos_tracking` (Visual Odometry)** 기능에게 바닥은 \*\*"보물 창고"\*\*입니다.

1.  **풍부한 특징점:**
      * 천장이나 벽(흰색 페인트)보다, 바닥(카펫 무늬, 타일 줄눈, 얼룩 등)에 \*\*특징점(Feature Points)\*\*이 훨씬 많습니다.
2.  **가까워서 정확함:**
      * 가까운 바닥의 특징점은 로봇 이동에 따른 변화가 뚜렷해서, 로봇의 속도와 이동량을 계산하는 데 핵심적인 역할을 합니다.

-----

### 3\. [솔루션] "스마트 ROI 설정법" (이게 정답입니다)

ZED SDK 설정의 장점은 \*\*"ROI를 기능별로 켜고 꿀 수 있다"\*\*는 점입니다.
**"Depth(장애물 감지)에서는 바닥을 버리고, Tracking(위치 추정)에서는 바닥을 쓰는"** 설정이 가능합니다.

`common_stereo.yaml`을 아래와 같이 수정하세요.

#### **Step 1: ROI 영역 설정 (바닥 30% 날리기)**

`manual_polygon`을 사용하여 **상단 70% 영역**만 유효하도록 설정합니다.
(좌표계: `[0,0]` 좌상단 \~ `[1,1]` 우하단)

  * **남길 영역:** `(0,0)` \~ `(1, 0.7)` 박스 (이미지 위쪽 70%만 살림)

<!-- end list -->

```yaml
region_of_interest:
    automatic_roi: false
    
    # [설정] 상단 70% 영역만 유효하게 설정 (하단 30% 바닥 제거)
    # 순서: 좌상 -> 우상 -> 우하(0.7) -> 좌하(0.7)
    manual_polygon: '[[0.0,0.0],[1.0,0.0],[1.0,0.7],[0.0,0.7]]' 
```

#### **Step 2: 기능별 적용 여부 설정 (핵심)**

```yaml
    # [Depth/PC 생성]: True (바닥 데이터 생성 안 함 -> CPU/대역폭 절약)
    apply_to_depth: true 

    # [위치 추정]: False (ROI 무시하고 전체 이미지 사용 -> 바닥 특징점으로 정밀 주행)
    apply_to_positional_tracking: false 

    # [객체 감지]: True (바닥에 납작하게 붙은 건 무시해도 됨, 필요시 false)
    apply_to_object_detection: true 

    # [매핑]: True (바닥 제외하고 벽만 매핑)
    apply_to_spatial_mapping: true
```

-----

### 4\. 주의사항 (Blind Spot)

하단 30%를 날릴 때 딱 하나 조심해야 할 것은 \*\*"로봇 바로 앞의 낮은 장애물"\*\*입니다.

  * 만약 로봇 바로 앞 30\~50cm 거리에 있는 **낮은 박스**나 **케이블**이 카메라 이미지의 하단 30% 영역에만 잡힌다면?
  * ROI로 날려버리면 로봇은 이 장애물을 못 보고 들이받습니다.
  * **확인법:** Rviz에서 `Camera` 뷰를 켜고, 하단 30%가 정말 "안전한 바닥(로봇 발밑)"인지, 아니면 "회피해야 할 근거리 장애물 영역"인지 확인 후 자르는 비율(`0.7`)을 조절하세요.

### 결론

1.  **바닥 ROI 제거:** **매우 효율적입니다.** (연산량, 통신 딜레이 감소)
2.  **설정 전략:** `apply_to_depth: true`로 설정해 PointCloud 부하를 줄이되, `apply_to_positional_tracking: false`로 설정해 주행 안정성은 챙기세요.
3.  **코드 수정보다 낫나요?:** 네, 이 설정을 쓰면 SDK 내부에서 아예 PointCloud 생성을 안 하므로, 제가 앞서 알려드린 `publishPointCloud` 코드 수정보다 **더 앞단에서 부하를 줄여주는 최고의 방법**입니다.

지금 바로 `manual_polygon` 설정을 적용해 보시길 강력 추천합니다\!
